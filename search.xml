<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>视频</title>
      <link href="/2023/10/03/%E4%B8%87%E4%B8%88%E6%B7%B1%E6%B8%8A/test/"/>
      <url>/2023/10/03/%E4%B8%87%E4%B8%88%E6%B7%B1%E6%B8%8A/test/</url>
      
        <content type="html"><![CDATA[<h1 id="test"><a href="#test" class="headerlink" title="test"></a>test</h1><div class="note no-icon white flat"><iframe src='//player.bilibili.com/player.html?bvid=BV1qm4y1V7LS&cid=1284800836&p=1&share_source=copy_web' scrolling='no' border='0' frameborder='no' framespacing='0' allowfullscreen='true'></iframe></div>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 万丈深渊 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>万丈深渊</title>
      <link href="/2023/08/30/%E4%B8%87%E4%B8%88%E6%B7%B1%E6%B8%8A/%E4%B8%87%E4%B8%88%E6%B7%B1%E6%B8%8A/"/>
      <url>/2023/08/30/%E4%B8%87%E4%B8%88%E6%B7%B1%E6%B8%8A/%E4%B8%87%E4%B8%88%E6%B7%B1%E6%B8%8A/</url>
      
        <content type="html"><![CDATA[<h1 id="项目上的一些查漏补缺"><a href="#项目上的一些查漏补缺" class="headerlink" title="项目上的一些查漏补缺"></a>项目上的一些查漏补缺</h1><div class="note no-icon white flat"><h2 id="从0到1数仓搭建"><a href="#从0到1数仓搭建" class="headerlink" title="从0到1数仓搭建"></a>从0到1数仓搭建</h2><details class="folding-tag" green><summary> 从0到1数仓搭建 </summary>              <div class='content'>              <ol><li>数据调研<ul><li>数据源定位： 确定需要从各个门店、仓库、销售渠道等收集数据，包括销售数据、库存数据、进货数据等。</li><li>数据格式处理： 调研不同数据源的数据格式，了解每个系统中的数据结构和字段，以便后续的数据整合和转换。</li><li>数据更新频率： 确定不同数据源的数据更新频率，区分实时数据和离线数据，以满足业务的及时性需求。</li><li>数据量估计： 估计每个数据源的数据量，考虑到永辉超市的规模，可能需要应用大数据处理技术。</li></ul></li><li>业务调研<ul><li>与业务部门对接： 与永辉超市的业务部门紧密沟通，了解他们的销售流程、库存管理等业务细节。</li><li>项目拆解： 将数仓项目分解为不同的子项目，可能包括销售分析、库存管理、顾客行为分析等，以便逐步建设。</li><li>数据分析需求： 详细了解业务部门的数据分析需求，例如他们是否需要分析销售趋势、热门商品、季节性需求等。</li></ul></li><li>明确统计指标<ul><li>报表分析： 分析业务部门提供的报表需求，可能包括日销售报表、销售额排名、商品类别分布等。</li><li>指标体系： 整理出指标体系，确定哪些指标是原子指标（直接从数据源计算得出）、派生指标（基于原子指标计算）以及衍生指标（更高层次的指标，可能需要多个派生指标计算得出）。</li></ul></li><li>维度建模<ul><li>构建DWD： 设计数据沉淀层，用于存储原始数据。在销售数据中，可能包括订单号、商品ID、销售时间等。</li><li>构建DIM： 构建维度层，存储维度信息，如商品维度、时间维度、门店维度等，方便后续分析和查询。</li></ul></li><li>指标汇总<ul><li>构建DWS： 建设数据服务层，用于存储已汇总指标数据。例如，可以在销售数据中添加商品类别、销售额等汇总信息，以支持更快的查询。</li></ul></li><li>技术选型<ul><li>工具选择： 根据数据调研和业务需求，选择合适的数据同步工具（如Apache Kafka）、数据处理工具（如Apache Spark或Flink）、数据存储工具（如Hadoop HDFS、关系型数据库等）等。</li><li>云服务考虑： 考虑是否使用云服务，如阿里云或腾讯云，以加速搭建过程并降低成本。<br>通过这些步骤，你可以根据永辉超市的实际情况，逐步搭建出一个满足业务需求的离线数仓，以便支持数据分析和业务决策。在每个步骤中，与业务和技术团队的紧密合作都是至关重要的。</li></ul></li></ol>              </div>            </details><h2 id="为什么做分层的时候在flink后面再落一次kafka？"><a href="#为什么做分层的时候在flink后面再落一次kafka？" class="headerlink" title="为什么做分层的时候在flink后面再落一次kafka？"></a>为什么做分层的时候在flink后面再落一次kafka？</h2><details class="folding-tag" green><summary> 为什么做分层的时候在flink后面再落一次kafka？ </summary>              <div class='content'>              <ol><li>在 Flink 应用程序中，将 Kafka 用作数据源和数据接收器（Sink）的一种常见模式是为了实现数据的分层处理。将 Kafka 用于 Flink 应用程序的不同层次有助于提高架构的可伸缩性、容错性和灵活性，以及实现更好的数据处理流程。以下是一些可能的原因和优势：<ul><li>解耦和异步处理： 将 Kafka 用作数据源和数据接收器可以实现解耦。Flink 可以从 Kafka 主题消费数据，然后在 Flink 内部进行各种数据处理，包括转换、过滤、聚合等。最终的结果可以再次写回到 Kafka 主题，以供其他应用程序或层次消费。这种方式使数据处理变得异步，并使不同层次的应用程序能够独立进行调优和扩展。</li><li>多层次处理： 在数据处理流程中，通常需要多个层次的处理，如原始数据处理、实时分析、离线分析等。通过将 Kafka 用于不同层次，每个层次可以独立处理数据，不会影响其他层次的性能。同时，这样的架构还可以让你在每个层次中使用不同的 Flink 程序，以更好地满足各层次的需求。</li><li>容错性和数据保持： 使用 Kafka 作为数据源和接收器有助于实现容错性。如果 Flink 任务在处理过程中失败，数据可以在 Kafka 主题中保留，从而允许 Flink 在重新启动后重新消费数据并继续处理。这种方式可以确保数据的持久性和一致性。</li><li>数据共享和交换： 将数据存储在 Kafka 主题中，使得不同的应用程序和团队可以共享和交换数据。各个团队可以根据需要从 Kafka 主题中获取数据，并进行相应的处理和分析。</li><li>整合其他系统： 使用 Kafka 作为数据交换的中间件，使得 Flink 应用程序能够与其他不同技术栈的系统进行集成。例如，将数据从 Flink 传递到其他数据存储或分析工具中。</li></ul></li></ol>              </div>            </details><h2 id="Flink数据倾斜的处理方案"><a href="#Flink数据倾斜的处理方案" class="headerlink" title="Flink数据倾斜的处理方案"></a>Flink数据倾斜的处理方案</h2><details class="folding-tag" green><summary> 为什么做分层的时候在flink后面再落一次kafka？ </summary>              <div class='content'>              <p>1.增加并行度（Increase Parallelism）： 增加任务的并行度可以将任务划分到更多的 TaskManager 上，使得数据可以更均匀地分布在不同的任务之间。<br>2.局部聚合（Local Aggregation）： 对于某些会引起数据倾斜的操作，可以尽量在每个分区内进行局部聚合，减少数据传输。例如，先对每个分区内的数据进行局部聚合，然后再进行全局聚合。<br>3.侧输出（Side Outputs）： 当发现数据倾斜时，可以将倾斜的数据分区输出到侧输出流，然后对这些数据采取特殊处理，如单独进行重分区、合并等。<br>4.动态重分区（Dynamic Repartitioning）： 监测任务执行过程中数据倾斜情况，一旦发现某些分区过大，可以动态地进行重分区操作，将数据重新分布。<br>5.重分区（Repartitioning）： 重分区是一种常见的处理数据倾斜的方法。通过增加分区数，将数据均匀分布到更多的分区中，从而减少单个分区的数据量。在 Flink 中，你可以使用 rebalance、keyBy 等操作来进行重分区。<br>6.Key 随机化（Key Randomization）： 在进行 keyBy 操作之前，对数据的 key 进行随机化处理，将数据均匀分散到不同的 key 上。这样可以减少特定 key 数据量过大的问题。<br>7.分桶（Bucketing）： 对于可能引起倾斜的 key，可以将它们分成多个桶，然后对每个桶内的数据进行局部聚合。这样可以减小每个桶内的数据量。<br>8.自适应优化（Adaptive Optimization）： 一些 Flink 版本引入了自适应优化机制，可以根据任务执行情况动态地调整数据流分区策略，从而减轻数据倾斜的影响。</p>              </div>            </details><h2 id="Yarn架构"><a href="#Yarn架构" class="headerlink" title="Yarn架构"></a>Yarn架构</h2><details class="folding-tag" green><summary> Yarn架构 </summary>              <div class='content'>              <p>1.client</p><ul><li>客户端发送mr任务到集群</li><li>客户端的种类有很多种<ol><li>ResourceManager</li></ol></li><li>资源协调框架的管理者</li><li>分为主节点和备用节点（防止单点故障）<ul><li>主备的切换基于Zookeeper进行管理</li></ul></li><li>时刻与NodeManager保持心跳，接受NodeManager的汇报<ul><li>NodeManager汇报当前节点的资源情况</li></ul></li><li>当有外部框架要使用资源的时候直接访问ResourceManager即可</li><li>如果有MR任务，先去ResourceManager申请资源，ResourceManager根据汇报相对灵活分配资源<ul><li>资源在NodeManager1，NodeManager1要负责开辟资源</li></ul><ol><li>NodeManager</li></ol></li><li>资源协调框架的执行者</li><li>每一个DataNode上默认有一个NodeManager</li><li>NodeManager汇报自己的信息到ResourceManager<ol><li>Container</li></ol></li><li>2.x资源的代名词</li><li>Container动态分配的<ol><li>ApplicationMaster</li></ol></li><li>我们本次Job任务的主导者</li><li>负责调度本次被分配的资源Container</li><li>当所有的节点任务全部完成，application告诉ResourceManager请求杀死当前ApplicationMaster线程</li><li>本次任务所有的资源都会被释放<ol><li>Task(MapTask—ReduceTask)</li></ol></li><li>开始按照MR的流程执行业务</li><li>当任务完成时，ApplicationMaster接收到当前节点的回馈</li></ul>              </div>            </details><h2 id="Spark提交job流程"><a href="#Spark提交job流程" class="headerlink" title="Spark提交job流程"></a>Spark提交job流程</h2><details class="folding-tag" green><summary> Spark提交job流程 </summary>              <div class='content'>              <p>1.bin/spark-submit \<br>—master yarn \<br>—executor-memory 1G \<br>—executor-cores 1 \<br>—class org.apache.spark.examples.SparkPi \<br>./examples/jars/spark-examples_2.12-3.3.2.jar \<br>10<br>2.$SPARK_HOME/bin/spark-sql \<br>—master yarn —deploy-mode client \<br>—driver-cores 1 —driver-memory 512M \<br>—num-executors 1 —executor-cores 1 —executor-memory 1G \<br>—queue default \<br>-f /root/xxx.sql</p>              </div>            </details><h2 id="个人对于面试理解"><a href="#个人对于面试理解" class="headerlink" title="个人对于面试理解"></a>个人对于面试理解</h2><details class="folding-tag" green><summary> Spark提交job流程 </summary>              <div class='content'>              <p>首先就是我们学习了这么就，有哪些知识。这些知识就是我们的武器，当然我们也可以扩展，就相当于去商店买更多的武器<br>我们也可以向下挖掘，对一个技术做更多的了解，这就相当于升级我们的武器。<br>那么，我，刘智勇有哪些武器呢？？？<br>   1.hadoop</p><ul><li>hadoop架构(就是这把武器由什么组成)</li><li>fdfs读写流程</li><li>mapreduce流程</li><li>hadoop高可用(JN)</li><li>hive优化</li><li>hive架构(metadata元数据库—进行数据映射，client客户端，driver解析器—将sql解析成mr)</li><li>hive执行流程(解析，编译，优化，执行)</li><li>hive事务</li><li>sql基础</li><li>开窗函数</li><li>行列转换</li><li>yarn架构(client，RM,NM)</li><li>yarn运行流程<br>2.spark</li><li>spark架构(client，executor)</li><li>spark运行流程</li><li>spark算子</li><li>sparkSQL</li><li>spark优化</li><li>spark内存管理<br>3.kafka</li><li>kafka架构(生产者，消费者，主题，分区)</li><li>kafka副本</li><li>kafka数据一致性(生产者消费语义)</li><li>kafka消费者分区策略(三种分区，负载均衡，可以处理单一worker压力过大的问题)</li><li>生产者数据产生发送流程(可以处理kafka生产速度)<br>4.flink</li><li>flink算子</li><li>flink状态</li><li>flink容错性(flink没有副本一说)</li><li>状态后端(只管理状态)</li><li>flink端到端的一致性</li><li>flink运行架构</li><li>flink并行度</li><li>flink水位线</li><li>flink优化</li></ul>              </div>            </details><h2 id="位运算通用模板"><a href="#位运算通用模板" class="headerlink" title="位运算通用模板"></a>位运算通用模板</h2><details class="folding-tag" green><summary> 位运算通用模板 </summary>              <div class='content'>              <p>先看题目描述:一个数组，只有一个数字出现了4次，其他所有数字都出现了而且仅仅只出现了3次<br>思路:将每一个数字变成二进制，将该数组的每一个二进制位进行相加，然后模3(也就是其他数字出现的次数)，就是这个特别的数字的倍数。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FindNumber</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="type">int</span> <span class="title function_">findNumber</span><span class="params">(<span class="type">int</span>[] nums)</span> &#123;</span><br><span class="line">        <span class="type">int</span>[] bitCount = <span class="keyword">new</span> <span class="title class_">int</span>[<span class="number">32</span>];</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> num : nums) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">32</span>; i++) &#123;</span><br><span class="line">                bitCount[i] += (num &gt;&gt; i) &amp; <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> <span class="variable">result</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">32</span>; i++) &#123;</span><br><span class="line">            result |= (bitCount[i] % <span class="number">3</span>) &lt;&lt; i;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">int</span>[] nums = &#123;<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>&#125;;</span><br><span class="line">        <span class="type">int</span> <span class="variable">foundNumber</span> <span class="operator">=</span> findNumber(nums);</span><br><span class="line">        System.out.println(foundNumber); <span class="comment">// 输出 1</span></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>              </div>            </details><h2 id="spark的demo"><a href="#spark的demo" class="headerlink" title="spark的demo"></a>spark的demo</h2><details class="folding-tag" green><summary> spark groupby算子 </summary>              <div class='content'>              <p>这是一个wordcount的demo，运行模式是local。</p><p>spark的groupBy算子是可以输出的，这一点区别于flink的keyBy算子，flink的keyBy算子是不可以输出的</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.mrhelloworld.wordcount</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCountLzy</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 建立连接</span></span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(<span class="string">&quot;local[*]&quot;</span>, <span class="string">&quot;WordCount&quot;</span>)</span><br><span class="line">    <span class="comment">// 创建 RDD读取数据</span></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">String</span>] = sparkContext.textFile(<span class="string">&quot;F:\\FeiQIU\\BD37\\11Spark\\002_code\\spark-rdd-demo\\data\\wordcount\\wd1.txt&quot;</span>, <span class="number">2</span>)</span><br><span class="line">    <span class="comment">// groupBy 实现 WordCount</span></span><br><span class="line">    <span class="keyword">val</span> value = rdd.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">    value.foreach(println)</span><br><span class="line">    <span class="keyword">val</span> value1 = value.map((s: <span class="type">String</span>) =&gt; &#123;s -&gt; <span class="number">1</span>&#125;)</span><br><span class="line">    value1.foreach(println)</span><br><span class="line">    <span class="keyword">val</span> unit = value1.groupBy(_._1).foreach(println)</span><br><span class="line">    <span class="comment">// 关闭连接</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>输出</p><p>(Spark,CompactBuffer((Spark,1)))<br>(Hello,CompactBuffer((Hello,1), (Hello,1), (Hello,1), (Hello,1)))<br>(HBase,CompactBuffer((HBase,1)))<br>(Hive,CompactBuffer((Hive,1), (Hive,1)))<br>(Hadoop,CompactBuffer((Hadoop,1), (Hadoop,1), (Hadoop,1)))<br>(Scala,CompactBuffer((Scala,1)))<br>(ZooKeeper,CompactBuffer((ZooKeeper,1)))</p>              </div>            </details><h2 id="拉链表"><a href="#拉链表" class="headerlink" title="拉链表"></a>拉链表</h2><details class="folding-tag" green><summary> 拉链 </summary>              <div class='content'>              <p><a href="https://zhuanlan.zhihu.com/p/618462962">https://zhuanlan.zhihu.com/p/618462962</a></p>              </div>            </details></div>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 万丈深渊 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>案例demo</title>
      <link href="/2023/08/30/%E4%B8%87%E4%B8%88%E6%B7%B1%E6%B8%8A/%E6%A1%88%E4%BE%8Bdemo/"/>
      <url>/2023/08/30/%E4%B8%87%E4%B8%88%E6%B7%B1%E6%B8%8A/%E6%A1%88%E4%BE%8Bdemo/</url>
      
        <content type="html"><![CDATA[<h1 id="案例demo"><a href="#案例demo" class="headerlink" title="案例demo"></a>案例demo</h1><div class="note no-icon white flat"><h2 id="IO流demo"><a href="#IO流demo" class="headerlink" title="IO流demo"></a>IO流demo</h2><details class="folding-tag" green><summary> IO流demo </summary>              <div class='content'>              <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>              </div>            </details><h2 id="Linux常用命令"><a href="#Linux常用命令" class="headerlink" title="Linux常用命令"></a>Linux常用命令</h2><details class="folding-tag" green><summary> Linux常用命令 </summary>              <div class='content'>              <p>tar文件解压缩:tar -xf your_file.tar -C /path/to/destination<br>创建文件夹 mkdir </p>              </div>            </details><h2 id="Mysql常用命令"><a href="#Mysql常用命令" class="headerlink" title="Mysql常用命令"></a>Mysql常用命令</h2><details class="folding-tag" green><summary> Linux常用命令 </summary>              <div class='content'>              <p>mysql创建用户:CREATE USER ‘username’@’localhost’ IDENTIFIED BY ‘password’;<br>设置用户密码等级: set global validate_password_policy=LOW;<br>设置用户密码长度:set global validate_password_length=6;<br>用户权限授予(所有权限):GRANT ALL PRIVILEGES ON database_name.* TO ‘username’@’localhost’;</p>              </div>            </details><h2 id="RPC协议"><a href="#RPC协议" class="headerlink" title="RPC协议"></a>RPC协议</h2><details class="folding-tag" green><summary> RPC协议 </summary>              <div class='content'>              <h3 id="什么是RPC协议"><a href="#什么是RPC协议" class="headerlink" title="什么是RPC协议"></a>什么是RPC协议</h3><details class="folding-tag" green><summary> 什么是RPC协议 </summary>              <div class='content'>              <p>远程过程调用（Remote Procedure Call，RPC）是一种计算机通信协议。它允许一个计算机程序在另一台计算机上执行代码，而不需要程序员显式编写网络代码。RPC协议的实现可以使分布式计算更加容易和透明。<br>RPC协议的核心是一个客户端和一个服务端，它们可以运行在不同的机器上。客户端调用服务端的某个函数，服务端执行该函数并返回结果。客户端可以像调用本地函数一样调用远程函数，而不需要知道底层的网络细节。</p>              </div>            </details><h3 id="如何实现RPC协议？"><a href="#如何实现RPC协议？" class="headerlink" title="如何实现RPC协议？"></a>如何实现RPC协议？</h3><details class="folding-tag" green><summary> 什么是RPC协议 </summary>              <div class='content'>              </div>            </details>              </div>            </details></div>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 万丈深渊 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2023/08/29/Java%E5%B9%B6%E5%8F%91/leetcode%E6%8C%89%E5%BA%8F%E6%89%93%E5%8D%B0/"/>
      <url>/2023/08/29/Java%E5%B9%B6%E5%8F%91/leetcode%E6%8C%89%E5%BA%8F%E6%89%93%E5%8D%B0/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>LeetCode89格雷编码</title>
      <link href="/2023/08/25/%E4%B8%87%E4%B8%88%E6%B7%B1%E6%B8%8A/%E4%BD%8D%E8%BF%90%E7%AE%97/%E6%A0%BC%E9%9B%B7%E7%BC%96%E7%A0%81/"/>
      <url>/2023/08/25/%E4%B8%87%E4%B8%88%E6%B7%B1%E6%B8%8A/%E4%BD%8D%E8%BF%90%E7%AE%97/%E6%A0%BC%E9%9B%B7%E7%BC%96%E7%A0%81/</url>
      
        <content type="html"><![CDATA[<h1 id="LeetCode89格雷编码"><a href="#LeetCode89格雷编码" class="headerlink" title="LeetCode89格雷编码"></a>LeetCode89格雷编码</h1><div class="note no-icon white flat"><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><p>n 位格雷码序列 是一个由 2n 个整数组成的序列，其中：</p><ul><li>每个整数都在范围 [0, 2n - 1] 内（含 0 和 2n - 1）</li><li>第一个整数是 0</li><li>一个整数在序列中出现 不超过一次</li><li>每对 相邻 整数的二进制表示 恰好一位不同 ，且第一个 和 最后一个 整数的二进制表示 恰好一位不同<br>给你一个整数 n ，返回任一有效的 n 位格雷码序列 。</li></ul><h2 id="示例1"><a href="#示例1" class="headerlink" title="示例1"></a>示例1</h2><div class="note no-icon green flat"><p>输入：n = 2<br>输出：[0,1,3,2]<br>解释：<br>[0,1,3,2] 的二进制表示是 [00,01,11,10] 。</p><ul><li>00 和 01 有一位不同</li><li>01 和 11 有一位不同</li><li>11 和 10 有一位不同</li><li>10 和 00 有一位不同<br>[0,2,3,1] 也是一个有效的格雷码序列，其二进制表示是 [00,10,11,01] 。</li><li>00 和 10 有一位不同</li><li>10 和 11 有一位不同</li><li>11 和 01 有一位不同</li><li>01 和 00 有一位不同</li></ul></div><h2 id="示例2"><a href="#示例2" class="headerlink" title="示例2"></a>示例2</h2><div class="note no-icon green flat"><p>输入：n = 1<br>输出：[0,1]</p></div><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><div class="note no-icon green flat"><p>使用数学归纳法，n位的格雷编码可以由n-1位的格雷编码得来。 前半部分和上一个格雷编码相同，首位添0<br>后半部分都是上一份的格雷编码首位添1，这两部分格雷编码在自己的那一部分都满足格雷编码的特性，因为都是加1和加0。<br>所以我们现在的问题要将这两部分连起来，只要连起来，整个格雷编码就做完了。我们发现第二部分的格雷编码倒着排刚好满足。大公高成</p></div><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><div class="note no-icon green flat"><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> bit;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">LeetCode89</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">LeetCode89</span> <span class="variable">leetCode89</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">LeetCode89</span>();</span><br><span class="line">        System.out.println( leetCode89.grayCode(<span class="number">10</span>));</span><br><span class="line">&#125;</span><br><span class="line">    <span class="keyword">public</span> List&lt;Integer&gt; <span class="title function_">grayCode</span><span class="params">(<span class="type">int</span> n)</span> &#123;</span><br><span class="line">        List&lt;Integer&gt; grayCode = <span class="literal">null</span>;</span><br><span class="line">        <span class="keyword">if</span>(n == <span class="number">0</span>)&#123;</span><br><span class="line">            grayCode = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">            grayCode.add(<span class="number">0</span>);</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span>(n&gt;<span class="number">0</span>)&#123;</span><br><span class="line">        grayCode = <span class="built_in">this</span>.grayCode(n - <span class="number">1</span>);</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">1</span>;i&lt;=<span class="number">1</span>&lt;&lt;(n-<span class="number">1</span>);i++)&#123;</span><br><span class="line">            grayCode.add(grayCode.get((<span class="number">1</span>&lt;&lt;(n-<span class="number">1</span>))-i)+(<span class="number">1</span>&lt;&lt;(n-<span class="number">1</span>)));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> grayCode;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div></div>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 万丈深渊 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>原码反码和补码</title>
      <link href="/2023/08/24/%E4%B8%87%E4%B8%88%E6%B7%B1%E6%B8%8A/%E4%BD%8D%E8%BF%90%E7%AE%97/%E5%8E%9F%E7%A0%81%E5%8F%8D%E7%A0%81%E8%A1%A5%E7%A0%81/"/>
      <url>/2023/08/24/%E4%B8%87%E4%B8%88%E6%B7%B1%E6%B8%8A/%E4%BD%8D%E8%BF%90%E7%AE%97/%E5%8E%9F%E7%A0%81%E5%8F%8D%E7%A0%81%E8%A1%A5%E7%A0%81/</url>
      
        <content type="html"><![CDATA[<h1 id="原码反码和补码"><a href="#原码反码和补码" class="headerlink" title="原码反码和补码"></a>原码反码和补码</h1><div class="note no-icon white flat"><h2 id="原码"><a href="#原码" class="headerlink" title="原码"></a>原码</h2><details class="folding-tag" green><summary> 原码 </summary>              <div class='content'>              <p>原码的表示与机器数真值表示的一样，即用第一位表示符号，其余位表示数值。也就是<br>正数：就是它对应的二进制数。<br>负数：将绝对值对应的二进制最左边位变为1。<br>例如的十进制的的正负1，用8位二进制的原码表示如下：</p><ol><li>【+1】= 原：[ 0000 0001 ]</li><li>【-1】= 原：[ 1000 0001 ]</li></ol>              </div>            </details><h2 id="反码"><a href="#反码" class="headerlink" title="反码"></a>反码</h2><details class="folding-tag" green><summary> 反码 </summary>              <div class='content'>              <p>正数 : 和原码相同。<br>负数 : 在其原码的基础上，符号位不变，其余各位取反。</p><ol><li>【+1】= 原： [ 0000 0001 ] = 反：[ 0000 0001 ]</li><li>【-1】 = 原：[ 1000 0001 ] = 反：[ 1111 1110 ]</li></ol>              </div>            </details><h2 id="补码"><a href="#补码" class="headerlink" title="补码"></a>补码</h2><details class="folding-tag" green><summary> 补码 </summary>              <div class='content'>              <p>正数 : 补码是其原码本身。<br>负数 : 补码是在其原码的基础上，符号位不变，其余各位取反后加1（即在反码的基础上加1）。</p><ol><li>【+1】= 原： [ 0000 0001 ] = 反：[ 0000 0001 ] = 补：[ 0000 0001 ]</li><li>【-1】 = 原：[ 1000 0001 ] = 反：[ 1111 1110 ] = 补：[ 1111 1111 ]</li></ol>              </div>            </details></div>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 万丈深渊 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink之二阶段提交和幂等写入</title>
      <link href="/2023/08/19/flink/Flink%E4%B9%8B%E4%BA%8C%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4%E5%92%8C%E5%B9%82%E7%AD%89%E5%86%99%E5%85%A5/"/>
      <url>/2023/08/19/flink/Flink%E4%B9%8B%E4%BA%8C%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4%E5%92%8C%E5%B9%82%E7%AD%89%E5%86%99%E5%85%A5/</url>
      
        <content type="html"><![CDATA[<h1 id="Flink之二阶段提交和幂等写入"><a href="#Flink之二阶段提交和幂等写入" class="headerlink" title="Flink之二阶段提交和幂等写入"></a>Flink之二阶段提交和幂等写入</h1><div class="note no-icon white flat"><h2 id="二阶段提交"><a href="#二阶段提交" class="headerlink" title="二阶段提交"></a>二阶段提交</h2><details class="folding-tag" green><summary> 运行环境 </summary>              <div class='content'>              </div>            </details><h2 id="幂等写入"><a href="#幂等写入" class="headerlink" title="幂等写入"></a>幂等写入</h2><details class="folding-tag" green><summary> 运行环境 </summary>              <div class='content'>              </div>            </details></div>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2023/08/17/flink/Flink%E5%B9%BF%E6%92%AD%E5%92%8C%E6%B5%81%E7%9A%84%E5%90%88%E5%B9%B6/"/>
      <url>/2023/08/17/flink/Flink%E5%B9%BF%E6%92%AD%E5%92%8C%E6%B5%81%E7%9A%84%E5%90%88%E5%B9%B6/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>如何用Java序列化实现拷贝</title>
      <link href="/2023/08/16/%E4%B8%87%E4%B8%88%E6%B7%B1%E6%B8%8A/Java%E5%BA%8F%E5%88%97%E5%8C%96/"/>
      <url>/2023/08/16/%E4%B8%87%E4%B8%88%E6%B7%B1%E6%B8%8A/Java%E5%BA%8F%E5%88%97%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<h1 id="Java序列化"><a href="#Java序列化" class="headerlink" title="Java序列化"></a>Java序列化</h1><div class="note white no-icon flat"><p>这是我今天在做项目的时候遇到的一个问题，大致意思就是如何用Java实现序列化。<br>因为所有代码都是0和1，就像我们把一块钱撕成无数份，只要我们直到如何撕得，我们依然可以把这一块钱给还原回来。<br>序列化就是撕碎的过程，反序列化就是拼接凑拢的过程。<br>而且序列化是可以在内存中实现的，这也是我以前一直不知道的地方。<br>话不多说，上代码。下面这个代码是截取的一部分，DataBean是一个pojo类，就不放出来了，字段太多。<br>意思达到了就行<br><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">public DataBean clone()&#123;</span><br><span class="line">    DataBean clone = new DataBean();</span><br><span class="line">    ByteArrayOutputStream baos = new ByteArrayOutputStream();</span><br><span class="line">    ObjectOutputStream oos = null;</span><br><span class="line">    try &#123;</span><br><span class="line">        oos = new ObjectOutputStream(baos);</span><br><span class="line">        oos.writeObject(this);</span><br><span class="line">        byte[] serializedObject = baos.toByteArray();</span><br><span class="line">        ByteArrayInputStream bais = new ByteArrayInputStream(serializedObject);</span><br><span class="line">        ObjectInputStream ois = new ObjectInputStream(bais);</span><br><span class="line">        clone = (DataBean) ois.readObject();</span><br><span class="line">    &#125; catch (Exception e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    return clone;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p></div>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 万丈深渊 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FlinkSource</title>
      <link href="/2023/08/16/flink/FlinkSource/"/>
      <url>/2023/08/16/flink/FlinkSource/</url>
      
        <content type="html"><![CDATA[<h1 id="FlinkSource"><a href="#FlinkSource" class="headerlink" title="FlinkSource"></a>FlinkSource</h1><div class="note white no-icon flat"><h2 id="如何将mysql中的输入导入到flink中"><a href="#如何将mysql中的输入导入到flink中" class="headerlink" title="如何将mysql中的输入导入到flink中"></a>如何将mysql中的输入导入到flink中</h2><details class="folding-tag" green><summary> 如何将mysql中的输入导入到flink中 </summary>              <div class='content'>              <p>这里学习到的是继承RichSourceFunction，重写里面的open和run方法。<br>open可以理解为初始化，run就是将查到的数据收集起来。<br>下面是代码时间,这是一个小demo，只要照搬，基本可以跑起来，我的所有东西都以可以跑起来为前提<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.RichSourceFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.sql.Connection;</span><br><span class="line"><span class="keyword">import</span> java.sql.DriverManager;</span><br><span class="line"><span class="keyword">import</span> java.sql.PreparedStatement;</span><br><span class="line"><span class="keyword">import</span> java.sql.ResultSet;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">MySQLRichSourceFunctionDemo</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        DataStream&lt;MyData&gt; myDataStream = env.addSource(<span class="keyword">new</span> <span class="title class_">MyRichSourceFunction</span>());</span><br><span class="line"></span><br><span class="line">        myDataStream.print();</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">&quot;MySQL RichSourceFunction Demo&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">MyData</span> &#123;</span><br><span class="line">        <span class="keyword">public</span> <span class="type">int</span> id;</span><br><span class="line">        <span class="keyword">public</span> String value;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">public</span> <span class="title function_">MyData</span><span class="params">(<span class="type">int</span> id, String value)</span> &#123;</span><br><span class="line">            <span class="built_in">this</span>.id = id;</span><br><span class="line">            <span class="built_in">this</span>.value = value;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> String <span class="title function_">toString</span><span class="params">()</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&quot;MyData&#123;&quot;</span> +</span><br><span class="line">                    <span class="string">&quot;id=&quot;</span> + id +</span><br><span class="line">                    <span class="string">&quot;, value=&#x27;&quot;</span> + value + <span class="string">&#x27;\&#x27;&#x27;</span> +</span><br><span class="line">                    <span class="string">&#x27;&#125;&#x27;</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">MyRichSourceFunction</span> <span class="keyword">extends</span> <span class="title class_">RichSourceFunction</span>&lt;MyData&gt; &#123;</span><br><span class="line">        <span class="keyword">private</span> Connection connection;</span><br><span class="line">        <span class="keyword">private</span> PreparedStatement preparedStatement;</span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">volatile</span> <span class="type">boolean</span> <span class="variable">isRunning</span> <span class="operator">=</span> <span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">            <span class="built_in">super</span>.open(parameters);</span><br><span class="line">            connection = DriverManager.getConnection(<span class="string">&quot;jdbc:mysql://localhost:3306/your_database&quot;</span>, <span class="string">&quot;your_username&quot;</span>, <span class="string">&quot;your_password&quot;</span>);</span><br><span class="line">            preparedStatement = connection.prepareStatement(<span class="string">&quot;SELECT id, value FROM your_table&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">run</span><span class="params">(SourceContext&lt;MyData&gt; sourceContext)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">            <span class="keyword">while</span> (isRunning) &#123;</span><br><span class="line">                <span class="type">ResultSet</span> <span class="variable">resultSet</span> <span class="operator">=</span> preparedStatement.executeQuery();</span><br><span class="line">                <span class="keyword">while</span> (resultSet.next()) &#123;</span><br><span class="line">                    <span class="type">int</span> <span class="variable">id</span> <span class="operator">=</span> resultSet.getInt(<span class="string">&quot;id&quot;</span>);</span><br><span class="line">                    <span class="type">String</span> <span class="variable">value</span> <span class="operator">=</span> resultSet.getString(<span class="string">&quot;value&quot;</span>);</span><br><span class="line">                    sourceContext.collect(<span class="keyword">new</span> <span class="title class_">MyData</span>(id, value));</span><br><span class="line">                &#125;</span><br><span class="line">                Thread.sleep(<span class="number">5000</span>); <span class="comment">// You can adjust the polling interval here</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">cancel</span><span class="params">()</span> &#123;</span><br><span class="line">            isRunning = <span class="literal">false</span>;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                <span class="keyword">if</span> (preparedStatement != <span class="literal">null</span>) &#123;</span><br><span class="line">                    preparedStatement.close();</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">if</span> (connection != <span class="literal">null</span>) &#123;</span><br><span class="line">                    connection.close();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>              </div>            </details></div>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>对Flink的一点深入研究</title>
      <link href="/2023/08/16/flink/%E5%AF%B9Flink%E7%9A%84%E4%B8%80%E7%82%B9%E6%B7%B1%E5%85%A5%E7%A0%94%E7%A9%B6/"/>
      <url>/2023/08/16/flink/%E5%AF%B9Flink%E7%9A%84%E4%B8%80%E7%82%B9%E6%B7%B1%E5%85%A5%E7%A0%94%E7%A9%B6/</url>
      
        <content type="html"><![CDATA[<h1 id="对Flink的一点深入研究"><a href="#对Flink的一点深入研究" class="headerlink" title="对Flink的一点深入研究"></a>对Flink的一点深入研究</h1><div class="note white no-icon flat"><h2 id="富函数必须先分区"><a href="#富函数必须先分区" class="headerlink" title="富函数必须先分区"></a>富函数必须先分区</h2><details class="folding-tag" green><summary> 富函数必须先分区 </summary>              <div class='content'>              <p>这是我昨天发现了，富函数必须先分区，比如richMapFunction和keyedProcessFunction。<br>而且分区之后不能使用别的算子，分区之后的下一个算子就必须是富函数，或者keyedProcessFunction，<br>不然这个流可能将不会再被记录成分区的流，也就是说不能再使用富函数。<br>注意，即使不分区依然可以用富函数，但是不会有任何结果，抛出的异常这Flink也不会提示，<br>这就是最狗比的地方。</p>              </div>            </details><h2 id="一点小细节"><a href="#一点小细节" class="headerlink" title="一点小细节"></a>一点小细节</h2><details class="folding-tag" green><summary> 一点小细节 </summary>              <div class='content'>              <p>这个细节就是object类型的数据,算了，先上代码吧。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pojo.DataBean;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.BufferedReader;</span><br><span class="line"><span class="keyword">import</span> java.io.InputStreamReader;</span><br><span class="line"><span class="keyword">import</span> java.net.HttpURLConnection;</span><br><span class="line"><span class="keyword">import</span> java.net.URL;</span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Test02</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        HashMap&lt;String, Object&gt; bean = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line">        bean.put(<span class="string">&quot;deviceId&quot;</span>, <span class="string">&quot;lzy-lzy&quot;</span>);</span><br><span class="line">        bean.put(<span class="string">&quot;anchor_id&quot;</span>,<span class="number">2</span>);</span><br><span class="line">System.out.println(<span class="string">&quot;lzy-lzy&quot;</span>.equals(bean.get(<span class="string">&quot;deviceId&quot;</span>)) &amp;&amp; <span class="string">&quot;2&quot;</span>.equals(bean.get(<span class="string">&quot;anchor_id&quot;</span>)));</span><br><span class="line">System.out.println(<span class="string">&quot;lzy-lzy&quot;</span>.equals(bean.get(<span class="string">&quot;deviceId&quot;</span>)) &amp;&amp; <span class="string">&quot;2&quot;</span>.equals(bean.get(<span class="string">&quot;anchor_id&quot;</span>).toString()));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>上面一个false，一个true。</p><h3 id="equals源码与解析"><a href="#equals源码与解析" class="headerlink" title="equals源码与解析"></a>equals源码与解析</h3><details class="folding-tag" blue><summary> equals源码 </summary>              <div class='content'>              <figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">public boolean equals(Object anObject) &#123;</span><br><span class="line">    if (this == anObject) &#123;</span><br><span class="line">        return true;</span><br><span class="line">    &#125;</span><br><span class="line">    if (anObject instanceof String) &#123;</span><br><span class="line">        String aString = (String)anObject;</span><br><span class="line">        if (coder() == aString.coder()) &#123;</span><br><span class="line">            return isLatin1() ? StringLatin1.equals(value, aString.value)</span><br><span class="line">                              : StringUTF16.equals(value, aString.value);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return false;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>破案了，String类重写equals方法，equals会将对象尝试着转换成String，如果可以转，就转成字符串，比较字符串内容<br>如果不可以转换成字符串，就是比较两个对象的地址。</p>              </div>            </details>              </div>            </details></div>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FlinkSink</title>
      <link href="/2023/08/15/flink/FlinkSInk/"/>
      <url>/2023/08/15/flink/FlinkSInk/</url>
      
        <content type="html"><![CDATA[<h1 id="FlinkSink"><a href="#FlinkSink" class="headerlink" title="FlinkSink"></a>FlinkSink</h1><div class="note white no-icon flat"><h2 id="官网指定sink"><a href="#官网指定sink" class="headerlink" title="官网指定sink"></a>官网指定sink</h2><details class="folding-tag" green><summary> 官网指定sink </summary>              <div class='content'>              <p>这是官方自定义的sink，比较简单，语义是最少一次，可能出现重复数据，<br>泛型只有String。下面是官方的文档，话不多说，上代码</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;String&gt; stream = ...;</span><br><span class="line">        </span><br><span class="line">KafkaSink&lt;String&gt; sink = KafkaSink.&lt;String&gt;builder()</span><br><span class="line">        .setBootstrapServers(brokers)</span><br><span class="line">        .setRecordSerializer(KafkaRecordSerializationSchema.builder()</span><br><span class="line">            .setTopic(&quot;topic-name&quot;)</span><br><span class="line">            .setValueSerializationSchema(new SimpleStringSchema())</span><br><span class="line">            .build()</span><br><span class="line">        )</span><br><span class="line">        .setDeliverGuarantee(DeliveryGuarantee.AT_LEAST_ONCE)</span><br><span class="line">        .build();</span><br><span class="line">        </span><br><span class="line">stream.sinkTo(sink);</span><br></pre></td></tr></table></figure>              </div>            </details><h2 id="自定义sink"><a href="#自定义sink" class="headerlink" title="自定义sink"></a>自定义sink</h2><details class="folding-tag" green><summary> 自定义Sink </summary>              <div class='content'>              <p>官网指定sink只能使用String，有的时候这并不是我们想要的，<br>这个时候就可以重写sink建造器，其他步骤都差不多，就是序列话器接口和反序列话器接口要自己实现<br>话不多说，上代码。<br><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">KafkaSink&lt;DataBean&gt; kafkaSink = KafkaSink.&lt;DataBean&gt;builder()</span><br><span class="line">              .setBootstrapServers(&quot;node01:9092,node02:9092,node03:9092&quot;)</span><br><span class="line">              .setRecordSerializer(KafkaRecordSerializationSchema.builder()</span><br><span class="line">                      .setTopic(&quot;lzytest&quot;)</span><br><span class="line">                      .setValueSerializationSchema(new MyJsonObjectSerializationSchema())</span><br><span class="line">                      .build()</span><br><span class="line">              )</span><br><span class="line">              .setDeliverGuarantee(DeliveryGuarantee.AT_LEAST_ONCE)</span><br><span class="line">              .build();</span><br></pre></td></tr></table></figure></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> kafka;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSON;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.DeserializationSchema;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.SerializationSchema;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.SimpleStringSchema;</span><br><span class="line"><span class="keyword">import</span> pojo.DataBean;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.nio.charset.StandardCharsets;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">MyJsonObjectSerializationSchema</span> <span class="keyword">implements</span> <span class="title class_">SerializationSchema</span>&lt;DataBean&gt; &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">open</span><span class="params">(InitializationContext context)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        SerializationSchema.<span class="built_in">super</span>.open(context);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">byte</span>[] serialize(DataBean bean) &#123;</span><br><span class="line">        <span class="type">String</span> <span class="variable">json</span> <span class="operator">=</span> JSON.toJSONString(bean);</span><br><span class="line">        <span class="keyword">return</span> json.getBytes(StandardCharsets.UTF_8);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>              </div>            </details></div>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SQL之会话划分问题</title>
      <link href="/2023/08/14/SQL/sql%E5%88%86%E4%BA%AB/"/>
      <url>/2023/08/14/SQL/sql%E5%88%86%E4%BA%AB/</url>
      
        <content type="html"><![CDATA[<h1 id="SQL之会话划分问题"><a href="#SQL之会话划分问题" class="headerlink" title="SQL之会话划分问题"></a>SQL之会话划分问题</h1><div class="note no-icon white flat"><h2 id="题目描述与需求"><a href="#题目描述与需求" class="headerlink" title="题目描述与需求"></a>题目描述与需求</h2><p>现有页面浏览记录表（page_view_events）如下，表中有每个用户的每次页面访问记录。</p><p>user_id是用户id，page_id是访问页面id，view_timestamp是访问时间戳id</p><div class="table-container"><table><thead><tr><th>user_id</th><th>page_id</th><th>view_timestamp</th></tr></thead><tbody><tr><td>100</td><td>home</td><td>1659950435</td></tr><tr><td><strong>100</strong></td><td>good_search</td><td>1659950446</td></tr><tr><td><strong>100</strong></td><td>good_list</td><td>1659950457</td></tr><tr><td><strong>100</strong></td><td>home</td><td>1659950541</td></tr><tr><td><strong>100</strong></td><td>good_detail</td><td>1659950552</td></tr><tr><td><strong>100</strong></td><td>cart</td><td>1659950563</td></tr><tr><td><strong>101</strong></td><td>home</td><td>1659950435</td></tr><tr><td><strong>101</strong></td><td>good_search</td><td>1659950446</td></tr><tr><td><strong>101</strong></td><td>good_list</td><td>1659950457</td></tr><tr><td><strong>101</strong></td><td>home</td><td>1659950541</td></tr><tr><td><strong>101</strong></td><td>good_detail</td><td>1659950552</td></tr><tr><td><strong>101</strong></td><td>cart</td><td>1659950563</td></tr><tr><td><strong>102</strong></td><td>good_search</td><td>1659950446</td></tr><tr><td><strong>102</strong></td><td>home</td><td>1659950435</td></tr><tr><td><strong>102</strong></td><td>good_list</td><td>1659950457</td></tr><tr><td><strong>103</strong></td><td>home</td><td>1659950541</td></tr><tr><td><strong>103</strong></td><td>good_detail</td><td>1659950552</td></tr><tr><td><strong>103</strong></td><td>cart</td><td>1659950563</td></tr></tbody></table></div><p>规定若同一用户的相邻两次访问记录时间间隔小于60s，则认为两次浏览记录属于同一会话。现有如下需求，为属于同一会话的访问记录增加一个相同的会话id字段，期望结果如下：</p><div class="table-container"><table><thead><tr><th>user_id</th><th>page_id</th><th>view_timestamp</th><th>session_id</th></tr></thead><tbody><tr><td><strong>100</strong></td><td>home</td><td>1659950435</td><td>100-1</td></tr><tr><td><strong>100</strong></td><td>good_search</td><td>1659950446</td><td>100-1</td></tr><tr><td><strong>100</strong></td><td>good_list</td><td>1659950457</td><td>100-1</td></tr><tr><td><strong>100</strong></td><td>home</td><td>1659950541</td><td>100-2</td></tr><tr><td><strong>100</strong></td><td>good_detail</td><td>1659950552</td><td>100-2</td></tr><tr><td><strong>100</strong></td><td>cart</td><td>1659950563</td><td>100-2</td></tr><tr><td><strong>101</strong></td><td>home</td><td>1659950435</td><td>101-1</td></tr><tr><td><strong>101</strong></td><td>good_search</td><td>1659950446</td><td>101-1</td></tr><tr><td><strong>101</strong></td><td>good_list</td><td>1659950457</td><td>101-1</td></tr><tr><td><strong>101</strong></td><td>home</td><td>1659950541</td><td>101-2</td></tr><tr><td><strong>101</strong></td><td>good_detail</td><td>1659950552</td><td>101-2</td></tr><tr><td><strong>101</strong></td><td>cart</td><td>1659950563</td><td>101-2</td></tr><tr><td><strong>102</strong></td><td>home</td><td>1659950435</td><td>102-1</td></tr><tr><td><strong>102</strong></td><td>good_search</td><td>1659950446</td><td>102-1</td></tr><tr><td><strong>102</strong></td><td>good_list</td><td>1659950457</td><td>102-1</td></tr><tr><td><strong>103</strong></td><td>home</td><td>1659950541</td><td>103-1</td></tr><tr><td><strong>103</strong></td><td>good_detail</td><td>1659950552</td><td>103-1</td></tr></tbody></table></div><h2 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h2><h3 id="建表语句"><a href="#建表语句" class="headerlink" title="建表语句"></a>建表语句</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> if <span class="keyword">exists</span> page_view_events;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> page_view_events</span><br><span class="line">(</span><br><span class="line">    user_id        <span class="type">int</span> comment <span class="string">&#x27;用户id&#x27;</span>,</span><br><span class="line">    page_id        <span class="type">varchar</span>(<span class="number">255</span>) comment <span class="string">&#x27;页面id&#x27;</span>,</span><br><span class="line">    view_timestamp <span class="type">bigint</span> comment <span class="string">&#x27;访问时间戳&#x27;</span></span><br><span class="line">)</span><br><span class="line">    comment <span class="string">&#x27;页面访问记录&#x27;</span>;</span><br></pre></td></tr></table></figure><h3 id="数据装载"><a href="#数据装载" class="headerlink" title="数据装载"></a><strong>数据装载</strong></h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> page_view_events</span><br><span class="line"><span class="keyword">values</span> (<span class="number">100</span>, <span class="string">&#x27;home&#x27;</span>, <span class="number">1659950435</span>),</span><br><span class="line">       (<span class="number">100</span>, <span class="string">&#x27;good_search&#x27;</span>, <span class="number">1659950446</span>),</span><br><span class="line">       (<span class="number">100</span>, <span class="string">&#x27;good_list&#x27;</span>, <span class="number">1659950457</span>),</span><br><span class="line">       (<span class="number">100</span>, <span class="string">&#x27;home&#x27;</span>, <span class="number">1659950541</span>),</span><br><span class="line">       (<span class="number">100</span>, <span class="string">&#x27;good_detail&#x27;</span>, <span class="number">1659950552</span>),</span><br><span class="line">       (<span class="number">100</span>, <span class="string">&#x27;cart&#x27;</span>, <span class="number">1659950563</span>),</span><br><span class="line">       (<span class="number">101</span>, <span class="string">&#x27;home&#x27;</span>, <span class="number">1659950435</span>),</span><br><span class="line">       (<span class="number">101</span>, <span class="string">&#x27;good_search&#x27;</span>, <span class="number">1659950446</span>),</span><br><span class="line">       (<span class="number">101</span>, <span class="string">&#x27;good_list&#x27;</span>, <span class="number">1659950457</span>),</span><br><span class="line">       (<span class="number">101</span>, <span class="string">&#x27;home&#x27;</span>, <span class="number">1659950541</span>),</span><br><span class="line">       (<span class="number">101</span>, <span class="string">&#x27;good_detail&#x27;</span>, <span class="number">1659950552</span>),</span><br><span class="line">       (<span class="number">101</span>, <span class="string">&#x27;cart&#x27;</span>, <span class="number">1659950563</span>),</span><br><span class="line">       (<span class="number">102</span>, <span class="string">&#x27;home&#x27;</span>, <span class="number">1659950435</span>),</span><br><span class="line">       (<span class="number">102</span>, <span class="string">&#x27;good_search&#x27;</span>, <span class="number">1659950446</span>),</span><br><span class="line">       (<span class="number">102</span>, <span class="string">&#x27;good_list&#x27;</span>, <span class="number">1659950457</span>),</span><br><span class="line">       (<span class="number">103</span>, <span class="string">&#x27;home&#x27;</span>, <span class="number">1659950541</span>),</span><br><span class="line">       (<span class="number">103</span>, <span class="string">&#x27;good_detail&#x27;</span>, <span class="number">1659950552</span>),</span><br><span class="line">       (<span class="number">103</span>, <span class="string">&#x27;cart&#x27;</span>, <span class="number">1659950563</span>);</span><br></pre></td></tr></table></figure><h2 id="思路分析"><a href="#思路分析" class="headerlink" title="思路分析"></a>思路分析</h2><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>一下就是大白话环节了。我原以为这是一道看上去简单其实很难的问题，后面发现，他是真不藏着掖着</p><p>，他是真简单。</p><h3 id="我的思路"><a href="#我的思路" class="headerlink" title="我的思路"></a>我的思路</h3><p>还是和以前一样，造表，一层造一层，直到造出我们需要的东西来为止。</p><p>首先，我们先分组排序，按照user_id分组timestamp排序,这显然是可以实现的，这个时候我们需要思考第一个逻辑，相邻不超过60s即判定为同一会话，那么，当前行的数据是和上一行作比较呢，还是和下一行作比较呢？还是和上上行作比较呢？事实上，在对user_id分组timestamp排序以后，我们只需要将当前行和上一行进行比较就可以了，这是显然的(不想打字了，累了)。然后将得到的数据和60作比较，小于60，标记为1，说明和上一行数据是同一会话，大于60，标记为0，说明和上一行数据不是同一会话。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span>,view_timestamp<span class="operator">-</span> <span class="built_in">LAG</span>(view_timestamp, <span class="number">1</span>, view_timestamp) <span class="keyword">OVER</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> view_timestamp) diff,</span><br><span class="line">if(view_timestamp<span class="operator">-</span> <span class="built_in">LAG</span>(view_timestamp, <span class="number">1</span>, view_timestamp) <span class="keyword">OVER</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> view_timestamp)<span class="operator">&lt;</span><span class="number">60</span>,<span class="number">1</span>,<span class="number">0</span>) is_lt_sixty </span><br><span class="line"><span class="keyword">from</span> page_view_events</span><br></pre></td></tr></table></figure><div class="table-container"><table><thead><tr><th>user_id</th><th>page_id</th><th>view_timestamp</th><th>diff</th><th>is_lt_sixty</th></tr></thead><tbody><tr><td>100</td><td>home</td><td>1659950435</td><td>0</td><td>1</td></tr><tr><td>100</td><td>good_search</td><td>1659950446</td><td>11</td><td>1</td></tr><tr><td>100</td><td>good_list</td><td>1659950457</td><td>11</td><td>1</td></tr><tr><td>100</td><td>home</td><td>1659950541</td><td>84</td><td>0</td></tr><tr><td>100</td><td>good_detail</td><td>1659950552</td><td>11</td><td>1</td></tr><tr><td>100</td><td>cart</td><td>1659950563</td><td>11</td><td>1</td></tr></tbody></table></div><p>这是部分数据。</p><p>第二部，如何划分会话，只要做完这一步，然后再拼接起来，这题就写完了。</p><p>我的思路，数0，将分组排序后的第一行的数据到当前行的数据这一段的0全部数出来，比如1659950457这一行就一个0没有，而到了1659950541这一行就有一个0.然后将数出来的0加1就是我们要的数值，最后再拼接即可</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> user_id,page_id,view_timestamp,concat(user_id,<span class="string">&#x27;-&#x27;</span>,<span class="built_in">count</span>(is_lt_sixty)<span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">ROWS</span> UNBOUNDED PRECEDING)<span class="operator">-</span><span class="built_in">sum</span>(is_lt_sixty)<span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">ROWS</span> UNBOUNDED PRECEDING)<span class="operator">+</span><span class="number">1</span>) session_id <span class="keyword">from</span> </span><br><span class="line">(<span class="keyword">select</span> <span class="operator">*</span>,view_timestamp<span class="operator">-</span> <span class="built_in">LAG</span>(view_timestamp, <span class="number">1</span>, view_timestamp) <span class="keyword">OVER</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> view_timestamp),</span><br><span class="line">if(view_timestamp<span class="operator">-</span> <span class="built_in">LAG</span>(view_timestamp, <span class="number">1</span>, view_timestamp) <span class="keyword">OVER</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> view_timestamp)<span class="operator">&lt;</span><span class="number">60</span>,<span class="number">1</span>,<span class="number">0</span>) is_lt_sixty </span><br><span class="line"><span class="keyword">from</span> page_view_events</span><br><span class="line">) lzy</span><br></pre></td></tr></table></figure><p>此即为完整答案。</p><p>感谢看完，我是智勇，希望对你有所帮助。</p></div>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink实时项目</title>
      <link href="/2023/08/14/%E4%B8%87%E4%B8%88%E6%B7%B1%E6%B8%8A/Flink%E5%AE%9E%E6%97%B6%E9%A1%B9%E7%9B%AE/"/>
      <url>/2023/08/14/%E4%B8%87%E4%B8%88%E6%B7%B1%E6%B8%8A/Flink%E5%AE%9E%E6%97%B6%E9%A1%B9%E7%9B%AE/</url>
      
        <content type="html"><![CDATA[<h1 id="Flink实时项目"><a href="#Flink实时项目" class="headerlink" title="Flink实时项目"></a>Flink实时项目</h1><div class="note white no-icon flat"><h2 id="指标"><a href="#指标" class="headerlink" title="指标"></a>指标</h2><details class="folding-tag" green><summary> 指标 </summary>              <div class='content'>              <details class="folding-tag" blue><summary> 新老用户 </summary>              <div class='content'>              <ol><li>新老用户</li></ol>              </div>            </details> <details class="folding-tag" blue><summary> 统计总观看量 </summary>              <div class='content'>              <ol><li>统计总观看量<ul><li>强相关性的指标放在一个任务里面</li></ul></li></ol>              </div>            </details>              </div>            </details></div>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 实时项目 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>看孤注一掷有感</title>
      <link href="/2023/08/14/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/%E7%9C%8B%E5%AD%A4%E6%B3%A8%E4%B8%80%E6%8E%B7%E6%9C%89%E6%84%9F/"/>
      <url>/2023/08/14/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/%E7%9C%8B%E5%AD%A4%E6%B3%A8%E4%B8%80%E6%8E%B7%E6%9C%89%E6%84%9F/</url>
      
        <content type="html"><![CDATA[<div class="note no-icon white flat"><p>8月12号22点看了电影孤注一掷，这是今年看的第6场电影。<br>前面几部分别是闪电侠，蜘蛛侠纵横宇宙， 八角笼中，封神第一部，消失的她，以及这一部，孤注一掷。<br>千人千面，孤注一掷依然无法给我带来太惊艳的感觉。<br>那个赌徒小天的故事刻画的是比较生动具体的，<br>但是潘生和安娜的人物刻画我是不满意的，很粗糙，很敷衍。<br>这两个故事的插入有点败情节。<br>而且小天的第一次的赌博也是我不能理解的，很僵硬。<br>这也是我个人认为这种电影最难演的地方，<br>这种心里活动的转变是最难刻画的，但是一旦演出来了。<br>一旦让观众感受到赌徒内心的改变是很自然的时候，这一类电影我感觉才是真正意义上的好电影。<br>但是孤注一掷也有可圈可点的地方，在那个诈骗地方，确实可以感受压抑，绝望，<br>尤其是潘生捡起马桶眼里的纸条，确实鸡皮疙瘩瞬间就起来了。<br>总的来说，勉强值回票价。<br>刚刚马哥说是20块钱，<br>但如果是20块钱的话，我的建议是无脑入。</p></div>]]></content>
      
      
      <categories>
          
          <category> 生活 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 电影 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>自律第一天</title>
      <link href="/2023/08/14/%E8%87%AA%E5%BE%8B/%E8%87%AA%E5%BE%8B%E7%AC%AC%E4%B8%80%E5%A4%A9/"/>
      <url>/2023/08/14/%E8%87%AA%E5%BE%8B/%E8%87%AA%E5%BE%8B%E7%AC%AC%E4%B8%80%E5%A4%A9/</url>
      
        <content type="html"><![CDATA[<div class="note no-icon white flat"><p>刘智勇呀，刘智勇呀，现在都有红胡子了，以后怎么办。<br>从今天开始，每天中午都要回宿舍睡觉。<br>回宿舍睡觉不允许带手机。<br>每天跑三公里，每天15个俯卧撑，以及不要手淫，不要看黄片，不要看色情电影。<br>每次有不干净的想法，默念，<br>色既是空，色既是空，色即是空。<br>女人都是洪水猛兽，尤其是穿黑丝的。</p></div>]]></content>
      
      
      <categories>
          
          <category> 生活 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自律 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2023/08/14/%E4%B8%87%E4%B8%88%E6%B7%B1%E6%B8%8A/qq%E4%B8%8A%E4%BB%A3%E7%90%86chatgpt/"/>
      <url>/2023/08/14/%E4%B8%87%E4%B8%88%E6%B7%B1%E6%B8%8A/qq%E4%B8%8A%E4%BB%A3%E7%90%86chatgpt/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>8月9日学习总结</title>
      <link href="/2023/08/09/%E4%B8%87%E4%B8%88%E6%B7%B1%E6%B8%8A/8%E6%9C%889%E6%97%A5%E6%80%BB%E7%BB%93/"/>
      <url>/2023/08/09/%E4%B8%87%E4%B8%88%E6%B7%B1%E6%B8%8A/8%E6%9C%889%E6%97%A5%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<h1 id="8月9日学习总结"><a href="#8月9日学习总结" class="headerlink" title="8月9日学习总结"></a>8月9日学习总结</h1><div class="note no-icon white flat"><details class="folding-tag" green><summary> 被阿里云搞麻了 </summary>              <div class='content'>              <p>下午弄了一下阿里云的SD，想在云上玩SD,跟着教程弄了一下，直接麻了<br>老说我的python版本太低了。主要还是技术差，也有可能是他给我的权限就不是管理者权限一开始。<br>总之，实例删除再来一次就好了，搞了好久，太麻了。<br>上云的步骤，还是不懂，但是记住了一点，实例是一台机器。<br>上云应该是上一个集群，集群里面有很多服务器，所以我们也可以创建很多实例。大概是这样。</p>              </div>            </details><details class="folding-tag" green><summary> FlinkSQL学习感悟与总结 </summary>              <div class='content'>              <ol><li>现有数据，再有映射，没有数据，映射屁用没有，有了数据，<br>其实就已经可以分析了，只是有了映射，可以使用sql操作分析更加方便一点。</li><li>数据有内部数据，外部数据，内部数据自己建立，外部数据连接器connector</li><li>映射有两种，一种是在sql里面，使用tableapi来操作，还有一种是使用schema，也是使用tableapi来操作</li></ol>              </div>            </details></div>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 万丈深渊 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>如何部署前端文件资源</title>
      <link href="/2023/08/09/%E4%B8%87%E4%B8%88%E6%B7%B1%E6%B8%8A/%E5%A6%82%E4%BD%95%E9%83%A8%E7%BD%B2%E5%89%8D%E7%AB%AF%E6%96%87%E4%BB%B6%E8%B5%84%E6%BA%90/"/>
      <url>/2023/08/09/%E4%B8%87%E4%B8%88%E6%B7%B1%E6%B8%8A/%E5%A6%82%E4%BD%95%E9%83%A8%E7%BD%B2%E5%89%8D%E7%AB%AF%E6%96%87%E4%BB%B6%E8%B5%84%E6%BA%90/</url>
      
        <content type="html"><![CDATA[<h1 id="如何部署前端文件资源"><a href="#如何部署前端文件资源" class="headerlink" title="如何部署前端文件资源"></a>如何部署前端文件资源</h1><div class="note no-icon white flat"><p>这是昨天学到的东西，就是说有一堆前端文件，我们如何运行他，<br>让这一堆文件在一个端口上跑起来。这就是如何部署前端资源文件。</p><details class="folding-tag" green><summary> Chatgpt参考方法 </summary>              <div class='content'>              <p>当您通过npm install -g命令全局安装一个包时，这个包的可执行文件通常会被添加到系统的PATH环境变量中，使其可以从任何位置访问。<br>要找到全局安装的地址，您可以按照以下步骤操作：</p><ol><li>查找全局包的安装路径：<br>首先，您需要找到npm的全局安装路径。您可以在命令行中运行以下命令来查找全局npm包的安装路径：<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm root -g</span><br></pre></td></tr></table></figure></li><li>进入全局安装路径：<br>运行上述命令后，它会显示全局安装路径。使用命令行中的cd命令进入这个路径，例如：<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /path/to/global/packages</span><br></pre></td></tr></table></figure></li><li>在全局包目录中查找可执行文件：<br>在全局包目录中，您应该能够找到您全局安装的npm包，其中可能包含可执行文件。查找包含您安装的包名称的文件夹，然后进入该文件夹。</li><li>找到可执行文件：<br>在包文件夹中，通常会有一个名为bin或dist等的文件夹，其中包含可执行文件。您可以在这个文件夹中找到要全局使用的工具的可执行文件。</li><li>运行可执行文件：<br>您可以直接在命令行中输入可执行文件的名称来运行它。例如，对于http-server工具，您可以运行：<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http-server</span><br></pre></td></tr></table></figure>请注意，全局安装的路径可能会因操作系统和您的npm配置而有所不同。在某些情况下，您可能需要查阅有关您的系统和npm配置的更多信息，以找到正确的全局安装路径。</li></ol>              </div>            </details></div>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 万丈深渊 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FlinkSql</title>
      <link href="/2023/08/08/flink/FlinkSql/"/>
      <url>/2023/08/08/flink/FlinkSql/</url>
      
        <content type="html"><![CDATA[<h1 id="FlinkSql"><a href="#FlinkSql" class="headerlink" title="FlinkSql"></a>FlinkSql</h1><h2 id="FlinkSQL-通用API"><a href="#FlinkSQL-通用API" class="headerlink" title="FlinkSQL 通用API"></a>FlinkSQL 通用API</h2><div class="note no-icon white flat"><h3 id="运行环境"><a href="#运行环境" class="headerlink" title="运行环境"></a>运行环境</h3><details class="folding-tag" green><summary> 运行环境 </summary>              <div class='content'>              <ol><li>TableEnvironment 是 Table API 和 SQL 的核心概念。它负责:<ul><li>在内部的 catalog 中注册 Table</li><li>注册外部的 catalog</li><li>加载可插拔模块</li><li>执行 SQL 查询</li><li>注册自定义函数 （scalar、table 或 aggregation）</li><li>DataStream 和 Table 之间的转换(面向 StreamTableEnvironment )</li></ul></li><li>Table 总是与特定的 TableEnvironment 绑定。 不能在同一条查询中使用不同<br>TableEnvironment 中的表，例如，对它们进行 join 或 union 操作。</li><li>不论输入数据源是流式的还是批式的，Table API 和 SQL 查询都会被转换成 DataStream 程序。</li><li>创建方式1<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">//设置环境配置和创建 Flink Table 环境</span><br><span class="line">EnvironmentSettings settings = EnvironmentSettings</span><br><span class="line">.newInstance()</span><br><span class="line">.inStreamingMode()</span><br><span class="line">.build();</span><br><span class="line">TableEnvironment tableEnvironment = TableEnvironment.create(settings);</span><br></pre></td></tr></table></figure></li><li>创建方式2<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">//直接获取流运行环境和创建Flink Table环境</span><br><span class="line">StreamExecutionEnvironment environment =</span><br><span class="line">StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">StreamTableEnvironment tableEnvironment =</span><br><span class="line">StreamTableEnvironment.create(environment);</span><br></pre></td></tr></table></figure></li></ol>              </div>            </details><h3 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h3><details class="folding-tag" green><summary> 创建表 </summary>              <div class='content'>              <ol><li>TableEnvironment 维护着一个由标识符（identifier）创建的表 catalog 的映射。<ul><li>标识符由三个部分组成：catalog 名称、数据库名称以及对象名称。</li><li>如果 catalog 或者数据库没有指明，就会使用当前默认值</li></ul></li><li>Table 可以是虚拟的（视图 VIEWS ）也可以是常规的（表 TABLES ）。<ul><li>视图 VIEWS 可以从已经存在的 Table 中创建，一般是 Table API 或者 SQL 的查询结果。</li><li>表 TABLES 描述的是外部数据，例如文件、数据库表或者消息队列。</li></ul></li></ol><h4 id="常规表分类"><a href="#常规表分类" class="headerlink" title="常规表分类"></a>常规表分类</h4><details class="folding-tag" blue><summary> 常规表分类 </summary>              <div class='content'>              <ol><li>临时表（Temporary Table）<ul><li>与单个 Flink 会话（session）的生命周期相关。</li><li>临时表通常保存于内存中并且仅在创建它们的 Flink 会话持续期间存在。这些表对于其它会话<br>是不可见的。</li></ul></li><li>永久表（Permanent Table）<ul><li>在多个 Flink 会话和群集（cluster）中可见。</li><li>永久表需要 catalog（例如 Hive Metastore）以维护表的元数据。一旦永久表被创建，它将<br>对任何连接到 catalog 的 Flink 会话可见且持续存在，直至被明确删除。</li></ul></li></ol>              </div>            </details><h4 id="表标识符"><a href="#表标识符" class="headerlink" title="表标识符"></a>表标识符</h4><details class="folding-tag" blue><summary> 表标识符 </summary>              <div class='content'>              <ol><li>表总是通过三元标识符注册，包括 catalog 名、数据库名和表名。</li><li>用户可以指定一个 catalog 和数据库作为 “当前catalog” 和”当前数据库”。</li><li>如果前两部分的标识符没有指定， 那么会使用当前的 catalog 和当前数据库。</li><li>用户也可以通过 Table API 或 SQL 切换当前的 catalog 和当前的数据库。<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">TableEnvironment tEnv = ...;</span><br><span class="line">tEnv.useCatalog(&quot;custom_catalog&quot;);</span><br><span class="line">tEnv.useDatabase(&quot;custom_database&quot;);</span><br><span class="line">Table table = ...;</span><br><span class="line">// register the view named &#x27;exampleView&#x27; in the catalog named</span><br><span class="line">&#x27;custom_catalog&#x27;</span><br><span class="line">// in the database named &#x27;custom_database&#x27;</span><br><span class="line">tableEnv.createTemporaryView(&quot;exampleView&quot;, table);</span><br><span class="line">// register the view named &#x27;exampleView&#x27; in the catalog named</span><br><span class="line">&#x27;custom_catalog&#x27;</span><br><span class="line">// in the database named &#x27;other_database&#x27;</span><br><span class="line">tableEnv.createTemporaryView(&quot;other_database.exampleView&quot;, table);</span><br><span class="line">// register the view named &#x27;example.View&#x27; in the catalog named</span><br><span class="line">&#x27;custom_catalog&#x27;</span><br><span class="line">// in the database named &#x27;custom_database&#x27;</span><br><span class="line">tableEnv.createTemporaryView(&quot;`example.View`&quot;, table);</span><br><span class="line">// register the view named &#x27;exampleView&#x27; in the catalog named</span><br><span class="line">&#x27;other_catalog&#x27;</span><br><span class="line">// in the database named &#x27;other_database&#x27;</span><br><span class="line">tableEnv.createTemporaryView(&quot;other_catalog.other_database.exampleView&quot;,</span><br><span class="line">table);</span><br></pre></td></tr></table></figure></li></ol>              </div>            </details><h4 id="fromDataStream"><a href="#fromDataStream" class="headerlink" title="fromDataStream"></a>fromDataStream</h4><details class="folding-tag" blue><summary> fromDataStream </summary>              <div class='content'>              <ol><li>想要将一个 DataStream 转换成表也很简单，可以通过调用表环境的 fromDataStream()方法来实<br>现， 返回的就是一个 Table 对象<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env =</span><br><span class="line">StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">// 获取表环境</span><br><span class="line">StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);</span><br><span class="line">// 读取数据源</span><br><span class="line">SingleOutputStreamOperator&lt;Event&gt; eventStream = env.addSource(...)</span><br><span class="line">// 将数据流转换成表</span><br><span class="line">Table eventTable = tableEnv.fromDataStream(eventStream);</span><br></pre></td></tr></table></figure></li><li>如果流里是POJO对象，那么表的一行就是一个对象，表的列名就是对象的属性名，也可以自己选<br>择对象的部分属性来组成表，然后使用as进行重命名<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Table table = tableEnv.fromDataStream(stream, $(&quot;user&quot;).as(&quot;myUser&quot;),</span><br><span class="line">$(&quot;url&quot;).as(&quot;myUrl&quot;));</span><br></pre></td></tr></table></figure></li><li>$ 在Java中的应用<ul><li>Flink对Scala的支持性真的非常棒，让只能用Java的人泪流满面</li><li>引用：import org.apache.flink.table.api.Expressions;</li><li>调用：Expressions.$(“cID”)</li></ul></li></ol>              </div>            </details><h4 id="createTemporaryView"><a href="#createTemporaryView" class="headerlink" title="createTemporaryView"></a>createTemporaryView</h4><details class="folding-tag" blue><summary> createTemporaryView </summary>              <div class='content'>              <ol><li>如果我们希望直接在 SQL 中引用这张表，就需要调用表环境的 createTemporaryView()方法来创<br>建虚拟视图</li><li>调用 createTemporaryView()方法创建虚拟表，传入的两个参数<ul><li>第一个依然是注册的表名</li><li>第二个可以直接就是DataStream</li><li>之后传入多个参数，用来指定表中的字段<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tableEnv.createTemporaryView(&quot;EventTable&quot;, eventStream,</span><br><span class="line">$(&quot;timestamp&quot;).as(&quot;ts&quot;),$(&quot;url&quot;));</span><br></pre></td></tr></table></figure></li></ul></li></ol>              </div>            </details>              </div>            </details><h3 id="流表数据类型"><a href="#流表数据类型" class="headerlink" title="流表数据类型"></a>流表数据类型</h3><details class="folding-tag" green><summary> 流表数据类型 </summary>              <div class='content'>              <h4 id="原子类型"><a href="#原子类型" class="headerlink" title="原子类型"></a>原子类型</h4><details class="folding-tag" blue><summary> 原子类型 </summary>              <div class='content'>              <ol><li>DataStream 中支持的数据类型，Table 中也是都支持的</li><li>在 Flink 中，基础数据类型（Integer、Double、String）和通用数据类型（不可再拆分的数据类<br>型）统一称作“原子类型”。</li><li>原子类型的 DataStream，转换之后就成了只有一列的Table，列字段（field）的数据类型可以由<br>原子类型推断出。</li><li>另外还可以在 fromDataStream()方法里增加参数，用来重新命名列字段<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">StreamTableEnvironment tableEnv = ...;</span><br><span class="line">DataStream&lt;Long&gt; stream = ...;</span><br><span class="line">// 将数据流转换成动态表，动态表只有一个字段，重命名为 myLong</span><br><span class="line">Table table = tableEnv.fromDataStream(stream, $(&quot;myLong&quot;));</span><br></pre></td></tr></table></figure></li></ol>              </div>            </details><h4 id="Tuple类型"><a href="#Tuple类型" class="headerlink" title="Tuple类型"></a>Tuple类型</h4><details class="folding-tag" blue><summary> Tuple类型 </summary>              <div class='content'>              <ol><li>当原子类型不做重命名时，默认的字段名就是“f0”，容易想到，这其实就是将原子类型看作了一元<br>组 Tuple1 的处理结果</li><li>Table 支持 Flink 中定义的元组类型 Tuple，对应在表中字段名默认就是元组中元素的属性名 f0、<br>f1、f2…。</li><li>所有字段都可以被重新排序，也可以提取其中的一部分字段。字段还可以通过调用表达式的 as()方<br>法来进行重命名<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">StreamTableEnvironment tableEnv = ...;</span><br><span class="line">DataStream&lt;Tuple2&lt;Long, Integer&gt;&gt; stream = ...;</span><br><span class="line">// 将数据流转换成只包含 f1 字段的表</span><br><span class="line">Table table = tableEnv.fromDataStream(stream, $(&quot;f1&quot;));</span><br><span class="line">// 将数据流转换成包含 f0 和 f1 字段的表，在表中 f0 和 f1 位置交换</span><br><span class="line">Table table = tableEnv.fromDataStream(stream, $(&quot;f1&quot;), $(&quot;f0&quot;));</span><br><span class="line">// 将 f1 字段命名为 myInt，f0 命名为 myLong</span><br><span class="line">Table table = tableEnv.fromDataStream(stream, $(&quot;f1&quot;).as(&quot;myInt&quot;),</span><br><span class="line">$(&quot;f0&quot;).as(&quot;myLong&quot;));</span><br></pre></td></tr></table></figure></li></ol>              </div>            </details><h4 id="POJO类型"><a href="#POJO类型" class="headerlink" title="POJO类型"></a>POJO类型</h4><details class="folding-tag" blue><summary> POJO类型 </summary>              <div class='content'>              <ol><li>Flink 也支持多种数据类型组合成的“复合类型”，最典型的就是简单 Java 对象（POJO 类型）。</li><li>由于 POJO 中已经定义好了可读性强的字段名，这种类型的数据流转换成 Table 就显得无比顺畅了</li><li>将 POJO 类型的 DataStream 转换成 Table，如果不指定字段名称，就会直接使用原始 POJO 类型<br>中的字段名称。</li><li>POJO 中的字段同样可以被重新排序、提取和重命名<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">StreamTableEnvironment tableEnv = ...;</span><br><span class="line">DataStream&lt;Event&gt; stream = ...;</span><br><span class="line">Table table = tableEnv.fromDataStream(stream);</span><br><span class="line">Table table = tableEnv.fromDataStream(stream, $(&quot;user&quot;));</span><br><span class="line">Table table = tableEnv.fromDataStream(stream, $(&quot;user&quot;).as(&quot;myUser&quot;),</span><br><span class="line">$(&quot;url&quot;).as(&quot;myUrl&quot;));</span><br></pre></td></tr></table></figure></li></ol>              </div>            </details><h4 id="Row类型"><a href="#Row类型" class="headerlink" title="Row类型"></a>Row类型</h4><details class="folding-tag" blue><summary> Row类型 </summary>              <div class='content'>              <ol><li>Flink 中还定义了一个在关系型表中更加通用的数据类型——行（Row），它是 Table 中数据的基<br>本组织形式。</li><li>Row 类型也是一种复合类型，它的长度固定，而且无法直接推断出每个字段的类型，所以在使用<br>时必须指明具体的类型信息；</li><li>在创建 Table 时调用的 CREATE语句就会将所有的字段名称和类型指定，这在 Flink 中被称为表<br>的“模式结构”（Schema）。</li><li>除此之外，Row 类型还附加了一个属性 RowKind，用来表示当前行在更新操作中的类型。</li><li>这样Row 就可以用来表示更新日志流（changelog stream）中的数据，从而架起了 Flink 中流和<br>表的转换桥梁</li><li>所以在更新日志流中，元素的类型必须是 Row，而且需要调用 ofKind()方法来指定更新类型<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;Row&gt; dataStream =</span><br><span class="line">env.fromElements(</span><br><span class="line">Row.ofKind(RowKind.INSERT, &quot;Alice&quot;, 12),</span><br><span class="line">Row.ofKind(RowKind.INSERT, &quot;Bob&quot;, 5),</span><br><span class="line">Row.ofKind(RowKind.UPDATE_BEFORE, &quot;Alice&quot;, 12),</span><br><span class="line">Row.ofKind(RowKind.UPDATE_AFTER, &quot;Alice&quot;, 100));</span><br><span class="line">// 将更新日志流转换为表</span><br><span class="line">Table table = tableEnv.fromChangelogStream(dataStream);</span><br></pre></td></tr></table></figure></li></ol>              </div>            </details>              </div>            </details><h3 id="查询表"><a href="#查询表" class="headerlink" title="查询表"></a>查询表</h3><details class="folding-tag" green><summary> 查询表 </summary>              <div class='content'>              <h4 id="TableAPI"><a href="#TableAPI" class="headerlink" title="TableAPI"></a>TableAPI</h4><details class="folding-tag" blue><summary> TableAPI </summary>              <div class='content'>              <ol><li>Table API 是关于 Scala 和 Java 的集成语言式查询 API。与 SQL 相反，Table API 的查询不是由字<br>符串指定，而是在宿主语言中逐步构建。</li><li>Table API 是基于 Table 类的，该类表示一个表，并提供使用关系操作的方法。这些方法返回一<br>个新的 Table 对象，该对象表示对输入 Table 进行关系操作的结果。</li><li>例如 table.groupBy(…).select() ，其中 groupBy(…) 指定 table 的分组，而<br>select(…) 在 table 分组上的投影。</li><li>文档 Table API 说明了所有流处理和批处理表支持的 Table API 算子。<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">// get a TableEnvironment</span><br><span class="line">TableEnvironment tableEnv = ...; // see &quot;Create a TableEnvironment&quot; section</span><br><span class="line">// register Orders table</span><br><span class="line">// scan registered Orders table</span><br><span class="line">Table orders = tableEnv.from(&quot;Orders&quot;);</span><br><span class="line">// compute revenue for all customers from France</span><br><span class="line">Table revenue = orders</span><br><span class="line">.filter($(&quot;cCountry&quot;).isEqual(&quot;FRANCE&quot;))</span><br><span class="line">.groupBy($(&quot;cID&quot;), $(&quot;cName&quot;))</span><br><span class="line">.select($(&quot;cID&quot;), $(&quot;cName&quot;), $(&quot;revenue&quot;).sum().as(&quot;revSum&quot;));</span><br><span class="line">// emit or convert Table</span><br><span class="line">// execute query</span><br></pre></td></tr></table></figure></li></ol>              </div>            </details><h4 id="SQL"><a href="#SQL" class="headerlink" title="SQL"></a>SQL</h4><details class="folding-tag" blue><summary> SQL </summary>              <div class='content'>              <ol><li>Flink SQL 是基于实现了SQL标准的 Apache Calcite 的。SQL 查询由常规字符串指定。</li><li>文档SQL 描述了Flink对流处理和批处理表的SQL支持。<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">// get a TableEnvironment</span><br><span class="line">TableEnvironment tableEnv = ...; // see &quot;Create a TableEnvironment&quot; section</span><br><span class="line">// register Orders table</span><br><span class="line">// compute revenue for all customers from France</span><br><span class="line">Table revenue = tableEnv.sqlQuery(</span><br><span class="line">&quot;SELECT cID, cName, SUM(revenue) AS revSum &quot; +</span><br><span class="line">&quot;FROM Orders &quot; +</span><br><span class="line">&quot;WHERE cCountry = &#x27;FRANCE&#x27; &quot; +</span><br><span class="line">&quot;GROUP BY cID, cName&quot;</span><br><span class="line">);</span><br><span class="line">// compute revenue for all customers from France and emit to &quot;RevenueFrance&quot;</span><br><span class="line">tableEnv.executeSql(</span><br><span class="line">&quot;INSERT INTO RevenueFrance &quot; +</span><br><span class="line">&quot;SELECT cID, cName, SUM(revenue) AS revSum &quot; +</span><br><span class="line">&quot;FROM Orders &quot; +</span><br><span class="line">&quot;WHERE cCountry = &#x27;FRANCE&#x27; &quot; +</span><br><span class="line">&quot;GROUP BY cID, cName&quot;</span><br><span class="line">);</span><br><span class="line">// emit or convert Table</span><br><span class="line">// execute query</span><br></pre></td></tr></table></figure></li></ol>              </div>            </details><h4 id="混搭方式"><a href="#混搭方式" class="headerlink" title="混搭方式"></a>混搭方式</h4><details class="folding-tag" blue><summary> 混搭方式 </summary>              <div class='content'>              <ol><li>Table API 和 SQL 查询的混用非常简单因为它们都返回 Table 对象：<ul><li>可以在 SQL 查询返回的 Table 对象上定义 Table API 查询。</li><li>在 TableEnvironment 中注册的结果表可以在 SQL 查询的 FROM 子句中引用，通过这种方法<br>就可以在 Table API 查询的结果上定义 SQL 查询。</li></ul></li><li>代码实现<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.yjxxt.flink;</span><br><span class="line"><span class="keyword">import</span> com.yjxxt.pojo.Emp;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span></span><br><span class="line">org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.Expressions;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.Table;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.java.StreamTableEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.codehaus.jackson.map.ObjectMapper;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Description</span> :</span></span><br><span class="line"><span class="comment">* <span class="doctag">@School</span>:优极限学堂</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Official</span>-Website: http://www.yjxxt.com</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Teacher</span>:李毅大帝</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Mail</span>:863159469@qq.com</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Hello03TableDQL</span> &#123;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line"><span class="comment">//运行环境</span></span><br><span class="line"><span class="type">StreamExecutionEnvironment</span> <span class="variable">environment</span> <span class="operator">=</span></span><br><span class="line">StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"><span class="type">StreamTableEnvironment</span> <span class="variable">tableEnvironment</span> <span class="operator">=</span></span><br><span class="line">StreamTableEnvironment.create(environment);</span><br><span class="line"><span class="comment">//创建表</span></span><br><span class="line">DataStreamSource&lt;String&gt; empSource =</span><br><span class="line">environment.readTextFile(<span class="string">&quot;data/emp.txt&quot;</span>);</span><br><span class="line">DataStream&lt;Emp&gt; empStream = empSource.map(line -&gt; <span class="keyword">new</span></span><br><span class="line"><span class="title class_">ObjectMapper</span>().readValue(line, Emp.class));</span><br><span class="line"><span class="type">Table</span> <span class="variable">empTable</span> <span class="operator">=</span> tableEnvironment.fromDataStream(empStream);</span><br><span class="line">empTable.where(Expressions.$(<span class="string">&quot;job&quot;</span>).isEqual(<span class="string">&quot;CLERK&quot;</span>))</span><br><span class="line">.groupBy(Expressions.$(<span class="string">&quot;deptno&quot;</span>))</span><br><span class="line">.select(Expressions.$(<span class="string">&quot;deptno&quot;</span>),</span><br><span class="line">Expressions.$(<span class="string">&quot;sal&quot;</span>).sum().as(<span class="string">&quot;sum_sal&quot;</span>))</span><br><span class="line">.execute()</span><br><span class="line">.print();</span><br><span class="line"><span class="comment">//创建视图</span></span><br><span class="line">tableEnvironment.createTemporaryView(<span class="string">&quot;v_emp&quot;</span>, empTable);</span><br><span class="line">tableEnvironment.executeSql(<span class="string">&quot;select deptno,sum(sal) as sumsal from v_emp where job = &#x27;CLERK&#x27; group by deptno&quot;</span>).print();</span><br><span class="line"><span class="comment">//查询数据--查询混用</span></span><br><span class="line">tableEnvironment.sqlQuery(<span class="string">&quot;select * from &quot;</span> +</span><br><span class="line">empTable.toString()).execute().print();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol>              </div>            </details>              </div>            </details><h3 id="输出表"><a href="#输出表" class="headerlink" title="输出表"></a>输出表</h3><details class="folding-tag" green><summary> 输出表 </summary>              <div class='content'>              <h4 id="insertInto"><a href="#insertInto" class="headerlink" title="insertInto"></a>insertInto</h4><details class="folding-tag" blue><summary> insertInto </summary>              <div class='content'>              <ol><li>Table 通过写入 TableSink 输出。TableSink 是一个通用接口，包括：<ul><li>用于支持多种文件格式（如 CSV、Apache Parquet、Apache Avro）</li><li>存储系统（如 JDBC、Apache HBase、Apache Cassandra、Elasticsearch）</li><li>消息队列系统（如 Apache Kafka、RabbitMQ）。</li></ul></li><li>批处理 Table 只能写入 BatchTableSink，而流处理 Table 需要指定写入<br>AppendStreamTableSink，RetractStreamTableSink 或者 UpsertStreamTableSink。</li><li>Table.insertInto(String tableName) 定义了一个完整的端到端管道将源表中的数据传输到一个被<br>注册的输出表中。<ul><li>该方法通过名称在 catalog 中查找输出表并确认 Table schema 和输出表 schema 一致。</li><li>可以通过方法 TablePipeline.explain() 和 TablePipeline.execute() 分别来解释和执行一个数<br>据流管道。<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">// get a TableEnvironment</span><br><span class="line">TableEnvironment tableEnv = ...;</span><br><span class="line">// create an output Table</span><br><span class="line">final Schema schema = Schema.newBuilder()</span><br><span class="line">.column(&quot;a&quot;, DataTypes.INT())</span><br><span class="line">.column(&quot;b&quot;, DataTypes.STRING())</span><br><span class="line">.column(&quot;c&quot;, DataTypes.BIGINT())</span><br><span class="line">.build();</span><br><span class="line">tableEnv.createTemporaryTable(&quot;CsvSinkTable&quot;,</span><br><span class="line">TableDescriptor.forConnector(&quot;filesystem&quot;)</span><br><span class="line">.schema(schema)</span><br><span class="line">.option(&quot;path&quot;, &quot;/path/to/file&quot;)</span><br><span class="line">.format(FormatDescriptor.forFormat(&quot;csv&quot;)</span><br><span class="line">.option(&quot;field-delimiter&quot;, &quot;|&quot;)</span><br><span class="line">.build())</span><br><span class="line">.build());</span><br><span class="line">// compute a result Table using Table API operators and/or SQL queries</span><br><span class="line">Table result = ...;</span><br><span class="line">// Prepare the insert into pipeline</span><br><span class="line">TablePipeline pipeline = result.insertInto(&quot;CsvSinkTable&quot;);</span><br><span class="line">// Print explain details</span><br><span class="line">pipeline.printExplain();</span><br><span class="line">// emit the result Table to the registered TableSink</span><br><span class="line">pipeline.execute();</span><br></pre></td></tr></table></figure></li></ul></li></ol>              </div>            </details><h4 id="表流转换"><a href="#表流转换" class="headerlink" title="表流转换"></a>表流转换</h4><details class="folding-tag" blue><summary> 表流转换 </summary>              <div class='content'>              <ol><li>toDataStream<ul><li>将一个 Table 对象转换成 DataStream 非常简单，只要直接调用表环境的方法<br>toDataStream()就可以了。</li><li>得到的流只是一个 仅追加流(只有插入) ，只有插入操作<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// 这里需要将要转换的 Table 对象作为参数传入</span><br><span class="line">tableEnv.toDataStream(myTable).print();</span><br></pre></td></tr></table></figure></li></ul></li><li>toChangelogStream<ul><li>对于有更新操作的表，我们不要试图直接把它转换成 DataStream 打印输出，而是记录一下<br>它的“更新日志”（change log）。</li><li>对于表的所有更新操作，就变成了一条更新日志的流，我们就可以转换成流打印输出了。</li><li>编码规则是：<ul><li>INSERT 插入操作编码为 add 消息；</li><li>DELETE 删除操作编码为 retract消息；</li></ul></li></ul></li></ol>              </div>            </details>              </div>            </details><h3 id="解释表"><a href="#解释表" class="headerlink" title="解释表"></a>解释表</h3><details class="folding-tag" green><summary> 解释表 </summary>              <div class='content'>              <ol><li>able API 提供了一种机制来解释计算 Table 的逻辑和优化查询计划。<ul><li>Table.explain() 返回一个 Table 的计划。</li><li>StatementSet.explain() 返回多 sink 计划。</li></ul></li><li>返回的计划包括<ul><li>关系查询的抽象语法树（the Abstract Syntax Tree），即未优化的逻辑查询计划，</li><li>优化的逻辑查询计划</li><li>物理执行计划。</li></ul></li><li>使用 Table.explain() 方法的相应输出：<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env =</span><br><span class="line">StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);</span><br><span class="line">DataStream&lt;Tuple2&lt;Integer, String&gt;&gt; stream1 = env.fromElements(new</span><br><span class="line">Tuple2&lt;&gt;(1, &quot;hello&quot;));</span><br><span class="line">DataStream&lt;Tuple2&lt;Integer, String&gt;&gt; stream2 = env.fromElements(new</span><br><span class="line">Tuple2&lt;&gt;(1, &quot;hello&quot;));</span><br><span class="line">// explain Table API</span><br><span class="line">Table table1 = tEnv.fromDataStream(stream1, $(&quot;count&quot;), $(&quot;word&quot;));</span><br><span class="line">Table table2 = tEnv.fromDataStream(stream2, $(&quot;count&quot;), $(&quot;word&quot;));</span><br><span class="line">Table table = table1.where($(&quot;word&quot;).like(&quot;F%&quot;)).unionAll(table2);</span><br><span class="line">System.out.println(table.explain());</span><br></pre></td></tr></table></figure></li><li>使用 StatementSet.explain() 的多 sink 计划的相应输出：<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">EnvironmentSettings settings = EnvironmentSettings.inStreamingMode();</span><br><span class="line">TableEnvironment tEnv = TableEnvironment.create(settings);</span><br><span class="line">final Schema schema = Schema.newBuilder()</span><br><span class="line">.column(&quot;count&quot;, DataTypes.INT())</span><br><span class="line">.column(&quot;word&quot;, DataTypes.STRING())</span><br><span class="line">.build();</span><br><span class="line">tEnv.createTemporaryTable(&quot;MySource1&quot;,</span><br><span class="line">TableDescriptor.forConnector(&quot;filesystem&quot;)</span><br><span class="line">.schema(schema)</span><br><span class="line">.option(&quot;path&quot;, &quot;/source/path1&quot;)</span><br><span class="line">.format(&quot;csv&quot;)</span><br><span class="line">.build());</span><br><span class="line">tEnv.createTemporaryTable(&quot;MySource2&quot;,</span><br><span class="line">TableDescriptor.forConnector(&quot;filesystem&quot;)</span><br><span class="line">.schema(schema)</span><br><span class="line">.option(&quot;path&quot;, &quot;/source/path2&quot;)</span><br><span class="line">getConfig＃setPlannerConfig 将其提供给 TableEnvironment。</span><br><span class="line">.format(&quot;csv&quot;)</span><br><span class="line">.build());</span><br><span class="line">tEnv.createTemporaryTable(&quot;MySink1&quot;,</span><br><span class="line">TableDescriptor.forConnector(&quot;filesystem&quot;)</span><br><span class="line">.schema(schema)</span><br><span class="line">.option(&quot;path&quot;, &quot;/sink/path1&quot;)</span><br><span class="line">.format(&quot;csv&quot;)</span><br><span class="line">.build());</span><br><span class="line">tEnv.createTemporaryTable(&quot;MySink2&quot;,</span><br><span class="line">TableDescriptor.forConnector(&quot;filesystem&quot;)</span><br><span class="line">.schema(schema)</span><br><span class="line">.option(&quot;path&quot;, &quot;/sink/path2&quot;)</span><br><span class="line">.format(&quot;csv&quot;)</span><br><span class="line">.build());</span><br><span class="line">StatementSet stmtSet = tEnv.createStatementSet();</span><br><span class="line">Table table1 = tEnv.from(&quot;MySource1&quot;).where($(&quot;word&quot;).like(&quot;F%&quot;));</span><br><span class="line">stmtSet.add(table1.insertInto(&quot;MySink1&quot;));</span><br><span class="line">Table table2 = table1.unionAll(tEnv.from(&quot;MySource2&quot;));</span><br><span class="line">stmtSet.add(table2.insertInto(&quot;MySink2&quot;));</span><br><span class="line">String explanation = stmtSet.explain();</span><br><span class="line">System.out.println(explanation);</span><br></pre></td></tr></table></figure></li></ol>              </div>            </details><h3 id="查询优化"><a href="#查询优化" class="headerlink" title="查询优化"></a>查询优化</h3><details class="folding-tag" green><summary> 查询优化 </summary>              <div class='content'>              <ol><li>Apache Flink 使用并扩展了 Apache Calcite 来执行复杂的查询优化。其中包括两种优化器：<ul><li>RBO（基于规则的优化器）</li><li>CBO（基于成本的优化器）</li></ul></li><li>优化方案：<ul><li>基于 Apache Calcite 的子查询解相关</li><li>投影下推（Projection Pushdown）</li><li>分区剪裁（Partition Prune）</li><li>谓词下推（Predicate Pushdown）</li><li>常量折叠（Constant Folding）</li><li>子计划消除重复数据以避免重复计算</li><li>特殊子查询重写，包括两部分：<ul><li>将 IN 和 EXISTS 转换为 left semi-joins</li><li>将 NOT IN 和 NOT EXISTS 转换为 left anti-join</li></ul></li><li>可选 join 重新排序<ul><li>通过 table.optimizer.join-reorder-enabled 启用</li></ul></li></ul></li><li>注意： 当前仅在子查询重写的结合条件下支持 IN / EXISTS / NOT IN / NOT EXISTS。</li><li>优化器不仅基于计划，而且还基于可从数据源获得的丰富统计信息以及每个算子（例如 io，cpu，<br>网络和内存）的细粒度成本来做出明智的决策。</li><li>高级用户可以通过 CalciteConfig 对象提供自定义优化，可以通过调用 TableEnvironment＃<br>getConfig＃setPlannerConfig 将其提供给 TableEnvironment。</li><li><div class="img-wrap"><div class="img-bg"><img class="img" src="http://localhost:4000/img/img_md/FlinkSQL.png"/></div></div></li></ol><h4 id="常量折叠"><a href="#常量折叠" class="headerlink" title="常量折叠"></a>常量折叠</h4><details class="folding-tag" blue><summary> 常量折叠 </summary>              <div class='content'>              <ol><li>常量折叠：对sql中的常量的加减乘除等操作进行预计算，避免执行过程频繁对常量重复执行加减<br>乘除计算：</li><li>折叠前：1+2+t1.value；折叠后：3+t1.value</li></ol>              </div>            </details><h4 id="谓词下推"><a href="#谓词下推" class="headerlink" title="谓词下推"></a>谓词下推</h4><details class="folding-tag" blue><summary> 谓词下推 </summary>              <div class='content'>              <ol><li>谓词推执行：这里就是把t2.id&lt;1000,下推到扫描 t2表的时候。</li><li>执行过程是：全量数据扫描，执行join操作，然后才进行fiter，这明显很浪费，id大于1000的不需<br>要执行join操作，将fliter操作下推到join之前执行，减少了join的数据量，大大提升性能。</li><li>可以理解成先过滤再链接，图就不给了。</li></ol>              </div>            </details><h4 id="投影下推"><a href="#投影下推" class="headerlink" title="投影下推"></a>投影下推</h4><details class="folding-tag" blue><summary> 谓词下推 </summary>              <div class='content'>              <ol><li>投影下推：可以用来避免加载不需要的字段。</li><li>由原来的sql可知，t1只需要加载t1.id，t1.value，t2只需要加载t2.id。假如表还有大量的其他字<br>段，由于SQL中没用到，加载多余字段就是浪费，所以将project操作下推执行，就不需要加载无<br>用字段。而且此时假如是列存储，只需要加载指定的列，优化更大。</li><li>可以理解成最终表的字段投影到子查询中，将子查询中的无关字段给去除</li></ol>              </div>            </details><h4 id="Hash-Join"><a href="#Hash-Join" class="headerlink" title="Hash Join"></a>Hash Join</h4><details class="folding-tag" blue><summary> Hash Join </summary>              <div class='content'>              <ol><li>根据代价 cost 选择批处理 join 有方式(sortmergejoin， hashjoin， boradcasthashjoin)。</li><li>比如前面例子， 再 filter 下推之后， 在 t2.id&lt;1000 的情况下， 由 1 百万数据量变为了 1 千条，<br>计算 cost 之后， 使用 broadcasthashjoin 最合适。</li></ol>              </div>            </details>              </div>            </details></div>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink容错机制</title>
      <link href="/2023/08/07/flink/Flink%E5%AE%B9%E9%94%99%E6%9C%BA%E5%88%B6/"/>
      <url>/2023/08/07/flink/Flink%E5%AE%B9%E9%94%99%E6%9C%BA%E5%88%B6/</url>
      
        <content type="html"><![CDATA[<h1 id="Flink容错机制"><a href="#Flink容错机制" class="headerlink" title="Flink容错机制"></a>Flink容错机制</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><div class="note no-icon white flat"></div>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>太不应该了</title>
      <link href="/2023/08/07/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/%E5%8F%88%E8%AF%B4%E9%94%99%E8%AF%9D%E4%BA%86/"/>
      <url>/2023/08/07/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/%E5%8F%88%E8%AF%B4%E9%94%99%E8%AF%9D%E4%BA%86/</url>
      
        <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="抱歉, 这个密码看着不太对, 请再试试." data-whm="抱歉, 这个文章不能被校验, 不过您还是能看看解密后的内容.">  <script id="hbeData" type="hbeData" data-hmacdigest="a05eb329e2d505fcda7e858183637ff6ccfc8069c7f41f1734e48e67bc5da8d1">9fc3823e76f721d0acfe40e4fcc48e2bd9c9cc8d4e858dc451452741cd6c89cb2aa1fd707e6ab94c65c281bd4bbac22f721c563cafdd74a17ce13f3a0bb71c365c2e1465d5780b88ec70c89d21b58f769616583214b1171fd4c797cdb02679b98058e18a954c92df9908899b84a14801dbd2383971d1578fbc20e512ac30c668404bf04ade057dc2b70cbb894f0e16614503ca195191aad5071a661aa2a364379f54613eccd3a5cf0f0c65abf9f34628595e0087fedfb758956742c8eb48465c6076953a945063aa5170084bb5f73b88211ebf92f34184582d9394c43a7d7ee12d3ee29aff13ff09f1e9e85d92c83365472e6fb7de9ec5c24ad8d7cc1bb12855d915887861027805305bc48758cbfc842690462a3e778512a4cec14cde7945278c9af512f5646c01da88d5ab2e453fa62106694dd28204bb0b8423d168c708406c162dabae0f5f8f8a42fdb7fdbf9644b6b9e5070af5b1f249c8ad98694e6e832127d3bccb528ed56f97f72c2d6763edd3f0bd39440a768dbcd7592918b922cb7bfce5e876441edf673db2b30713fd64756024eeddfc39409f018088d7ee781f77193460c72fc02e88c490fa037afac55e393910aad79d12a094ad139208ae6e0a747a2facaefd942be829b2eb05a042061cb50e7b93e0d17f26e634dfc0599529b2cb20a90442798eb11d21f939a8fd15a82e22176300bd9ef5ba834b4d1278f8d0574faa8aaf4db6ae6a25fc7dcc7053207b4158c8adf7815037962f0cb6f03e16e470833d96ee631ec07e5e245940dd789b1c66f5ec31630c907c3a6cc5e29b6332c7006eb62aa7dd76a27667d66abfaec44d2baab32987bdaa63c34a20ab6c431f8a71e7875c44ad20dab82a12d14bd3a4724cef2c4c170333d681add00e29452fcadde80b37aaf71591be1bba3eb3344a72686d921bdf78fb3ecdb171eca8c78c4160e994fc01e7d50966992ac691b54a1d1d93ad6226acb7840b7047cf</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-xray">      <input class="hbe hbe-input-field hbe-input-field-xray" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-xray" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-xray">您好, 这里需要密码.</span>      </label>      <svg class="hbe hbe-graphic hbe-graphic-xray" width="300%" height="100%" viewBox="0 0 1200 60" preserveAspectRatio="none">        <path d="M0,56.5c0,0,298.666,0,399.333,0C448.336,56.5,513.994,46,597,46c77.327,0,135,10.5,200.999,10.5c95.996,0,402.001,0,402.001,0"></path>        <path d="M0,2.5c0,0,298.666,0,399.333,0C448.336,2.5,513.994,13,597,13c77.327,0,135-10.5,200.999-10.5c95.996,0,402.001,0,402.001,0"></path>      </svg>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      
      
      <categories>
          
          <category> 生活 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 我的秘密 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>牛客SQL161</title>
      <link href="/2023/08/07/SQL/SQL161/"/>
      <url>/2023/08/07/SQL/SQL161/</url>
      
        <content type="html"><![CDATA[<h1 id="牛客SQL161"><a href="#牛客SQL161" class="headerlink" title="牛客SQL161"></a>牛客SQL161</h1><h2 id="吐槽与感悟"><a href="#吐槽与感悟" class="headerlink" title="吐槽与感悟"></a>吐槽与感悟</h2><div class="note no-icon white flat"><p>麻了，真的麻了。要我说，以后去手术台也不用打啥麻药了，做两道SQL题，人就麻了。<br>这个题思路很简单，不涉及什么技巧，难的是一些细节上的考虑</p></div><h2 id="题目概述"><a href="#题目概述" class="headerlink" title="题目概述"></a>题目概述</h2><div class="note no-icon white flat"><div class="img-wrap"><div class="img-bg"><img class="img" src="http://localhost:4000/img/img_sql/牛客SQL161.png"/></div></div><div class="img-wrap"><div class="img-bg"><img class="img" src="http://localhost:4000/img/img_sql/牛客161_2.png"/></div></div><p>这就是上述题目了，思路特别简单，就是先把观看时间算出来，根据观看时间算出这次播放是否完播，<br>然后再进行分组聚合算出所有的指标。<br>以后写sql能先过滤的先过滤的，先过滤的话题目会变得简单许多。<br>这里的过滤就是过滤最近的一个月，他这里题目不严谨，最近的一个月这里指的是近一个月才发布的视频。<br>这个指标是只有组合后才可以算出来的。<br>所以第一步是先造伪列，第二部造出指标过滤，最后就是算，然后就完事了</p></div><h2 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h2><div class="note no-icon white flat"><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> temp <span class="keyword">as</span>(</span><br><span class="line"><span class="keyword">select</span> t1.<span class="operator">*</span>,</span><br><span class="line">t2.duration,</span><br><span class="line">t2.release_time,</span><br><span class="line">TIMESTAMPDIFF(<span class="keyword">second</span>,start_time,end_time) watched_time,</span><br><span class="line">t2.video_id video_id1</span><br><span class="line"><span class="keyword">from</span> tb_video_info t2 </span><br><span class="line"><span class="keyword">right</span> <span class="keyword">join</span> tb_user_video_log t1</span><br><span class="line"><span class="keyword">on</span> t1.video_id <span class="operator">=</span> t2. video_id</span><br><span class="line"><span class="keyword">WHERE</span> DATEDIFF(<span class="type">DATE</span>((<span class="keyword">SELECT</span> <span class="built_in">MAX</span>(end_time) <span class="keyword">FROM</span> tb_user_video_log)), <span class="type">DATE</span>(t2.release_time)) <span class="operator">&lt;=</span> <span class="number">29</span>),</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">temp2 <span class="keyword">as</span></span><br><span class="line">(<span class="keyword">select</span> video_id1,<span class="type">date</span>(<span class="built_in">max</span>(end_time)) last_watch_time,</span><br><span class="line"><span class="number">3</span><span class="operator">*</span><span class="built_in">count</span>(comment_id) comment_num,</span><br><span class="line"><span class="number">5</span><span class="operator">*</span><span class="built_in">sum</span>(if_like) like_num,</span><br><span class="line"><span class="number">100</span><span class="operator">*</span>(<span class="built_in">sum</span>(if(watched_time<span class="operator">&gt;=</span>duration,<span class="number">1</span>,<span class="number">0</span>))<span class="operator">/</span><span class="built_in">count</span>(watched_time)) full_wacth,</span><br><span class="line"><span class="number">2</span><span class="operator">*</span><span class="built_in">sum</span>(if_retweet ) retweet_num <span class="keyword">from</span> temp</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> video_id</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> video_id1,round((comment_num<span class="operator">+</span>like_num<span class="operator">+</span>full_wacth<span class="operator">+</span>retweet_num)<span class="operator">/</span>(TIMESTAMPDIFF(<span class="keyword">day</span>,last_watch_time,today)<span class="operator">+</span><span class="number">1</span>),<span class="number">0</span>) hot_index <span class="keyword">from</span> temp2 </span><br><span class="line"><span class="keyword">full</span> <span class="keyword">join</span></span><br><span class="line">(<span class="keyword">select</span> <span class="type">date</span>(<span class="built_in">max</span>(end_time)) today <span class="keyword">from</span> tb_user_video_log) lzy2</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> hot_index <span class="keyword">desc</span> limit <span class="number">3</span></span><br></pre></td></tr></table></figure></div>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>待到秋来九月八，一日看尽长安花</title>
      <link href="/2023/08/05/SQL/%E7%89%9B%E5%AE%A2SQL/"/>
      <url>/2023/08/05/SQL/%E7%89%9B%E5%AE%A2SQL/</url>
      
        <content type="html"><![CDATA[<h1 id="牛客SQL"><a href="#牛客SQL" class="headerlink" title="牛客SQL"></a>牛客SQL</h1><h2 id="感慨"><a href="#感慨" class="headerlink" title="感慨"></a>感慨</h2><div class="note no-icon white flat"><p>现在只能说是渐入家境了。基本上任何sql理清楚需求后都变得特别简单<br>昨天到今天将近2个小时的时间，写了4道sql题，这放在以前是根本不可能的。<br>但是现在我已经基本上做到了。</p></div><h2 id="牛客SQL270"><a href="#牛客SQL270" class="headerlink" title="牛客SQL270"></a>牛客SQL270</h2><div class="note no-icon white flat"><div class="img-wrap"><div class="img-bg"><img class="img" src="http://localhost:4000/img/img_sql/SQL270.png"/></div></div><p>思路：这个思路很简单，先造出一条伪列，伪列表示这个岗位数据在相同岗位中排名第几。<br>再造一条伪列，表示这个岗位一共有多少次考试。<br>造出这两个伪列后，以其为临时表。<br>然后框框一顿敲，搞定。<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> temp <span class="keyword">as</span>(</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span>,<span class="built_in">row_number</span>()<span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> job <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) rk,</span><br><span class="line"><span class="built_in">count</span>(job)<span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> job ) number</span><br><span class="line"><span class="keyword">from</span> grade)</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span></span><br><span class="line">(<span class="keyword">select</span> id,job,score,rk <span class="keyword">from</span> temp</span><br><span class="line"><span class="keyword">where</span> number<span class="operator">%</span><span class="number">2</span> <span class="operator">=</span> <span class="number">0</span> <span class="keyword">and</span> (rk <span class="operator">=</span> number<span class="operator">/</span><span class="number">2</span> <span class="keyword">or</span> rk <span class="operator">=</span> number<span class="operator">/</span><span class="number">2</span><span class="operator">+</span><span class="number">1</span>)</span><br><span class="line"><span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line"><span class="keyword">select</span> id,job,score,rk <span class="keyword">from</span> temp</span><br><span class="line"><span class="keyword">where</span> number<span class="operator">%</span><span class="number">2</span> <span class="operator">=</span> <span class="number">1</span> <span class="keyword">and</span> (rk <span class="operator">=</span> (number<span class="operator">+</span><span class="number">1</span>)<span class="operator">/</span><span class="number">2</span>)) lzy</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> id</span><br></pre></td></tr></table></figure></p></div><h2 id="牛客SQL275"><a href="#牛客SQL275" class="headerlink" title="牛客SQL275"></a>牛客SQL275</h2><div class="note no-icon white flat"><p><div class="img-wrap"><div class="img-bg"><img class="img" src="http://localhost:4000/img/img_sql/SQL275.png"/></div>&lt;/div&gt;<br>思路：这个主要是行专列<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> temp <span class="keyword">as</span>(</span><br><span class="line">    <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> order_info</span><br><span class="line">    <span class="keyword">where</span> status <span class="operator">=</span> <span class="string">&#x27;completed&#x27;</span> <span class="keyword">and</span> <span class="type">date</span><span class="operator">&gt;</span><span class="string">&#x27;2025-10-15&#x27;</span> <span class="keyword">and</span> (product_name<span class="operator">=</span> <span class="string">&#x27;C++&#x27;</span> <span class="keyword">or</span> product_name<span class="operator">=</span> <span class="string">&#x27;Python&#x27;</span> <span class="keyword">or</span> product_name<span class="operator">=</span><span class="string">&#x27;Java&#x27;</span>)</span><br><span class="line">    ),</span><br><span class="line">    lzy1 <span class="keyword">as</span>(</span><br><span class="line"><span class="keyword">select</span>  user_id,<span class="type">date</span>,number cnt,rk <span class="keyword">from</span></span><br><span class="line">    (<span class="keyword">select</span> <span class="operator">*</span>,<span class="built_in">count</span>(user_id)<span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id) number,</span><br><span class="line">    <span class="built_in">row_number</span>()<span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> <span class="type">date</span> <span class="keyword">asc</span> ) rk</span><br><span class="line">    <span class="keyword">from</span> temp) lzy <span class="keyword">where</span> number<span class="operator">&gt;</span><span class="number">1</span> <span class="keyword">and</span> rk<span class="operator">&lt;</span><span class="number">3</span>)</span><br><span class="line"><span class="keyword">select</span> user_id,<span class="built_in">min</span>(first_buy_date) ,<span class="built_in">min</span>(second_buy_date),cnt <span class="keyword">from</span></span><br><span class="line">    (<span class="keyword">select</span> user_id,cnt,</span><br><span class="line">            <span class="keyword">case</span></span><br><span class="line">                <span class="keyword">when</span></span><br><span class="line">                    rk <span class="operator">=</span> <span class="number">1</span> <span class="keyword">then</span> <span class="type">date</span></span><br><span class="line">             <span class="keyword">else</span> <span class="string">&#x27;9999-12-31&#x27;</span></span><br><span class="line">             <span class="keyword">end</span> <span class="keyword">as</span> first_buy_date,</span><br><span class="line">         <span class="keyword">case</span></span><br><span class="line">         <span class="keyword">when</span></span><br><span class="line">         rk <span class="operator">=</span> <span class="number">2</span> <span class="keyword">then</span> <span class="type">date</span></span><br><span class="line">         <span class="keyword">else</span> <span class="string">&#x27;9999-12-31&#x27;</span></span><br><span class="line">         <span class="keyword">end</span> <span class="keyword">as</span> second_buy_date</span><br><span class="line">     <span class="keyword">from</span> lzy1) lzy2</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> user_id,cnt</span><br></pre></td></tr></table></figure></p></div><h2 id="SQL280"><a href="#SQL280" class="headerlink" title="SQL280"></a>SQL280</h2><div class="note no-icon white flat"><p>题就不给了，这个题很简单，就是简单的日期裁剪，就不说了<br>链接:<br><a href="https://www.nowcoder.com/practice/83f84aa5c32b4cf5a75558d02dd7743c?tpId=82&amp;tqId=37924&amp;rp=1&amp;ru=/exam/company&amp;qru=/exam/company&amp;sourceUrl=%2Fexam%2Fcompany&amp;difficulty=5&amp;judgeStatus=undefined&amp;tags=&amp;title=">https://www.nowcoder.com/practice/83f84aa5c32b4cf5a75558d02dd7743c?tpId=82&amp;tqId=37924&amp;rp=1&amp;ru=/exam/company&amp;qru=/exam/company&amp;sourceUrl=%2Fexam%2Fcompany&amp;difficulty=5&amp;judgeStatus=undefined&amp;tags=&amp;title=</a><br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> temp <span class="keyword">as</span>(</span><br><span class="line"><span class="keyword">select</span> formatted_date ,job,<span class="built_in">sum</span>(num) cnt <span class="keyword">from</span> </span><br><span class="line">(<span class="keyword">SELECT</span> <span class="operator">*</span>,DATE_FORMAT(<span class="type">date</span>, <span class="string">&#x27;%Y-%m&#x27;</span>) <span class="keyword">AS</span> formatted_date </span><br><span class="line"><span class="keyword">from</span> resume_info) lzy </span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> formatted_date ,job)</span><br><span class="line">,</span><br><span class="line">lzy <span class="keyword">as</span>(</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span>,<span class="built_in">SUBSTRING</span>(formatted_date, <span class="number">1</span>, <span class="number">4</span>) <span class="keyword">AS</span> <span class="keyword">year</span>,</span><br><span class="line"><span class="built_in">SUBSTRING</span>(formatted_date, <span class="number">6</span>, <span class="number">7</span>) <span class="keyword">AS</span> <span class="keyword">month</span></span><br><span class="line"><span class="keyword">from</span> temp)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> l1.job,l1.formatted_date first_year_mon,</span><br><span class="line">l1.cnt first_year_cnt,</span><br><span class="line">l2.formatted_date second_year_mon,</span><br><span class="line">l2.cnt second_year_cnt <span class="keyword">from</span> lzy l1 </span><br><span class="line"><span class="keyword">join</span>  lzy l2 <span class="keyword">on</span> l1.month<span class="operator">=</span>l2.month <span class="keyword">and</span> l1.year<span class="operator">=</span>l2.year<span class="number">-1</span> <span class="keyword">and</span> l1.job<span class="operator">=</span>l2.job <span class="keyword">and</span> l1.formatted_date <span class="operator">&lt;</span><span class="string">&#x27;2026-01-01&#x27;</span></span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> first_year_mon <span class="keyword">desc</span>,job <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure></p></div><h2 id="SQL285"><a href="#SQL285" class="headerlink" title="SQL285"></a>SQL285</h2><div class="note no-icon white flat"><p>这个题是最简单的，就是简单的造伪列，这个就不说了，太简单了<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> temp <span class="keyword">as</span>(</span><br><span class="line"><span class="keyword">select</span> lzy.user_id id,u.name,<span class="built_in">sum</span>(grade) grade_num <span class="keyword">from</span></span><br><span class="line">(<span class="keyword">SELECT</span> user_id,if(type<span class="operator">=</span><span class="string">&#x27;add&#x27;</span>,grade_num,<span class="number">0</span><span class="operator">-</span>grade_num) grade <span class="keyword">FROM</span> grade_info) lzy</span><br><span class="line"><span class="keyword">right</span> <span class="keyword">join</span> <span class="keyword">user</span> u <span class="keyword">on</span> u.id <span class="operator">=</span> lzy.user_id </span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> user_id,u.name)</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> id,name,grade_num <span class="keyword">from</span></span><br><span class="line">(<span class="keyword">select</span> <span class="operator">*</span>,<span class="built_in">max</span>(grade_num)<span class="keyword">over</span>() max_grade_num <span class="keyword">from</span> temp)lzy</span><br><span class="line"><span class="keyword">where</span> grade_num<span class="operator">=</span>max_grade_num</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> id <span class="keyword">asc</span>;</span><br></pre></td></tr></table></figure></p></div>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>浅浅的总结一下flink</title>
      <link href="/2023/08/04/flink/flink%E6%B5%85%E6%B5%85%E7%9A%84%E6%80%BB%E7%BB%93%E4%B8%80%E4%B8%8B/"/>
      <url>/2023/08/04/flink/flink%E6%B5%85%E6%B5%85%E7%9A%84%E6%80%BB%E7%BB%93%E4%B8%80%E4%B8%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="水位线"><a href="#水位线" class="headerlink" title="水位线"></a>水位线</h1><div class="note no-icon white flat"><p>水位线本来是不懂的，后来强哥稍稍提点了一下，瞬间醍醐灌顶。<br>就一个原则，他的时间是指定出来的时间（事件时间），不再是我们现实世界里的时间。<br>比如说它指定了一个时间戳字段为时间， 那么9点的时间不来那么9点就永远不会来。<br>哪怕有几千万条数据的时间戳是一样的，也就是说是同一时间到达的，<br>即使现实里面已经过去了很久很久，在flink中，他们都是同时的。<br>然后就是上车模型，比如说一个车负责装着8点到9点来的人，那么只要9点到了，这个车立马就走。<br>那如何判断9点到了呢？？就是9点的那个人来了就是9点到了。<br>相当于每一个人衣服身上有一个时间戳，司机只能通过判断衣服上的时间戳来判断时间。<br>那么如何判断起始时间8点呢？？就是说第一个上车的人，然后将他的时间取整，就是这辆车的起始时间。<br>也就是说如果第一个人是8点15分上车，<br>那么即使第二个人是8点5分来的，那么第二个人依然可以上车。（在并发，网络io，机器处理速率等各种情况下时间戳前面的人可能会后到达）<br>如果有延迟时间，比如5分钟，那就是9时5分钟（还是9时5分的那个人）来了才开车，但是还是只允许8点到9点的人上车。<br>这是最简单的水位线，还有并发情况下的水位线。</p></div><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><div class="note no-icon white flat"><p>代码主要是建立</p><ol><li>运行环境</li><li>逻辑代码<ul><li>算子</li><li>状态</li><li>窗口</li><li>process算子(底层算子)</li><li>侧输出</li></ul></li><li>状态保存，状态的逻辑代码<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.state;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.atguigu.bean.WaterSensor;</span><br><span class="line"><span class="keyword">import</span> com.atguigu.functions.WaterSensorMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.eventtime.WatermarkStrategy;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.state.ListState;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.state.ListStateDescriptor;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.state.ValueState;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.state.ValueStateDescriptor;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.typeinfo.Types;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.KeyedProcessFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.Collector;</span><br><span class="line"><span class="keyword">import</span> java.time.Duration;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * TODO 针对每种传感器输出最高的3个水位值</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> cjp</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@version</span> 1.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">KeyedListStateDemo</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;WaterSensor&gt; sensorDS = env</span><br><span class="line">                .socketTextStream(<span class="string">&quot;hadoop102&quot;</span>, <span class="number">7777</span>)</span><br><span class="line">                .map(<span class="keyword">new</span> <span class="title class_">WaterSensorMapFunction</span>())</span><br><span class="line">                .assignTimestampsAndWatermarks(</span><br><span class="line">                        WatermarkStrategy.&lt;WaterSensor&gt;forBoundedOutOfOrderness(Duration.ofSeconds(<span class="number">3</span>))</span><br><span class="line">                                .withTimestampAssigner((element, ts) -&gt; element.getTs() * <span class="number">1000L</span>)</span><br><span class="line">                );</span><br><span class="line"></span><br><span class="line">        sensorDS.keyBy(r -&gt; r.getId())</span><br><span class="line">                .process(<span class="keyword">new</span> <span class="title class_">KeyedProcessFunction</span>&lt;String, WaterSensor, String&gt;() &#123;</span><br><span class="line"></span><br><span class="line">                            ListState&lt;Integer&gt; vcListState;</span><br><span class="line"></span><br><span class="line">                            <span class="meta">@Override</span></span><br><span class="line">                            <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                                <span class="built_in">super</span>.open(parameters);</span><br><span class="line">                                vcListState = getRuntimeContext().getListState(<span class="keyword">new</span> <span class="title class_">ListStateDescriptor</span>&lt;Integer&gt;(<span class="string">&quot;vcListState&quot;</span>, Types.INT));</span><br><span class="line">                            &#125;</span><br><span class="line"></span><br><span class="line">                            <span class="meta">@Override</span></span><br><span class="line">                            <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">processElement</span><span class="params">(WaterSensor value, Context ctx, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                                <span class="comment">// 1.来一条，存到list状态里</span></span><br><span class="line">                                vcListState.add(value.getVc());</span><br><span class="line"></span><br><span class="line">                                <span class="comment">// 2.从list状态拿出来(Iterable)， 拷贝到一个List中，排序， 只留3个最大的</span></span><br><span class="line">                                Iterable&lt;Integer&gt; vcListIt = vcListState.get();</span><br><span class="line">                                <span class="comment">// 2.1 拷贝到List中</span></span><br><span class="line">                                List&lt;Integer&gt; vcList = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">                                <span class="keyword">for</span> (Integer vc : vcListIt) &#123;</span><br><span class="line">                                    vcList.add(vc);</span><br><span class="line">                                &#125;</span><br><span class="line">                                <span class="comment">// 2.2 对List进行降序排序</span></span><br><span class="line">                                vcList.sort((o1, o2) -&gt; o2 - o1);</span><br><span class="line">                                <span class="comment">// 2.3 只保留最大的3个(list中的个数一定是连续变大，一超过3就立即清理即可)</span></span><br><span class="line">                                <span class="keyword">if</span> (vcList.size() &gt; <span class="number">3</span>) &#123;</span><br><span class="line">                                    <span class="comment">// 将最后一个元素清除（第4个）</span></span><br><span class="line">                                    vcList.remove(<span class="number">3</span>);</span><br><span class="line">                                &#125;</span><br><span class="line"></span><br><span class="line">                                out.collect(<span class="string">&quot;传感器id为&quot;</span> + value.getId() + <span class="string">&quot;,最大的3个水位值=&quot;</span> + vcList.toString());</span><br><span class="line"></span><br><span class="line">                                <span class="comment">// 3.更新list状态</span></span><br><span class="line">                                vcListState.update(vcList);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//                                vcListState.get();            //取出 list状态 本组的数据，是一个Iterable</span></span><br><span class="line"><span class="comment">//                                vcListState.add();            // 向 list状态 本组 添加一个元素</span></span><br><span class="line"><span class="comment">//                                vcListState.addAll();         // 向 list状态 本组 添加多个元素</span></span><br><span class="line"><span class="comment">//                                vcListState.update();         // 更新 list状态 本组数据（覆盖）</span></span><br><span class="line"><span class="comment">//                                vcListState.clear();          // 清空List状态 本组数据</span></span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                ).print();</span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li></ol></div><h1 id="些许感悟"><a href="#些许感悟" class="headerlink" title="些许感悟"></a>些许感悟</h1><div class="note no-icon white flat"><p>没加窗口，那么就是一条一条处理，它的方法也是来一条处理一条。<br>但是加了窗口以后，就是批处理了，所提供的函数也直接是一个集合。</p></div>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>maven中的provded属性</title>
      <link href="/2023/08/04/%E9%94%99%E8%AF%AF%E6%97%A5%E5%BF%97%E8%AE%B0%E5%BD%95/maven%E7%9A%84provided%E5%B1%9E%E6%80%A7/"/>
      <url>/2023/08/04/%E9%94%99%E8%AF%AF%E6%97%A5%E5%BF%97%E8%AE%B0%E5%BD%95/maven%E7%9A%84provided%E5%B1%9E%E6%80%A7/</url>
      
        <content type="html"><![CDATA[<div class="note no-icon white flat"><p>太痛苦辣！！！太痛苦辣！！！<br>以后都会将自己走过的坑一一记录下来，不然对不起自己两个小时的努力。<br>这个问题很简单，maven中的<dependency>下的子属性如果添加了provided属性，<br>那么将不会再添加包，只是不会报编译错误。但是运行的时候是不会找包的。<br>截个图</p><div class="img-wrap"><div class="img-bg"><img class="img" src="http://localhost:4000/img/img_md/maven.png"/></div></div></div>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 太痛苦辣 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2023年8月3号夜游外滩</title>
      <link href="/2023/08/03/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/2023%E5%B9%B48%E6%9C%883%E5%8F%B7%E5%A4%9C%E6%B8%B8%E5%A4%96%E6%BB%A9/"/>
      <url>/2023/08/03/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/2023%E5%B9%B48%E6%9C%883%E5%8F%B7%E5%A4%9C%E6%B8%B8%E5%A4%96%E6%BB%A9/</url>
      
        <content type="html"><![CDATA[<h1 id="夜游外滩"><a href="#夜游外滩" class="headerlink" title="夜游外滩"></a>夜游外滩</h1><div class="note no-icon white flat"><p>昨天晚上和豪帅去了一次外滩，记录一下。<br>总体而言，感觉还可以，果然，外滩最好的解锁方式就是钱。<br>有钱，在外滩就是爽。<br>没有拍照，看到两个贼贼贼贼贼贼好看的外国小姐姐，超高的鼻梁，<br>雪白的肌肤，直的不能再直的背。<br>还有那腿，虽然我也不高，但是比我还高真的狠狠羡慕了，要能摸两下，拿豪帅二十年寿命换我也愿意啊。<br>背影太杀我了啊啊啊！！！<br>第一次碰巧遇见的时候她们坐在一个门店外面蹭空调，因为实在是太热了，我们也想蹭的，但想了想，还是算了。<br>不能给女人靠近我的机会。<br>第二次碰到的时候是在上地铁的时候，果然，有些人生来就是要鹤立鸡群的，仅仅只是遥遥的一眼<br>我就感觉就是她。可惜是反方向的地铁，不然还能再看两眼。<br>下地铁的时候，碰到了“我是秦始皇”的事情，一个女生（大长腿，很难不注意到）在地铁口打电话，非常激动（确信），目测是和男友吵架了。<br>我本来想上去给他一个陌生人的鼓励的，说不定就被陌生人（我）拿下了。但想了想，不行，这铁沸羊羊行为，我刘某铁不能做。<br>虽然腿也不错，但是和之前那个外国小姐姐还是稍逊一筹。<br>果然，人上地铁的时候，不能遇见太惊艳的人，不然到下地铁了都忘不了。</p></div>]]></content>
      
      
      <categories>
          
          <category> 生活 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 逛街 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>算子状态</title>
      <link href="/2023/08/01/flink/%E7%AE%97%E5%AD%90%E7%8A%B6%E6%80%81/"/>
      <url>/2023/08/01/flink/%E7%AE%97%E5%AD%90%E7%8A%B6%E6%80%81/</url>
      
        <content type="html"><![CDATA[<h1 id="算子状态"><a href="#算子状态" class="headerlink" title="算子状态"></a>算子状态</h1><div class="img-wrap"><div class="img-bg"><img class="img" src="http://localhost:4000/img/img_md/算子状态.png"/></div></div><h2 id="状态定义"><a href="#状态定义" class="headerlink" title="状态定义"></a>状态定义</h2><div class="note white no-icon flat"><ol><li>在Flink中，算子任务可以分为无状态和有状态两种情况。<ul><li>无状态算子<ul><li>无状态的算子任务只需要观察每个独立事件，根据当前输入的数据直接转换输出结果。</li><li>如map、filter、flatMap，计算时不依赖其他数据，就都属于无状态的算子。</li><li><div class="img-wrap"><div class="img-bg"><img class="img" src="http://localhost:4000/img/img_md/算子状态2.png"/></div></div></li></ul></li><li>有状态算子<ul><li>当前数据之外，还需要一些其他数据来得到计算结果。这里的“其他数据”，就是所谓的<br>状态（state）</li><li>比如，做求和（sum）计算时，需要保存之前所有数据的和，这就是状态；</li><li><div class="img-wrap"><div class="img-bg"><img class="img" src="http://localhost:4000/img/img_md/算子状态3.png"/></div></div></li></ul></li></ul></li><li>为什么流式计算需要状态<ul><li>离线任务失败：<ul><li>重启任务，然后重新读一遍输入数据，最后把昨天数据重新计算一遍即可。</li></ul></li><li>实时任务失败：<ul><li>重启任务，然后重新读一遍输入数据，最后把昨天数据重新计算一遍就不可以了。</li><li>因为实时任务第一重要的就是时效性，很明显重新计算违背了时效性原则</li></ul></li></ul></li></ol></div><h2 id="托管方式"><a href="#托管方式" class="headerlink" title="托管方式"></a>托管方式</h2><div class="note white no-icon flat"><div class="img-wrap"><div class="img-bg"><img class="img" src="http://localhost:4000/img/img_md/托管方式.png"/></div></div><h3 id="原始状态-Raw-State"><a href="#原始状态-Raw-State" class="headerlink" title="原始状态(Raw State)"></a>原始状态(Raw State)</h3><div class="note green no-icon flat"><ol><li>原始状态则是自定义的，相当于就是开辟了一块内存，需要我们自己管理，实现状态的序列化和故<br>障恢复</li><li>Flink不会对状态进行任何自动操作，也不知道状态的具体数据类型，只会把它当作最原始的字节<br>（Byte）数组来存储。</li><li>程序员需要花费大量的精力来处理状态的管理和维护。所以只有在遇到托管状态无法实现的特殊需<br>求时，我们才会考虑使用原始状态；</li><li>一般情况下不推荐使用</li></ol></div><h3 id="托管状态-Managed-State"><a href="#托管状态-Managed-State" class="headerlink" title="托管状态(Managed State)"></a>托管状态(Managed State)</h3><div class="note green no-icon flat"><ol><li>托管状态就是由Flink统一管理的，状态的存储访问、故障恢复和重组等一系列问题都由Flink实<br>现，我们只要调接口就可以；</li><li>托管状态是由Flink的运行时（Runtime）来托管的；在配置容错机制后，状态会自动持久化保<br>存，并在发生故障时自动恢复。</li><li>当应用发生横向扩展时，状态也会自动地重组分配到所有的子任务实例上。</li><li>Flink提供了值状态（ValueState）、列表状态（ListState）、映射状态（MapState）、聚合状态<br>（AggregateState）等多种结构，内部支持各种数据类型。</li></ol></div><h3 id="状态类型"><a href="#状态类型" class="headerlink" title="状态类型"></a>状态类型</h3><div class="note green no-icon flat"><ol><li>在Flink中，一个算子任务会按照并行度分为多个并行子任务执行，而不同的子任务会占据不同的<br>任务槽（task slot）。</li><li>由于不同的slot在计算资源上是物理隔离的，所以Flink能管理的状态在并行任务间是无法共享的，<br>每个状态只能针对当前子任务的实例有效。</li><li>而很多有状态的操作（比如聚合、窗口）都是要先做keyBy进行按键分区的。</li><li>按键分区之后，任务所进行的所有计算都应该只针对当前key有效，所以状态也应该按照key彼此<br>隔离。在这种情况下，状态的访问方式又会有所不同。</li><li>基于这样的想法，我们又可以将托管状态分为两类：算子状态和按键分区状态。</li></ol><h3 id="算子状态-Operator-State"><a href="#算子状态-Operator-State" class="headerlink" title="算子状态(Operator State)"></a>算子状态(Operator State)</h3><div class="img-wrap"><div class="img-bg"><img class="img" src="http://localhost:4000/img/img_md/托管方式.png"/></div></div></div></div>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>水位线</title>
      <link href="/2023/07/31/flink/%E6%B0%B4%E4%BD%8D%E7%BA%BF/"/>
      <url>/2023/07/31/flink/%E6%B0%B4%E4%BD%8D%E7%BA%BF/</url>
      
        <content type="html"><![CDATA[<h1 id="Flink水位线"><a href="#Flink水位线" class="headerlink" title="Flink水位线"></a>Flink水位线</h1><h2 id="水位线意义"><a href="#水位线意义" class="headerlink" title="水位线意义"></a>水位线意义</h2><div class="note white no-icon flat"><ol><li>“水位线”就是用来度量事件时间</li><li>如何选择时间窗口<ul><li>想要统计一段时间内的数据，需要划分时间窗口，这时只要判断一下事件时间就可以知道数<br>据属于哪个窗口了。明确了一个数据的所属窗口，还不能直接进行计算。因为窗口处理的是<br>有界数据，我们需要等窗口的数据都到齐了，才能计算出最终的统计结果。那什么时候数据<br>就都到齐了呢？对于时间窗口来说这很明显：到了窗口的结束时间，自然就应该收集到了所<br>有数据，就可以触发计算输出结果了。比如我们想统计 8 点~9 点的用户点击量，那就是从 8<br>点开始收集数据，到 9点截止，将收集的数据做处理计算。这有点类似于班车，每小时发一<br>班，那么8 点之后来的人都会上同一班车，到 9 点钟准时发车；9 点之后来的人，就只好等<br>下一班 10点发的车了。</li><li>如何确认9点钟？<ul><li>在处理时间语义下，都是以当前任务所在节点的系统时间为准的。这就相当于每辆车里都挂<br>了一个钟，司机看到到了 9 点就直接发车。这种方式简单粗暴容易实现，但因为车上的钟是<br>独立运行的，以它为标准就不能准确地判断商品的生产时间。在分布式环境下，这样会因为<br>网络传输延迟的不确定而导致误差。比如有些商品在 8 点 59 分 59 秒生产出来，可是从下生<br>产线到运至车上又要花费几秒，那就赶不上 9 点钟这班车了。而且现在分布式系统中有很多<br>辆 9点发的班车，所以同时生产出的一批商品，需要平均分配到不同班车上，可这些班车距<br>离有近有远、上面挂的钟有快有慢，这就可能导致有些商品上车了、有些却被漏掉；先后生<br>产出的商品，到达车上的顺序也可能乱掉：统计结果的正确性受到了影响。</li></ul></li><li>如何给车装个表？<ul><li>在处理时间语义下，都是以当前任务所在节点的系统时间为准的。这就相当于每辆车里都挂<br>了一个钟，司机看到到了 9 点就直接发车。这种方式简单粗暴容易实现，但因为车上的钟是<br>独立运行的，以它为标准就不能准确地判断商品的生产时间。在分布式环境下，这样会因为<br>网络传输延迟的不确定而导致误差。比如有些商品在 8 点 59 分 59 秒生产出来，可是从下生<br>产线到运至车上又要花费几秒，那就赶不上 9 点钟这班车了。而且现在分布式系统中有很多<br>辆 9点发的班车，所以同时生产出的一批商品，需要平均分配到不同班车上，可这些班车距<br>离有近有远、上面挂的钟有快有慢，这就可能导致有些商品上车了、有些却被漏掉；先后生<br>产出的商品，到达车上的顺序也可能乱掉：统计结果的正确性受到了影响。</li></ul></li><li>水位线是是数据流中的一部分，随着数据一起流动，在不同任务之间传输。</li><li>水位线可以看做是插入到数据流中的一个标记点，主要内容就是一个时间戳，用来指示当前的事件<br>时间(实际就是用来度量事件时间的)。</li><li>而它插入流中的位置，就应该是在某个数据到来之后；这样就可以从这个数据中提取时间戳，作为<br>当前水位线的时间戳了。</li></ul></li></ol></div><h2 id="有序流中的水位线"><a href="#有序流中的水位线" class="headerlink" title="有序流中的水位线"></a>有序流中的水位线</h2><div class="note white no-icon flat"><ol><li>在理想状态下，数据应该按照它们生成的先后顺序、排好队进入流中；也就是说，它们处理的过程<br>会保持原先的顺序不变，遵守先来后到的原则。这样的话我们从每个数据中提取时间戳，就可以保<br>证总是从小到大增长的，从而插入的水位线也会不断增长、事件时钟不断向前推进。</li><li>实际应用中，如果当前数据量非常大，可能会有很多数据的时间戳是相同的，这时每来一条数据就<br>提取时间戳、插入水位线就做了大量的无用功。而且即使时间戳不同，同时涌来的数据时间差会非<br>常小（比如几毫秒），往往对处理计算也没什么影响。所以为了提高效率，一般会每隔一段时间生<br>成一个水位线，这个水位线的时间戳，就是当前最新数据的时间戳。<div class="img-wrap"><div class="img-bg"><img class="img" src="http://localhost:4000/img/img_md/水位线.png"/></div></div></li></ol></div><h2 id="乱序流中的水位线"><a href="#乱序流中的水位线" class="headerlink" title="乱序流中的水位线"></a>乱序流中的水位线</h2><div class="note white no-icon flat"><ol><li><div class="img-wrap"><div class="img-bg"><img class="img" src="http://localhost:4000/img/img_md/乱序流中的水位线.png"/></div></div></li><li>有序流的处理非常简单，看起来水位线也并没有起到太大的作用。但这种情况只存在于理想状态<br>下。我们知道在分布式系统中，数据在节点间传输，会因为网络传输延迟的不确定性，导致顺序发<br>生改变，这就是所谓的“乱序数据”。这里所说的“乱序”（out-of-order），是指数据的先后顺序不<br>一致，主要就是基于数据的产生时间而言的。如下图所示，一个7秒时产生的数据，生成时间自然<br>要比9秒的数据早；但是经过数据缓存和传输之后，处理任务可能先收到了9秒的数据，之后7秒的<br>数据才姗姗来迟。这时如果我们希望插入水位线，来指示当前的事件时间进展，又该怎么做？</li><li><div class="img-wrap"><div class="img-bg"><img class="img" src="http://localhost:4000/img/img_md/水位线2.png"/></div></div></li><li>最直观的想法自然是跟之前一样，我们还是靠数据来驱动，每来一个数据就提取它的时间戳、插入<br>一个水位线。不过现在的情况是数据乱序，所以有可能新的时间戳比之前的还小，如果直接将这个<br>时间的水位线再插入，我们的“时钟”就回退了——水位线就代表了时钟，时光不能倒流，所以水位<br>线的时间戳也不能减小。解决思路也很简单：我们插入新的水位线时，要先判断一下时间戳是否比<br>之前的大，否则就不再生成新的水位线，也就是说，只有数据的时间戳比当前时钟大，才能推动时<br>钟前进，这时才插入水位线。</li><li><div class="img-wrap"><div class="img-bg"><img class="img" src="http://localhost:4000/img/img_md/水位线3.png"/></div></div></li><li>如果考虑到大量数据同时到来的处理效率，我们同样可以周期性地生成水位线。这时只需要保存一<br>下之前所有数据中的最大时间戳，需要插入水位线时，就直接以它作为时间戳生成新的水位线</li><li>这样做尽管可以定义出一个事件时钟，却也会带来一个非常大的问题：我们无法正确处理“迟到”的<br>数据。在上面的例子中，当9秒产生的数据到来之后，我们就直接将时钟推进到了9秒；如果有一<br>个窗口结束时间就是9秒（比如，要统计0~9秒的所有数据），那么这时窗口就应该关闭、将收集<br>到的所有数据计算输出结果了。但事实上，由于数据是乱序的，还可能有时间戳为7秒、8秒的数<br>据在9秒的数据之后才到来，这就是“迟到数据”（late data）。它们本来也应该属于0~9秒这个窗<br>口，但此时窗口已经关闭，于是这些数据就被遗漏了，这会导致统计结果不正确。如果用之前我们<br>类比班车的例子，现在的状况就是商品不是按照生产时间顺序到来的，所以有可能出现这种情况：<br>9点生产的商品已经到了，我们认为已经到了9点，所以直接发车；但是可能还会有8点59分59秒<br>生产的商品迟到了，没有赶上这班车。那怎么解决这个问题呢？</li><li><div class="img-wrap"><div class="img-bg"><img class="img" src="http://localhost:4000/img/img_md/水位线4.png"/></div></div></li><li>其实我们有很多生活中的经验。假如是一个团队出去团建，那肯定希望每个人都不能落下；如果有<br>人因为堵车没能准时到车上，我们可以稍微等一会儿。9点发车，我们可以等到9点10分，等人都<br>到齐了再出发。当然，实际应用的网络环境不可能跟北京的交通一样堵，所以不需要等那么久，或<br>许只要等一两秒钟就可以了。具体在商品班车的例子里，我们可以多等2秒钟，也就是当生产时间<br>为9点零2秒的商品到达，时钟推进到9点零2秒，这时就认为所有8点到9点生产的商品都到齐了，<br>可以正式发车。不过这样相当于更改了发车时间，属于“违规操作”。为了做到形式上仍然是9点发<br>车，我们可以更改一下时钟推进的逻辑：当一个商品到达时，不要直接用它的生产时间作为当前时<br>间，而是减上两秒，这就相当于把车上的逻辑时钟调慢了。这样一来，当9点生产的商品到达时，<br>我们当前车上的时间是8点59分58秒，还没到发车时间；当9点零2秒生产的商品到达时，车上时<br>间刚好是9点，这时该到的商品都到齐了，准时发车就没问题了。回到上面的例子，为了让窗口能<br>够正确收集到迟到的数据，我们也可以等上2秒；也就是用当前已有数据的最大时间戳减去2秒，<br>就是要插入的水位线的时间戳，这样的话，9秒的数据到来之后，事件时钟不会直接推进到9秒，<br>而是进展到了7秒；必须等到11秒的数据到来之后，事件时钟才会进展到9秒，这时迟到数据也都<br>已收集齐，0~9秒的窗口就可以正确计算结果了。</li><li><div class="img-wrap"><div class="img-bg"><img class="img" src="http://localhost:4000/img/img_md/水位线5.png"/></div></div></li><li>如果仔细观察就会看到，这种“等2秒”的策略其实并不能处理所有的乱序数据。比如22秒的数据到<br>来之后，插入的水位线时间戳为20，也就是当前时钟已经推进到了20秒；对于10~20秒的窗口，<br>这时就该关闭了。但是之后又会有17秒的迟到数据到来，它本来应该属于10~20秒窗口，现在却<br>被遗漏丢弃了。那又该怎么办呢？</li><li>既然现在等2秒还是等不到17秒产生的迟到数据，那自然我们可以试着多等几秒，也就是把时钟调<br>得更慢一些。最终的目的，就是要让窗口能够把所有迟到数据都收进来，得到正确的计算结果。对<br>应到水位线上，其实就是要保证，当前时间已经进展到了这个时间戳，在这之后不可能再有迟到数<br>据来了。</li><li>第一个水位线时间戳为7，它表示当前事件时间是7秒，7秒之前的数据都已经到齐，之后再也不会<br>有了；同样，第二个、第三个水位线时间戳分别为12和20，表示11秒、20秒之前的数据都已经到<br>齐，如果有对应的窗口就可以直接关闭了，统计的结果一定是正确的。这里由于水位线是周期性生<br>成的，所以插入的位置不一定是在时间戳最大的数据后面。另外需要注意的是，这里一个窗口所收<br>集的数据，并不是之前所有已经到达的数据。因为数据属于哪个窗口，是由数据本身的时间戳决定<br>的，一个窗口只会收集真正属于它的那些数据。也就是说，上图中尽管水位线W(20)之前有时间戳<br>为22的数据到来，10~20秒的窗口中也不会收集这个数据，进行计算依然可以得到正确的结果。<br>关于窗口的原理，我们会在后面继续展开讲解。</li><li><div class="img-wrap"><div class="img-bg"><img class="img" src="http://localhost:4000/img/img_md/水位线6.png"/></div></div></li><li>水位线特点<ul><li>水位线是插入到数据流中的一个标记，可以认为是一个特殊的数据</li><li>水位线主要的内容是一个时间戳，用来表示当前事件时间的进展</li><li>水位线是基于数据的时间戳生成的</li><li>水位线的时间戳必须单调递增，以确保任务的事件时间时钟一直向前推进</li><li>水位线可以通过设置延迟，来保证正确处理乱序数据</li><li>一个水位线Watermark(t)，表示在当前流中事件时间已经达到了时间戳t, 这代表t之前的所有<br>数据都到齐了，之后流中不会出现时间戳t’≤t的数据</li></ul></li></ol></div><h2 id="内置水位线生成器"><a href="#内置水位线生成器" class="headerlink" title="内置水位线生成器"></a>内置水位线生成器</h2><div class="note white no-icon flat"><ol><li>Flink 内置水位线生成器<ul><li>1.10版本之前<ul><li>AssignerWithPeriodicWatermarks<ul><li>周期性的生成 watermark，默认周期是200ms，也可以通过<br>setAutoWatermarkInterval设置周期时间</li></ul></li><li>AssignerWithPunctuatedWatermarks<ul><li>阶段性的生成 watermark，即每来一条数据就生成一个wm</li></ul></li></ul></li><li>1.11版本以后<ul><li>WatermarkStrategy<ul><li>单调递增策略（forMonotonousTimestamps）</li><li>固定乱序长度策略（forBoundedOutOfOrderness）</li><li>不生成策略（noWatermarks）</li></ul></li></ul></li></ul></li></ol><h3 id="有序流"><a href="#有序流" class="headerlink" title="有序流"></a>有序流</h3><div class="note green no-icon flat"><ol><li>对于有序流，主要特点就是时间戳单调增长（Monotonously Increasing Timestamps），所以永<br>远不会出现迟到数据的问题。</li><li>直接调用WatermarkStrategy.forMonotonousTimestamps()方法就可以实现。</li><li>直接拿当前最大的时间戳作为水位线就可以了。</li><li>代码实现<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> com.yjxxt.util.KafkaUtil;</span><br><span class="line"><span class="keyword">import</span> org.apache.commons.lang3.RandomStringUtils;</span><br><span class="line"><span class="keyword">import</span></span><br><span class="line">org.apache.flink.api.common.eventtime.SerializableTimestampAssigner;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.eventtime.WatermarkStrategy;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.typeinfo.Types;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple3;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span></span><br><span class="line">org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span></span><br><span class="line">org.apache.flink.streaming.api.functions.windowing.WindowFunction;</span><br><span class="line"><span class="keyword">import</span></span><br><span class="line">org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.Time;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.windows.TimeWindow;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.Collector;</span><br><span class="line"><span class="keyword">import</span> java.util.Locale;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Description</span> :</span></span><br><span class="line"><span class="comment">* <span class="doctag">@School</span>:优极限学堂</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Official</span>-Website: http://www.yjxxt.com</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Teacher</span>:李毅大帝</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Mail</span>:863159469@qq.com</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Hello13WaterMarkInOrder</span> &#123;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"><span class="comment">//生产Kafka有序数据数据--模拟弹幕[用户名:消息:时间戳]</span></span><br><span class="line"><span class="keyword">new</span> <span class="title class_">Thread</span>(() -&gt; &#123;</span><br><span class="line"><span class="type">String</span> <span class="variable">uname</span> <span class="operator">=</span></span><br><span class="line">RandomStringUtils.randomAlphabetic(<span class="number">8</span>).toLowerCase(Locale.ROOT);</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">100</span>; i &lt; <span class="number">200</span>; i++) &#123;</span><br><span class="line">KafkaUtil.sendMsg(<span class="string">&quot;yjxxt&quot;</span>, uname + <span class="string">&quot;:&quot;</span> + i + <span class="string">&quot;:&quot;</span> +</span><br><span class="line">System.currentTimeMillis());</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">Thread.sleep(<span class="number">1000</span>);</span><br><span class="line">&#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">e.printStackTrace();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;).start();</span><br><span class="line"><span class="comment">//运行环境</span></span><br><span class="line"><span class="type">StreamExecutionEnvironment</span> <span class="variable">environment</span> <span class="operator">=</span></span><br><span class="line">StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">environment.setParallelism(<span class="number">1</span>);</span><br><span class="line"><span class="comment">//读取数据源</span></span><br><span class="line">DataStreamSource&lt;String&gt; source =</span><br><span class="line">environment.fromSource(KafkaUtil.getKafkaSource(<span class="string">&quot;yjxxt&quot;</span>, <span class="string">&quot;liyidd&quot;</span>),</span><br><span class="line">WatermarkStrategy.noWatermarks(), <span class="string">&quot;Kafka Source&quot;</span>);</span><br><span class="line"><span class="comment">//转换数据</span></span><br><span class="line">source.map(line -&gt; &#123;</span><br><span class="line"><span class="keyword">return</span> Tuple3.of(line.split(<span class="string">&quot;:&quot;</span>)[<span class="number">0</span>],</span><br><span class="line">line.split(<span class="string">&quot;:&quot;</span>)[<span class="number">1</span>], Long.parseLong(line.split(<span class="string">&quot;:&quot;</span>)[<span class="number">2</span>]));</span><br><span class="line">&#125;, Types.TUPLE(Types.STRING, Types.STRING, Types.LONG))</span><br><span class="line">.assignTimestampsAndWatermarks(WatermarkStrategy.</span><br><span class="line">&lt;Tuple3&lt;String, String, Long&gt;&gt;forMonotonousTimestamps()</span><br><span class="line">.withTimestampAssigner(<span class="keyword">new</span></span><br><span class="line"><span class="title class_">SerializableTimestampAssigner</span>&lt;Tuple3&lt;String, String, Long&gt;&gt;() &#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="type">long</span> <span class="title function_">extractTimestamp</span><span class="params">(Tuple3&lt;String,</span></span><br><span class="line"><span class="params">String, Long&gt; tuple3, <span class="type">long</span> ts)</span> &#123;</span><br><span class="line"><span class="keyword">return</span> tuple3.f2;</span><br><span class="line">&#125;&#125;))</span><br><span class="line">.keyBy(tuple3 -&gt; tuple3.f0)</span><br><span class="line">.window(TumblingEventTimeWindows.of(Time.seconds(<span class="number">10</span>)))</span><br><span class="line">.apply(<span class="keyword">new</span> <span class="title class_">WindowFunction</span>&lt;Tuple3&lt;String, String, Long&gt;,</span><br><span class="line">String, String, TimeWindow&gt;() &#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">apply</span><span class="params">(String key, TimeWindow window,</span></span><br><span class="line"><span class="params">Iterable&lt;Tuple3&lt;String, String, Long&gt;&gt; input, Collector&lt;String&gt; out)</span></span><br><span class="line"><span class="keyword">throws</span> Exception &#123;</span><br><span class="line"><span class="type">StringBuffer</span> <span class="variable">buffer</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StringBuffer</span>();</span><br><span class="line">buffer.append(<span class="string">&quot;[key]&quot;</span>);</span><br><span class="line"><span class="keyword">for</span> (Tuple3&lt;String, String, Long&gt; tuple3 : input)</span><br><span class="line">&#123;</span><br><span class="line">buffer.append(<span class="string">&quot;[&quot;</span> + tuple3.f1 + <span class="string">&quot;_&quot;</span> +</span><br><span class="line">tuple3.f2 + <span class="string">&quot;]&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line">buffer.append(<span class="string">&quot;[&quot;</span> + window + <span class="string">&quot;]&quot;</span>);</span><br><span class="line"><span class="comment">//返回结果</span></span><br><span class="line">out.collect(buffer.toString());</span><br><span class="line">&#125;</span><br><span class="line">&#125;).print();</span><br><span class="line"><span class="comment">//运行环境</span></span><br><span class="line">environment.execute();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol></div><h3 id="无序流"><a href="#无序流" class="headerlink" title="无序流"></a>无序流</h3><div class="note green no-icon flat"><ol><li>由于乱序流中需要等待迟到数据到齐，所以必须设置一个固定量的延迟时间（Fixed Amount of<br>Lateness）。</li><li>调用 WatermarkStrategy. forBoundedOutOfOrderness()方法就可以实现。</li><li>这个方法需要传入一个 maxOutOfOrderness 参数，表示“最大乱序程度”</li><li>Tips：<ul><li>当程序开始时,WaterMark会被设置为Long的最小值,以保证它不会丢数据</li><li>当程序关闭时,WaterMark会被设置为Long的最大值,以保证它大到足以关闭所有已经开启的<br>窗口</li></ul></li><li>代码实现<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> com.yjxxt.util.KafkaUtil;</span><br><span class="line"><span class="keyword">import</span> org.apache.commons.lang3.RandomStringUtils;</span><br><span class="line"><span class="keyword">import</span></span><br><span class="line">org.apache.flink.api.common.eventtime.SerializableTimestampAssigner;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.eventtime.WatermarkStrategy;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.typeinfo.Types;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple3;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span></span><br><span class="line">org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span></span><br><span class="line">org.apache.flink.streaming.api.functions.windowing.WindowFunction;</span><br><span class="line"><span class="keyword">import</span></span><br><span class="line">org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.Time;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.windows.TimeWindow;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.Collector;</span><br><span class="line"><span class="keyword">import</span> java.time.Duration;</span><br><span class="line"><span class="keyword">import</span> java.util.Locale;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Description</span> :</span></span><br><span class="line"><span class="comment">* <span class="doctag">@School</span>:优极限学堂</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Official</span>-Website: http://www.yjxxt.com</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Teacher</span>:李毅大帝</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Mail</span>:863159469@qq.com</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Hello14WaterMarkOutOrder</span> &#123;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"><span class="comment">//生产Kafka有序数据数据--模拟弹幕[用户名:消息:时间戳]</span></span><br><span class="line"><span class="keyword">new</span> <span class="title class_">Thread</span>(() -&gt; &#123;</span><br><span class="line"><span class="type">String</span> <span class="variable">uname</span> <span class="operator">=</span></span><br><span class="line">RandomStringUtils.randomAlphabetic(<span class="number">8</span>).toLowerCase(Locale.ROOT);</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">100</span>; i &lt; <span class="number">200</span>; i++) &#123;</span><br><span class="line"><span class="keyword">if</span> (i % <span class="number">5</span> != <span class="number">0</span>) &#123;</span><br><span class="line">KafkaUtil.sendMsg(<span class="string">&quot;yjxxt&quot;</span>, uname + <span class="string">&quot;:&quot;</span> + i + <span class="string">&quot;:&quot;</span> +</span><br><span class="line">System.currentTimeMillis());</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">KafkaUtil.sendMsg(<span class="string">&quot;yjxxt&quot;</span>, uname + <span class="string">&quot;:&quot;</span> + i + <span class="string">&quot;:&quot;</span> +</span><br><span class="line">(System.currentTimeMillis() - (<span class="type">long</span>) (Math.random() * <span class="number">10000</span>)));</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">Thread.sleep(<span class="number">1000</span>);</span><br><span class="line">&#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">e.printStackTrace();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;).start();</span><br><span class="line"><span class="comment">//运行环境</span></span><br><span class="line"><span class="type">StreamExecutionEnvironment</span> <span class="variable">environment</span> <span class="operator">=</span></span><br><span class="line">StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">environment.setParallelism(<span class="number">1</span>);</span><br><span class="line"><span class="comment">//读取数据源</span></span><br><span class="line">DataStreamSource&lt;String&gt; source =</span><br><span class="line">environment.fromSource(KafkaUtil.getKafkaSource(<span class="string">&quot;yjxxt&quot;</span>, <span class="string">&quot;liyidd&quot;</span>),</span><br><span class="line">WatermarkStrategy.noWatermarks(), <span class="string">&quot;Kafka Source&quot;</span>);</span><br><span class="line"><span class="comment">//转换数据</span></span><br><span class="line">source.map(line -&gt; &#123;</span><br><span class="line"><span class="keyword">return</span> Tuple3.of(line.split(<span class="string">&quot;:&quot;</span>)[<span class="number">0</span>],</span><br><span class="line">line.split(<span class="string">&quot;:&quot;</span>)[<span class="number">1</span>], Long.parseLong(line.split(<span class="string">&quot;:&quot;</span>)[<span class="number">2</span>]));</span><br><span class="line">&#125;, Types.TUPLE(Types.STRING, Types.STRING, Types.LONG))</span><br><span class="line">.assignTimestampsAndWatermarks(WatermarkStrategy.</span><br><span class="line">&lt;Tuple3&lt;String, String,</span><br><span class="line">Long&gt;&gt;forBoundedOutOfOrderness(Duration.ofSeconds(<span class="number">8</span>))</span><br><span class="line">.withTimestampAssigner(<span class="keyword">new</span></span><br><span class="line"><span class="title class_">SerializableTimestampAssigner</span>&lt;Tuple3&lt;String, String, Long&gt;&gt;() &#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="type">long</span> <span class="title function_">extractTimestamp</span><span class="params">(Tuple3&lt;String,</span></span><br><span class="line"><span class="params">String, Long&gt; tuple3, <span class="type">long</span> ts)</span> &#123;</span><br><span class="line"><span class="keyword">return</span> tuple3.f2;</span><br><span class="line">&#125;</span><br><span class="line">&#125;))</span><br><span class="line">.keyBy(tuple3 -&gt; tuple3.f0)</span><br><span class="line">.window(TumblingEventTimeWindows.of(Time.seconds(<span class="number">10</span>)))</span><br><span class="line">.apply(<span class="keyword">new</span> <span class="title class_">WindowFunction</span>&lt;Tuple3&lt;String, String, Long&gt;,</span><br><span class="line">String, String, TimeWindow&gt;() &#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">apply</span><span class="params">(String key, TimeWindow window,</span></span><br><span class="line"><span class="params">Iterable&lt;Tuple3&lt;String, String, Long&gt;&gt; input, Collector&lt;String&gt; out)</span></span><br><span class="line"><span class="keyword">throws</span> Exception &#123;</span><br><span class="line"><span class="type">StringBuffer</span> <span class="variable">buffer</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StringBuffer</span>();</span><br><span class="line">buffer.append(<span class="string">&quot;[key]&quot;</span>);</span><br><span class="line"><span class="keyword">for</span> (Tuple3&lt;String, String, Long&gt; tuple3 :</span><br><span class="line">input) &#123;</span><br><span class="line">buffer.append(<span class="string">&quot;[&quot;</span> + tuple3.f1 + <span class="string">&quot;_&quot;</span> +</span><br><span class="line">tuple3.f2 + <span class="string">&quot;]&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line">buffer.append(<span class="string">&quot;[&quot;</span> + window + <span class="string">&quot;]&quot;</span>);</span><br><span class="line"><span class="comment">//返回结果</span></span><br><span class="line">out.collect(buffer.toString());</span><br><span class="line">&#125;</span><br><span class="line">&#125;).print();</span><br><span class="line"><span class="comment">//运行环境</span></span><br><span class="line">environment.execute();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol></div></div><h2 id="自定义水位线"><a href="#自定义水位线" class="headerlink" title="自定义水位线"></a>自定义水位线</h2><div class="note white no-icon flat"><ol><li>Flink有两种不同的生成水位线的方式：一种是周期性的（Periodic），另一种是定点式的<br>（Punctuated）。<ul><li>周期性的（Periodic）：周期性调用的方法中发出水位线<ul><li>Periodic Generator</li><li>周期性生成器一般是通过 onEvent()观察判断输入的事件，而在 onPeriodicEmit()里发<br>出水位线</li></ul></li><li>定点式的（Punctuated）：在事件触发的方法中发出水位线<ul><li>Punctuated Generator</li><li>定点式生成器会不停地检测 onEvent()中的事件，当发现带有水位线信息的特殊事件<br>时，就立即发出水位线。</li></ul></li></ul></li><li>WatermarkGenerator 接口中有两个方法<ul><li>onEvent()：在每个事件到来时调用</li><li>onPeriodicEmit()：由框架周期性调用</li></ul></li></ol><h3 id="WatermarkGenerator"><a href="#WatermarkGenerator" class="headerlink" title="WatermarkGenerator"></a>WatermarkGenerator</h3><div class="note green no-icon flat"><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> com.yjxxt.util.KafkaUtil;</span><br><span class="line"><span class="keyword">import</span> org.apache.commons.lang3.RandomStringUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.eventtime.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.SimpleStringSchema;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.connector.kafka.source.KafkaSource;</span><br><span class="line"><span class="keyword">import</span></span><br><span class="line">org.apache.flink.connector.kafka.source.enumerator.initializer.OffsetsInitializer;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span></span><br><span class="line">org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span></span><br><span class="line">org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.Time;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Description</span> :</span></span><br><span class="line"><span class="comment">* <span class="doctag">@School</span>:优极限学堂</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Official</span>-Website: http://www.yjxxt.com</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Teacher</span>:李毅大帝</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Mail</span>:863159469@qq.com</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Hello12WatermarkGenerator</span> &#123;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"><span class="comment">//启动一个线程专门发送消息给Kafka，这样我们才有数据消费</span></span><br><span class="line"><span class="keyword">new</span> <span class="title class_">Thread</span>(() -&gt; &#123;</span><br><span class="line"><span class="type">String</span> <span class="variable">uname</span> <span class="operator">=</span> RandomStringUtils.randomAlphabetic(<span class="number">8</span>);</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">1000</span>; i &lt; <span class="number">2000</span>; i++) &#123;</span><br><span class="line">KafkaUtil.sendMsg(<span class="string">&quot;yjxxt&quot;</span>, uname + i % <span class="number">2</span> + <span class="string">&quot;:&quot;</span> + i + <span class="string">&quot;:&quot;</span> +</span><br><span class="line">System.currentTimeMillis());</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">Thread.sleep(<span class="number">100</span>);</span><br><span class="line">&#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">e.printStackTrace();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;).start();</span><br><span class="line"><span class="comment">//获取环境</span></span><br><span class="line"><span class="type">StreamExecutionEnvironment</span> <span class="variable">environment</span> <span class="operator">=</span></span><br><span class="line">StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">environment.setParallelism(<span class="number">1</span>);</span><br><span class="line"><span class="comment">//设置Kafka连接</span></span><br><span class="line">KafkaSource&lt;String&gt; source = KafkaSource.&lt;String&gt;builder()</span><br><span class="line">.setBootstrapServers(<span class="string">&quot;node01:9092,node02:9092,node03:9092&quot;</span>)</span><br><span class="line">.setTopics(<span class="string">&quot;yjxxt&quot;</span>)</span><br><span class="line">.setGroupId(<span class="string">&quot;flink_KafkaConnector&quot;</span>)</span><br><span class="line">.setStartingOffsets(OffsetsInitializer.latest())</span><br><span class="line">.setValueOnlyDeserializer(<span class="keyword">new</span> <span class="title class_">SimpleStringSchema</span>())</span><br><span class="line">.build();</span><br><span class="line"><span class="comment">//读取数据源</span></span><br><span class="line">DataStreamSource&lt;String&gt; kafkaSource =</span><br><span class="line">environment.fromSource(source, WatermarkStrategy.noWatermarks(),</span><br><span class="line"><span class="string">&quot;KafkaSource&quot;</span>);</span><br><span class="line">kafkaSource.assignTimestampsAndWatermarks(<span class="keyword">new</span></span><br><span class="line"><span class="title class_">YjxxtWatermarkStrategy</span>())</span><br><span class="line">.keyBy(t -&gt; t.split(<span class="string">&quot;:&quot;</span>)[<span class="number">0</span>])</span><br><span class="line">.window(TumblingEventTimeWindows.of(Time.seconds(<span class="number">5</span>)))</span><br><span class="line">.reduce((t1, t2) -&gt; t1 + <span class="string">&quot;[&quot;</span> + t2.split(<span class="string">&quot;:&quot;</span>)[<span class="number">1</span>] + <span class="string">&quot;,&quot;</span> +</span><br><span class="line">t2.split(<span class="string">&quot;:&quot;</span>)[<span class="number">2</span>] + <span class="string">&quot;]&quot;</span>)</span><br><span class="line">.map(t -&gt; <span class="string">&quot;[&quot;</span> + System.currentTimeMillis() + <span class="string">&quot;][&quot;</span> + t + <span class="string">&quot;]&quot;</span>)</span><br><span class="line">.print(<span class="string">&quot;YjxxtWatermarkStrategy&quot;</span>);</span><br><span class="line"><span class="comment">//执行环境</span></span><br><span class="line">environment.execute();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">YjxxtWatermarkStrategy</span> <span class="keyword">implements</span> <span class="title class_">WatermarkStrategy</span>&lt;String&gt; &#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> WatermarkGenerator&lt;String&gt;</span><br><span class="line"><span class="title function_">createWatermarkGenerator</span><span class="params">(WatermarkGeneratorSupplier.Context context)</span> &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">YjxxtPeriodicGenerator</span>();</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> TimestampAssigner&lt;String&gt;</span><br><span class="line"><span class="title function_">createTimestampAssigner</span><span class="params">(TimestampAssignerSupplier.Context context)</span> &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">SerializableTimestampAssigner</span>&lt;String&gt;() &#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="type">long</span> <span class="title function_">extractTimestamp</span><span class="params">(String element, <span class="type">long</span></span></span><br><span class="line"><span class="params">recordTimestamp)</span> &#123;</span><br><span class="line"><span class="keyword">return</span> Long.valueOf(element.split(<span class="string">&quot;:&quot;</span>)[<span class="number">2</span>]);</span><br><span class="line">&#125;</span><br><span class="line">&#125;;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* 周期型生成水位线</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">YjxxtPeriodicGenerator</span> <span class="keyword">implements</span></span><br><span class="line"><span class="title class_">WatermarkGenerator</span>&lt;String&gt; &#123;</span><br><span class="line"><span class="comment">// 延迟时间</span></span><br><span class="line"><span class="keyword">private</span> <span class="type">Long</span> <span class="variable">lateTime</span> <span class="operator">=</span> <span class="number">3000L</span>;</span><br><span class="line"><span class="comment">// 观察到的最大时间戳</span></span><br><span class="line"><span class="keyword">private</span> <span class="type">Long</span> <span class="variable">maxTimestamp</span> <span class="operator">=</span> Long.MIN_VALUE;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onEvent</span><span class="params">(String element, <span class="type">long</span> eventTimestamp,</span></span><br><span class="line"><span class="params">WatermarkOutput output)</span> &#123;</span><br><span class="line">maxTimestamp = Math.max(Long.valueOf(element.split(<span class="string">&quot;:&quot;</span>)[<span class="number">2</span>]),</span><br><span class="line">maxTimestamp); <span class="comment">//</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onPeriodicEmit</span><span class="params">(WatermarkOutput output)</span> &#123;</span><br><span class="line">output.emitWatermark(<span class="keyword">new</span> <span class="title class_">Watermark</span>(maxTimestamp - lateTime -</span><br><span class="line"><span class="number">1L</span>));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* 定点型生成水位线</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">YjxxtPunctuatedGenerator</span> <span class="keyword">implements</span></span><br><span class="line"><span class="title class_">WatermarkGenerator</span>&lt;String&gt; &#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onEvent</span><span class="params">(String element, <span class="type">long</span> eventTimestamp,</span></span><br><span class="line"><span class="params">WatermarkOutput output)</span> &#123;</span><br><span class="line"><span class="keyword">if</span> (Integer.parseInt(element.split(<span class="string">&quot;:&quot;</span>)[<span class="number">1</span>]) % <span class="number">100</span> == <span class="number">0</span>) &#123;</span><br><span class="line">output.emitWatermark(<span class="keyword">new</span></span><br><span class="line"><span class="title class_">Watermark</span>(Long.valueOf(element.split(<span class="string">&quot;:&quot;</span>)[<span class="number">2</span>]) - <span class="number">1L</span>));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onPeriodicEmit</span><span class="params">(WatermarkOutput output)</span> &#123;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div></div>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink Window Functions</title>
      <link href="/2023/07/29/flink/FlinkWindowFunction/"/>
      <url>/2023/07/29/flink/FlinkWindowFunction/</url>
      
        <content type="html"><![CDATA[<h1 id="Flink-Window-Functions"><a href="#Flink-Window-Functions" class="headerlink" title="Flink Window Functions"></a>Flink Window Functions</h1><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><div class="note white no-icon flat"><ul><li>定义了窗口分配器，我们知道了数据属于哪个窗口，可以将数据收集起来了；至于收集起来到底要<br>做什么，其实还完全没有头绪。</li><li>所以在窗口分配器之后，必须再接上一个定义窗口如何进行计算的操作，这就是所谓的“窗口函数”（window functions）</li><li>Flink提供了两大类窗口函数，分别为增量聚合函数和全量窗口函数。<ul><li>增量聚合函数(incremental aggregation functions）<ul><li>窗口将数据收集起来，最基本的处理操作当然就是进行聚合。窗口对无限流的切分，可<br>以看作得到了一个有界数据集。如果我们等到所有数据都收集齐，在窗口到了结束时间<br>要输出结果的一瞬间再去进行聚合，显然就不够高效</li><li>为了提高实时性，我们可以再次将流处理的思路发扬光大：就像 DataStream 的简单聚<br>合一样，每来一条数据就立即进行计算，中间只要保持一个简单的聚合状态就可以了；<br>区别只是在于不立即输出结果，而是要等到窗口结束时间。等到窗口到了结束时间需要<br>输出计算结果的时候，我们只需要拿出之前聚合的状态直接输出，这无疑就大大提高了<br>程序运行的效率和实时性。</li><li>典型的增量聚合函数有ReduceFunction、AggregateFunction。</li></ul></li><li>全窗口聚合函数(full window functions)<ul><li>典型的批处理思路了—养肥了再杀</li><li>全量窗口函数需要对所有进入该窗口的数据进行缓存，等到窗口触发时才会遍历窗口内<br>所有数据，进行结果计算。</li><li>因为有些场景下，我们要做的计算必须基于全部的数据才有效，这时做增量聚合就没什<br>么意义了；另外，输出的结果有可能要包含上下文中的一些信息（比如窗口的起始时<br>间），这是增量聚合函数做不到的。所以，我们还需要有更丰富的窗口计算方式，这就<br>可以用全窗口函数来实现。</li><li>全窗口函数也有两种：WindowFunction 和 ProcessWindowFunction。</li></ul></li></ul></li></ul></div><h2 id="增量聚合函数"><a href="#增量聚合函数" class="headerlink" title="增量聚合函数"></a>增量聚合函数</h2><div class="note white no-icon flat"><h3 id="ReduceFunction"><a href="#ReduceFunction" class="headerlink" title="ReduceFunction"></a>ReduceFunction</h3><div class="note green no-icon flat"><ul><li>最基本的聚合方式就是归约（reduce）</li><li>窗口函数中也提供了 ReduceFunction：只要基于 WindowedStream 调用.reduce()方法，然后传<br>入 ReduceFunction 作为参数，就可以指定以归约两个元素的方式去对窗口中数据进行聚合了。</li><li>ReduceFunction 可以解决大多数归约聚合的问题，但是这个接口有一个限制，就是聚合状态的类<br>型、输出结果的类型都必须和输入数据类型一样。</li><li>代码实现<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.typeinfo.Types;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span></span><br><span class="line">org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Description</span> :</span></span><br><span class="line"><span class="comment">* <span class="doctag">@School</span>:优极限学堂</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Official</span>-Website: http://www.yjxxt.com</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Teacher</span>:李毅大帝</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Mail</span>:863159469@qq.com</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Hello09WindowFunctionByReduce</span> &#123;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"><span class="comment">//运行环境</span></span><br><span class="line"><span class="type">StreamExecutionEnvironment</span> <span class="variable">environment</span> <span class="operator">=</span></span><br><span class="line">StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"><span class="comment">//获取数据源-admin:3</span></span><br><span class="line">DataStreamSource&lt;String&gt; source =</span><br><span class="line">environment.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">19523</span>);</span><br><span class="line"><span class="comment">//CountWindow--Tumbling--增量计算</span></span><br><span class="line">source.map(word -&gt; Tuple2.of(word.split(<span class="string">&quot;:&quot;</span>)[<span class="number">0</span>],</span><br><span class="line">Integer.parseInt(word.split(<span class="string">&quot;:&quot;</span>)[<span class="number">1</span>])), Types.TUPLE(Types.STRING,</span><br><span class="line">Types.INT))</span><br><span class="line">.keyBy(tuple2 -&gt; tuple2.f0)</span><br><span class="line">.countWindow(<span class="number">3</span>)</span><br><span class="line">.reduce((t1, t2) -&gt; &#123;</span><br><span class="line">System.out.println(<span class="string">&quot;窗口增量计算函数-来一条算一条.main[&quot;</span></span><br><span class="line">+ t1 + <span class="string">&quot;][&quot;</span> + t2 + <span class="string">&quot;]&quot;</span>);</span><br><span class="line">t1.f0 = t1.f0 + <span class="string">&quot;_&quot;</span> + t2.f0;</span><br><span class="line">t1.f1 = t1.f1 + t2.f1;</span><br><span class="line"><span class="keyword">return</span> t1;</span><br><span class="line">&#125;).print(<span class="string">&quot;CountWindow--Tumbling:&quot;</span>).setParallelism(<span class="number">1</span>);</span><br><span class="line"><span class="comment">//运行环境</span></span><br><span class="line">environment.execute();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></div><h3 id="AggregateFunction"><a href="#AggregateFunction" class="headerlink" title="AggregateFunction"></a>AggregateFunction</h3><div class="note green no-icon flat"><ul><li>ReduceFunction 可以解决大多数归约聚合的问题，但是这个接口有一个限制，就是聚合状态的类<br>型、输出结果的类型都必须和输入数据类型一样。这就迫使我们必须在聚合前，先将数据转换<br>（map）成预期结果类型；而在有些情况下，还需要对状态进行进一步处理才能得到输出结果，<br>这时它们的类型可能不同，使用 ReduceFunction 就会非常麻烦。</li><li>例如，如果我们希望计算一组数据的平均值，应该怎样做聚合呢？很明显，这时我们需要计算两个<br>状态量：数据的总和（sum），以及数据的个数（count），而最终输出结果是两者的商<br>（sum/count）。如果用 ReduceFunction，那么我们应该先把数据转换成二元组(sum, count)的<br>形式，然后进行归约聚合，最后再将元组的两个元素相除转换得到最后的平均值。本来应该只是一<br>个任务，可我们却需要 map-reduce-map 三步操作，这显然不够高效。</li><li>AggregateFunction 可以看作是 ReduceFunction 的通用版本，这里有三种类型：<ul><li>输入类型（IN）、累加器类型（ACC）和输出类型（OUT）。</li><li>输入类型 IN 就是输入流中元素的数据类型；</li><li>累加器类型 ACC 则是我们进行聚合的中间状态类型；</li><li>而输出类型当然就是最终计算结果的类型了。</li></ul></li><li>接口中有四个方法：<ul><li>createAccumulator()：<ul><li>创建一个累加器，这就是为聚合创建了一个初始状态，每个聚合任务只会调用一次。</li></ul></li><li>add()：<ul><li>将输入的元素添加到累加器中。这就是基于聚合状态，对新来的数据进行进一步聚合的<br>过程。方法传入两个参数：</li><li>当前新到的数据 value，和当前的累加器accumulator；</li><li>返回一个新的累加器值，也就是对聚合状态进行更新。每条数据到来之后都会调用这个<br>方法。</li></ul></li><li>getResult()：<ul><li>从累加器中提取聚合的输出结果。也就是说，我们可以定义多个状态，然后再基于这些<br>聚合的状态计算出一个结果进行输出。</li><li>比如之前我们提到的计算平均值，就可以把 sum 和 count 作为状态放入累加器，而在<br>调用这个方法时相除得到最终结果。</li><li>这个方法只在窗口要输出结果时调用。</li></ul></li><li>merge()：<ul><li>合并两个累加器，并将合并后的状态作为一个累加器返回。这个方法只在需要合并窗口<br>的场景下才会被调用；</li><li>最常见的合并窗口（Merging Window）的场景就是会话窗口（Session Windows）。</li></ul></li></ul></li><li>代码实现<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.AggregateFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.typeinfo.Types;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span></span><br><span class="line">org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Description</span> :</span></span><br><span class="line"><span class="comment">* <span class="doctag">@School</span>:优极限学堂</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Official</span>-Website: http://www.yjxxt.com</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Teacher</span>:李毅大帝</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Mail</span>:863159469@qq.com</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Hello10WindowFunctionsByAggregate</span> &#123;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"><span class="comment">//运行环境</span></span><br><span class="line"><span class="type">StreamExecutionEnvironment</span> <span class="variable">environment</span> <span class="operator">=</span></span><br><span class="line">StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"><span class="comment">//获取数据源-admin:3</span></span><br><span class="line">DataStreamSource&lt;String&gt; source =</span><br><span class="line">environment.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">19523</span>);</span><br><span class="line"><span class="comment">//CountWindow--Tumbling--增量计算</span></span><br><span class="line">source.map(word -&gt; Tuple2.of(word.split(<span class="string">&quot;:&quot;</span>)[<span class="number">0</span>],</span><br><span class="line">Integer.parseInt(word.split(<span class="string">&quot;:&quot;</span>)[<span class="number">1</span>])), Types.TUPLE(Types.STRING,</span><br><span class="line">Types.INT))</span><br><span class="line">.keyBy(tuple2 -&gt; tuple2.f0)</span><br><span class="line">.countWindow(<span class="number">3</span>)</span><br><span class="line">.aggregate(<span class="keyword">new</span> <span class="title class_">AggregateFunction</span>&lt;Tuple2&lt;String,</span><br><span class="line">Integer&gt;, Tuple2&lt;Integer, Integer&gt;, Double&gt;() &#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> Tuple2&lt;Integer, Integer&gt; <span class="title function_">createAccumulator</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="comment">//初始化累加器</span></span><br><span class="line"><span class="keyword">return</span> Tuple2.of(<span class="number">0</span>, <span class="number">0</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> Tuple2&lt;Integer, Integer&gt; <span class="title function_">add</span><span class="params">(Tuple2&lt;String,</span></span><br><span class="line"><span class="params">Integer&gt; in, Tuple2&lt;Integer, Integer&gt; acc)</span> &#123;</span><br><span class="line">acc.f0 = acc.f0 + in.f1;</span><br><span class="line">acc.f1 += <span class="number">1</span>;</span><br><span class="line"><span class="keyword">return</span> acc;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> Double <span class="title function_">getResult</span><span class="params">(Tuple2&lt;Integer, Integer&gt;</span></span><br><span class="line"><span class="params">acc)</span> &#123;</span><br><span class="line"><span class="comment">//判断除数不能为0</span></span><br><span class="line"><span class="keyword">if</span> (acc.f1 == <span class="number">0</span>) &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="number">0.0</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> acc.f0 * <span class="number">1.0</span> / acc.f1;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> Tuple2&lt;Integer, Integer&gt;</span><br><span class="line"><span class="title function_">merge</span><span class="params">(Tuple2&lt;Integer, Integer&gt; integerIntegerTuple2, Tuple2&lt;Integer,</span></span><br><span class="line"><span class="params">Integer&gt; acc1)</span> &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;)</span><br><span class="line">.print(<span class="string">&quot;CountWindow--Tumbling:&quot;</span>).setParallelism(<span class="number">1</span>);</span><br><span class="line"><span class="comment">//运行环境</span></span><br><span class="line">environment.execute();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></div></div><h2 id="全量窗口函数"><a href="#全量窗口函数" class="headerlink" title="全量窗口函数"></a>全量窗口函数</h2><div class="note white no-icon flat"><h3 id="ProcessWindowFunction"><a href="#ProcessWindowFunction" class="headerlink" title="ProcessWindowFunction"></a>ProcessWindowFunction</h3><div class="note green no-icon flat"><ul><li>ProcessWindowFunction 是 Window API 中最底层的通用窗口函数接口。之所以说它“最底层”，<br>是因为除了可以拿到窗口中的所有数据之外，ProcessWindowFunction 还可以获取到一个“上下<br>文对象”（Context）。</li><li>上下文对象非常强大，不仅能够获取窗口信息，还可以访问当前的时间和状态信息。这里的时间就<br>包括了处理时间（processing time）和事件时间水位线（eventtime watermark）。</li><li>全量窗口的好处是以牺牲性能和资源为代价的。作为一个全窗口函数，ProcessWindowFunction<br>同样需要将所有数据缓存下来、等到窗口触发计算时才使用。它其实就是一个增强版的<br>WindowFunction。</li><li>代码实现<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.typeinfo.Types;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple3;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span></span><br><span class="line">org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span></span><br><span class="line">org.apache.flink.streaming.api.functions.windowing.WindowFunction;</span><br><span class="line"><span class="keyword">import</span></span><br><span class="line">org.apache.flink.streaming.api.windowing.assigners.SlidingProcessingTimeWindows;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.Time;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.windows.TimeWindow;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.Collector;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Description</span> :</span></span><br><span class="line"><span class="comment">* <span class="doctag">@School</span>:优极限学堂</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Official</span>-Website: http://www.yjxxt.com</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Teacher</span>:李毅大帝</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Mail</span>:863159469@qq.com</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Hello12WindowFunctionsByWindow</span> &#123;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"><span class="comment">//运行环境</span></span><br><span class="line"><span class="type">StreamExecutionEnvironment</span> <span class="variable">environment</span> <span class="operator">=</span></span><br><span class="line">StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"><span class="comment">//获取数据源-admin:3</span></span><br><span class="line">DataStreamSource&lt;String&gt; source =</span><br><span class="line">environment.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">19523</span>);</span><br><span class="line"><span class="comment">//TimeWindow--Sliding</span></span><br><span class="line">source.map(word -&gt; Tuple2.of(word.split(<span class="string">&quot;:&quot;</span>)[<span class="number">0</span>],</span><br><span class="line">Integer.parseInt(word.split(<span class="string">&quot;:&quot;</span>)[<span class="number">1</span>])), Types.TUPLE(Types.STRING,</span><br><span class="line">Types.INT))</span><br><span class="line">.keyBy(tuple2 -&gt; tuple2.f0)</span><br><span class="line">.window(SlidingProcessingTimeWindows.of(Time.seconds(<span class="number">5</span>),</span><br><span class="line">Time.seconds(<span class="number">5</span>)))</span><br><span class="line">.apply(<span class="keyword">new</span> <span class="title class_">WindowFunction</span>&lt;Tuple2&lt;String, Integer&gt;,</span><br><span class="line">Tuple3&lt;String, Integer, String&gt;, String, TimeWindow&gt;() &#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">apply</span><span class="params">(String s, TimeWindow window,</span></span><br><span class="line"><span class="params">Iterable&lt;Tuple2&lt;String, Integer&gt;&gt; input, Collector&lt;Tuple3&lt;String,</span></span><br><span class="line"><span class="params">Integer, String&gt;&gt; out)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"><span class="comment">//计算总和</span></span><br><span class="line"><span class="type">int</span> <span class="variable">sum</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (Tuple2&lt;String, Integer&gt; tuple2 : input) &#123;</span><br><span class="line">sum += tuple2.f1;</span><br><span class="line">&#125;</span><br><span class="line">out.collect(Tuple3.of(s, sum,</span><br><span class="line">window.toString()));</span><br><span class="line">&#125;</span><br><span class="line">&#125;)</span><br><span class="line">.print(<span class="string">&quot;TimeWindow--Sliding:&quot;</span>).setParallelism(<span class="number">1</span>);</span><br><span class="line"><span class="comment">//运行环境</span></span><br><span class="line">environment.execute();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></div></div>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>窗口函数</title>
      <link href="/2023/07/29/flink/%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0/"/>
      <url>/2023/07/29/flink/%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="Flink窗口函数"><a href="#Flink窗口函数" class="headerlink" title="Flink窗口函数"></a>Flink窗口函数</h1><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><div class="note white no-icon flat"><div class="img-wrap"><div class="img-bg"><img class="img" src="http://localhost:4000/img/img_md/窗口函数.png"/></div></div><ul><li>在流处理应用中，数据是连续不断的，因此我们不可能等到所有数据都到了才开始处理。当然我们<br>可以每来一个消息就处理一次，但是有时我们需要做一些聚合类的处理，例如：在过去的1分钟内<br>有多少用户点击了我们的网页。在这种情况下，我们必须定义一个窗口，用来收集最近一分钟内的<br>数据，并对这个窗口内的数据进行计算.</li><li>Windows是flink处理无限流的核心,Windows将流拆分为有限大小的“桶”，我们可以在其上应用计<br>算。Flink 认为 Batch 是 Streaming 的一个特例，所以 Flink 底层引擎是一个流式引擎，在上面实<br>现了流处理和批处理。而窗口（window）就是从 Streaming 到 Batch 的一个桥梁。Flink 提供了<br>非常完善的窗口机制.</li><li>窗口分类：<ul><li><div class="img-wrap"><div class="img-bg"><img class="img" src="http://localhost:4000/img/img_md/窗口分类.png"/></div></div></li><li>基于时间划分驱动（Time Window）例如：每30秒钟</li><li>基于数据数量驱动（Count Window）例如：每一百个元素，与时间无关。</li></ul></li></ul></div><h2 id="keyed-amp-non-keyed"><a href="#keyed-amp-non-keyed" class="headerlink" title="keyed&amp;non-keyed"></a>keyed&amp;non-keyed</h2><div class="note white no-icon flat"><ul><li>Flink 窗口在 keyed streams 和 non-keyed streams 上使用的基本结构：<ul><li>keyed streams 要调用 keyBy(…) 后再调用 window(…)</li><li>non-keyed streams 只用直接调用 windowAll(…) 。</li><li><div class="img-wrap"><div class="img-bg"><img class="img" src="http://localhost:4000/img/img_md/keyword_windows.png"/></div></div></li><li><details class="folding-tag" green no-icon><summary> 概述 </summary>          <div class='content'>          <p>在实际案例中Keyed Window 使用最多,所以我们需要掌握Keyed Window的算子,在每个<br>窗口算子中包含了<br>Windows Assigner、Windows Trigger(窗口触发器)、Evictor(数据剔除器)、<br>Lateness(时延设定)、<br>Output (输出标签)以及Windows Function,其中Windows Assigner和Windows<br>Functions是所有窗口算子<br>必须指定的属性,其余的属性都是根据实际情况选择指定.<br>code:<br>stream.keyBy(…)是Keyed类型数据集<br>.window(…)//指定窗口分配器类型<br>[.trigger(…)]//指定触发器类型(可选)<br>[.evictor(…)] // 指定evictor或者不指定(可选)<br>[.allowedLateness(…)] //指定是否延迟处理数据(可选)<br>[.sideOutputLateData(…)] // 指定Output lag(可选)<br>.reduce/aggregate/fold/apply() //指定窗口计算函数<br>[.getSideOutput(…)] //根据Tag输出数据(可选)<br>intro:<br>Windows Assigner : 指定窗口的类型,定义如何将数据流分配到一个或多个窗口<br>Windows Trigger : 指定窗口触发的时机,定义窗口满足什么样的条件触发计算<br>Evictor : 用于数据剔除<br>allowedLateness : 标记是否处理迟到数据,当迟到数据达到窗口是否触发计算<br>Output Tag: 标记输出标签,然后在通过getSideOutput将窗口中的数据根据标签输出<br>Windows Function: 定义窗口上数据处理的逻辑,例如对数据进行Sum操作</p>          </div>        </details></li></ul></li><li>定义窗口前确定你的 stream 是 keyed 还是 non-keyed：<ul><li>keyed：<ul><li>对于 keyed stream，其中数据的任何属性都可以作为 key。 属于同一个 key 的元素会<br>被发送到同一个 task。</li><li>使用 keyed stream 允许你的窗口计算由多个 task 并行，因为每个逻辑上的 keyed<br>stream 都可以被单独处理。</li></ul></li><li>non-keyed：<ul><li>对于 non-keyed stream，原始的 stream 不会被分割为多个逻辑上的 stream。</li><li>所以所有的窗口计算会被同一个 task 完成，也就是 parallelism 为 1。</li></ul></li></ul></li><li>WindowAssigner 负责将 stream 中的每个数据分发到一个或多个窗口中。<ul><li>window(…)</li><li>windowAll(…)</li></ul></li></ul></div><h2 id="Count-Window"><a href="#Count-Window" class="headerlink" title="Count Window"></a>Count Window</h2><div class="note white no-icon flat"><ul><li>计数窗口基于元素的个数来截取数据，到达固定的个数时就触发计算并关闭窗口.</li><li>这相当于座位有限、“人满就发车”，是否发车与时间无关。每个窗口截取数据的个数，就是窗口的<br>大小。</li><li>计数窗口相比时间窗口就更加简单，我们只需指定窗口大小，就可以把数据分配到对应的窗口中<br>了。</li></ul><h3 id="Tumbling-Window"><a href="#Tumbling-Window" class="headerlink" title="Tumbling Window"></a>Tumbling Window</h3><div class="note green no-icon flat"><ul><li>滚动窗口的 assigner 分发元素到指定大小的窗口。滚动窗口的大小是固定的，且各自范围之间不<br>重叠。</li><li>滚动计数窗口只需要传入一个长整型的参数 size，表示窗口的大小。</li><li>使用场景：适用于按照指定的周期来统计指标。</li><li>比如说：每2个事件做一次统计。</li><li>图不贴了。滚动窗口我个人认为就是滑动窗口的特定形式</li></ul></div><h3 id="Sliding-Window"><a href="#Sliding-Window" class="headerlink" title="Sliding Window"></a>Sliding Window</h3><div class="note green no-icon flat"><ul><li>滑动窗口的 assigner 分发元素到指定大小的窗口，窗口大小通过 window size 参数设置。</li><li>滑动窗口需要传入两个参数：size 和 slide，前者表示窗口大小，后者表示滑动步长。</li><li>因此，如果 slide 小于窗口大小，滑动窗口可以允许窗口重叠。这种情况下，一个元素可能会被分<br>发到多个窗口。如果slide大于窗口大小，那么就会丢失部分数据</li><li>代码实现<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.typeinfo.Types;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span></span><br><span class="line">org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Description</span> :</span></span><br><span class="line"><span class="comment">* <span class="doctag">@School</span>:优极限学堂</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Official</span>-Website: http://www.yjxxt.com</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Teacher</span>:李毅大帝</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Mail</span>:863159469@qq.com</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Hello02CountWindow</span> &#123;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"><span class="comment">//运行环境</span></span><br><span class="line"><span class="type">StreamExecutionEnvironment</span> <span class="variable">environment</span> <span class="operator">=</span></span><br><span class="line">StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"><span class="comment">//获取数据源-admin:3</span></span><br><span class="line">DataStreamSource&lt;String&gt; source =</span><br><span class="line">environment.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">19523</span>);</span><br><span class="line"><span class="comment">//CountWindow--Tumbling</span></span><br><span class="line"><span class="comment">// source.map(word -&gt; Tuple2.of(word.split(&quot;:&quot;)[0],</span></span><br><span class="line">Integer.parseInt(word.split(<span class="string">&quot;:&quot;</span>)[<span class="number">1</span>]), Types.TUPLE(Types.STRING, Types.INT);</span><br><span class="line"><span class="comment">// .keyBy(tuple2 -&gt; tuple2.f0)</span></span><br><span class="line"><span class="comment">// .countWindow(3)</span></span><br><span class="line"><span class="comment">// .reduce((t1, t2) -&gt; &#123;</span></span><br><span class="line"><span class="comment">// t1.f1 = t1.f1 + t2.f1;</span></span><br><span class="line"><span class="comment">// return t1;</span></span><br><span class="line"><span class="comment">// &#125;).print(&quot;CountWindow--</span></span><br><span class="line">Tumbling:<span class="string">&quot;).setParallelism(1);</span></span><br><span class="line"><span class="string">//</span></span><br><span class="line"><span class="string">//CountWindow--Sliding</span></span><br><span class="line"><span class="string">source.map(word -&gt; Tuple2.of(word.split(&quot;</span>:<span class="string">&quot;)[0],</span></span><br><span class="line"><span class="string">Integer.parseInt(word.split(&quot;</span>:<span class="string">&quot;)[1])), Types.TUPLE(Types.STRING,</span></span><br><span class="line"><span class="string">Types.INT))</span></span><br><span class="line"><span class="string">.keyBy(tuple2 -&gt; tuple2.f0)</span></span><br><span class="line"><span class="string">.countWindow(3, 2)</span></span><br><span class="line"><span class="string">.reduce((t1, t2) -&gt; &#123;</span></span><br><span class="line"><span class="string">t1.f1 = t1.f1 + t2.f1;</span></span><br><span class="line"><span class="string">return t1;</span></span><br><span class="line"><span class="string">&#125;).print(&quot;</span>CountWindow--Sliding:<span class="string">&quot;).setParallelism(1);</span></span><br><span class="line"><span class="string">//运行环境</span></span><br><span class="line"><span class="string">environment.execute();</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">&#125;</span></span><br></pre></td></tr></table></figure></li></ul></div><h3 id="Window-All"><a href="#Window-All" class="headerlink" title="Window All"></a>Window All</h3><div class="note green no-icon flat"><ul><li>countWindowAll 数量窗口 (不分区数量滚动窗口【滑动窗口与滚动窗口的区别，在于滑动窗口会<br>有数据元素重叠可能，而滚动窗口不存在元素重叠】)</li><li>图不贴了，意思懂就行</li><li>代码实现<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.typeinfo.Types;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span></span><br><span class="line">org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Description</span> :</span></span><br><span class="line"><span class="comment">* <span class="doctag">@School</span>:优极限学堂</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Official</span>-Website: http://www.yjxxt.com</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Teacher</span>:李毅大帝</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Mail</span>:863159469@qq.com</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Hello03CountWindowAll</span> &#123;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"><span class="comment">//运行环境</span></span><br><span class="line"><span class="type">StreamExecutionEnvironment</span> <span class="variable">environment</span> <span class="operator">=</span></span><br><span class="line">StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"><span class="comment">//获取数据源-admin:3</span></span><br><span class="line">DataStreamSource&lt;String&gt; source =</span><br><span class="line">environment.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">19523</span>);</span><br><span class="line"><span class="comment">//CountWindowAll--Tumbling</span></span><br><span class="line"><span class="comment">// source.map(word -&gt; Tuple2.of(word.split(&quot;:&quot;)[0],</span></span><br><span class="line">Integer.parseInt(word.split(<span class="string">&quot;:&quot;</span>)[<span class="number">1</span>]), Types.TUPLE(Types.STRING, Types.INT);</span><br><span class="line"><span class="comment">// .countWindowAll(3)</span></span><br><span class="line"><span class="comment">// .reduce((t1, t2) -&gt; &#123;</span></span><br><span class="line"><span class="comment">// t1.f0= t1.f0+&quot;_&quot;+t2.f0;</span></span><br><span class="line"><span class="comment">// t1.f1 = t1.f1 + t2.f1;</span></span><br><span class="line"><span class="comment">// return t1;</span></span><br><span class="line"><span class="comment">// &#125;).print(&quot;CountWindowAll--</span></span><br><span class="line">Tumbling:<span class="string">&quot;).setParallelism(1);</span></span><br><span class="line"><span class="string">//CountWindow--Sliding</span></span><br><span class="line"><span class="string">source.map(word -&gt; Tuple2.of(word.split(&quot;</span>:<span class="string">&quot;)[0],</span></span><br><span class="line"><span class="string">Integer.parseInt(word.split(&quot;</span>:<span class="string">&quot;)[1])), Types.TUPLE(Types.STRING,</span></span><br><span class="line"><span class="string">Types.INT))</span></span><br><span class="line"><span class="string">.countWindowAll(3, 2)</span></span><br><span class="line"><span class="string">.reduce((t1, t2) -&gt; &#123;</span></span><br><span class="line"><span class="string">t1.f0 = t1.f0 + &quot;</span>_<span class="string">&quot; + t2.f0;</span></span><br><span class="line"><span class="string">t1.f1 = t1.f1 + t2.f1;</span></span><br><span class="line"><span class="string">return t1;</span></span><br><span class="line"><span class="string">&#125;).print(&quot;</span>CountWindowAll--Sliding:<span class="string">&quot;).setParallelism(1);</span></span><br><span class="line"><span class="string">//运行环境</span></span><br><span class="line"><span class="string">environment.execute();</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">&#125;</span></span><br></pre></td></tr></table></figure></li></ul></div></div><h2 id="Time-Window"><a href="#Time-Window" class="headerlink" title="Time Window"></a>Time Window</h2><div class="note white no-icon flat"><ol><li>时间窗口是最常用的窗口类型，又可以细分为滚动、滑动和会话三种。<ul><li>翻滚窗口（Tumbling Window，无重叠）</li><li>滑动窗口（Sliding Window，有重叠）</li><li>会话窗口（Session Window，活动间隙）</li></ul></li><li>除了Flink自定义的的，还可以继承 WindowAssigner 类来实现自定义的 window assigner。<ul><li>所有内置的 window assigner（除了 global window）都是基于时间分发数据的，<br>processing time 或 event time 均可。</li></ul></li></ol><h3 id="Tumbling-Window"><a href="#Tumbling-Window" class="headerlink" title="Tumbling Window"></a>Tumbling Window</h3><div class="note green no-icon flat"><ul><li>滚动窗口的 assigner 分发元素到指定大小的窗口。滚动窗口的大小是固定的，且各自范围之间不<br>重叠。</li><li>比如说，如果你指定了滚动窗口的大小为 5 分钟，那么每 5 分钟就会有一个窗口被计算，且一个<br>新的窗口被创建（如下图所示）。</li></ul></div><h3 id="Sliding-Window-1"><a href="#Sliding-Window-1" class="headerlink" title="Sliding Window"></a>Sliding Window</h3><div class="note green no-icon flat"><ul><li>滑动窗口的 assigner 分发元素到指定大小的窗口，窗口大小通过 window size 参数设置。</li><li>滑动窗口需要一个额外的滑动距离（window slide）参数来控制生成新窗口的频率。</li><li>因此，如果 slide 小于窗口大小，滑动窗口可以允许窗口重叠。这种情况下，一个元素可能会被分<br>发到多个窗口。</li><li>比如说，你设置了大小为 10 分钟，滑动距离 5 分钟的窗口，你会在每 5 分钟得到一个新的窗口，<br>里面包含之前 10 分钟到达的数据（如下图所示）。</li><li>图就不贴了，懂得都懂</li><li>代码实现<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.typeinfo.Types;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span></span><br><span class="line">org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span></span><br><span class="line">org.apache.flink.streaming.api.windowing.assigners.SlidingProcessingTimeWindows;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.Time;</span><br><span class="line"><span class="keyword">import</span> java.time.LocalDateTime;</span><br><span class="line"><span class="keyword">import</span> java.time.format.DateTimeFormatter;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Description</span> :</span></span><br><span class="line"><span class="comment">* <span class="doctag">@School</span>:优极限学堂</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Official</span>-Website: http://www.yjxxt.com</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Teacher</span>:李毅大帝</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Mail</span>:863159469@qq.com</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Hello04TimeWindow</span> &#123;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"><span class="comment">//运行环境</span></span><br><span class="line"><span class="type">StreamExecutionEnvironment</span> <span class="variable">environment</span> <span class="operator">=</span></span><br><span class="line">StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"><span class="comment">//获取数据源-admin:3</span></span><br><span class="line">DataStreamSource&lt;String&gt; source =</span><br><span class="line">environment.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">19523</span>);</span><br><span class="line"><span class="comment">//TimeWindow--Tumbling</span></span><br><span class="line"><span class="comment">// source.map(word -&gt; Tuple2.of(word.split(&quot;:&quot;)[0],</span></span><br><span class="line">Integer.parseInt(word.split(<span class="string">&quot;:&quot;</span>)[<span class="number">1</span>]), Types.TUPLE(Types.STRING, Types.INT)</span><br><span class="line"><span class="comment">// .keyBy(tuple2 -&gt; tuple2.f0)</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line">.window(TumblingProcessingTimeWindows.of(Time.seconds(<span class="number">5</span>)));</span><br><span class="line"><span class="comment">// .reduce((t1, t2) -&gt; &#123;</span></span><br><span class="line"><span class="comment">// t1.f1 = t1.f1 + t2.f1;</span></span><br><span class="line"><span class="comment">// return t1;</span></span><br><span class="line"><span class="comment">// &#125;)</span></span><br><span class="line"><span class="comment">// .map(tuple2 -&gt; &#123;</span></span><br><span class="line"><span class="comment">// tuple2.f0 =</span></span><br><span class="line">LocalDateTime.now().format(DateTimeFormatter.ofPattern(<span class="string">&quot;yyyy年MM月dd日HH 时mm分ss秒SSS毫秒&quot;</span>)) + tuple2.f0;</span><br><span class="line"><span class="comment">// return tuple2;</span></span><br><span class="line"><span class="comment">// &#125;, Types.TUPLE(Types.STRING, Types.INT))</span></span><br><span class="line"><span class="comment">// .print(&quot;TimeWindow--Tumbling:&quot;).setParallelism(1);</span></span><br><span class="line"><span class="comment">//TimeWindow--Sliding</span></span><br><span class="line">source.map(word -&gt; Tuple2.of(word.split(<span class="string">&quot;:&quot;</span>)[<span class="number">0</span>],</span><br><span class="line">Integer.parseInt(word.split(<span class="string">&quot;:&quot;</span>)[<span class="number">1</span>])), Types.TUPLE(Types.STRING,</span><br><span class="line">Types.INT))</span><br><span class="line">.keyBy(tuple2 -&gt; tuple2.f0)</span><br><span class="line">.window(SlidingProcessingTimeWindows.of(Time.seconds(<span class="number">5</span>),</span><br><span class="line">Time.seconds(<span class="number">2</span>)))</span><br><span class="line">.reduce((t1, t2) -&gt; &#123;</span><br><span class="line">t1.f1 = t1.f1 + t2.f1;</span><br><span class="line"><span class="keyword">return</span> t1;</span><br><span class="line">&#125;).map(tuple2 -&gt; &#123;</span><br><span class="line">tuple2.f0 = LocalDateTime.now().format(DateTimeFormatter.ofPattern(<span class="string">&quot;yyyy年MM月dd日HH时mm分ss秒SSS毫秒&quot;</span>)) + tuple2.f0;</span><br><span class="line"><span class="keyword">return</span> tuple2;</span><br><span class="line">&#125;, Types.TUPLE(Types.STRING, Types.INT))</span><br><span class="line">.print(<span class="string">&quot;TimeWindow--Sliding:&quot;</span>).setParallelism(<span class="number">1</span>);</span><br><span class="line"><span class="comment">//运行环境</span></span><br><span class="line">environment.execute();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></div><h3 id="Session-Windown"><a href="#Session-Windown" class="headerlink" title="Session Windown"></a>Session Windown</h3><div class="note green no-icon flat"><ul><li>会话窗口的 assigner 会把数据按活跃的会话分组。</li><li>与滚动窗口和滑动窗口不同，会话窗口不会相互重叠，且没有固定的开始或结束时间。</li><li>会话窗口的 assigner 可以设置固定的会话间隔（session gap）或 用 session gap extractor 函数来<br>动态地定义多长时间算作不活跃。</li><li>当超出了不活跃的时间段，当前的会话就会关闭，并且将接下来的数据分发到新的会话窗口。</li><li>说人话就是，会话窗口只有当你离开了才会计算数据，至于怎么样才算走了，他说了算</li><li>代码实现<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.typeinfo.Types;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.assigners.ProcessingTimeSessionWindows;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.Time;</span><br><span class="line"><span class="keyword">import</span> java.time.LocalDateTime;</span><br><span class="line"><span class="keyword">import</span> java.time.format.DateTimeFormatter;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Description</span> :</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@School</span>:优极限学堂</span></span><br><span class="line"><span class="comment">10.4.4. Minor Defects</span></span><br><span class="line"><span class="comment">代码实现</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Official</span>-Website: http://www.yjxxt.com</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Teacher</span>:李毅大帝</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Mail</span>:863159469@qq.com</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Hello05TimeWindowSession</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"><span class="comment">//运行环境</span></span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">environment</span> <span class="operator">=</span></span><br><span class="line">                StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"><span class="comment">//获取数据源-admin:3</span></span><br><span class="line">        DataStreamSource&lt;String&gt; source =</span><br><span class="line">                environment.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">19523</span>);</span><br><span class="line"><span class="comment">//TimeWindow--Session</span></span><br><span class="line">        source.map(word -&gt; Tuple2.of(word.split(<span class="string">&quot;:&quot;</span>)[<span class="number">0</span>],</span><br><span class="line">                        Integer.parseInt(word.split(<span class="string">&quot;:&quot;</span>)[<span class="number">1</span>])), Types.TUPLE(Types.STRING,</span><br><span class="line">                        Types.INT))</span><br><span class="line">                .keyBy(tuple2 -&gt; tuple2.f0)</span><br><span class="line">                .window(ProcessingTimeSessionWindows.withGap(Time.seconds(<span class="number">5</span>)))</span><br><span class="line">                .reduce((t1, t2) -&gt; &#123;</span><br><span class="line">                    t1.f1 = t1.f1 + t2.f1;</span><br><span class="line">                    <span class="keyword">return</span> t1;</span><br><span class="line">                &#125;)</span><br><span class="line">                .map(tuple2 -&gt; &#123;</span><br><span class="line">                    tuple2.f0 =</span><br><span class="line">                            LocalDateTime.now().format(DateTimeFormatter.ofPattern(<span class="string">&quot;yyyy年MM月dd日HH时mm分ss秒SSS毫秒&quot;</span>)) + tuple2.f0;</span><br><span class="line">                    <span class="keyword">return</span> tuple2;</span><br><span class="line">                &#125;, Types.TUPLE(Types.STRING, Types.INT))</span><br><span class="line">                .print(<span class="string">&quot;TimeWindow--Session:&quot;</span>).setParallelism(<span class="number">1</span>);</span><br><span class="line"><span class="comment">//运行环境</span></span><br><span class="line">        environment.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></div><h3 id="Minor-Defects"><a href="#Minor-Defects" class="headerlink" title="Minor Defects"></a>Minor Defects</h3><div class="note green no-icon flat"><ul><li>图就不贴了，就是明月几时有，把酒问青天，和flink毛关系没有</li><li>代码实现<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> com.yjxxt.util.KafkaUtil;</span><br><span class="line"><span class="keyword">import</span> org.apache.commons.lang3.RandomStringUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.eventtime.WatermarkStrategy;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.SimpleStringSchema;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.typeinfo.Types;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple3;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.connector.kafka.source.KafkaSource;</span><br><span class="line"><span class="keyword">import</span></span><br><span class="line">org.apache.flink.connector.kafka.source.enumerator.initializer.OffsetsInitializer;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.KeyedStream;</span><br><span class="line"><span class="keyword">import</span></span><br><span class="line">org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span></span><br><span class="line">org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.Time;</span><br><span class="line"><span class="keyword">import</span> java.time.LocalDateTime;</span><br><span class="line"><span class="keyword">import</span> java.time.format.DateTimeFormatter;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Description</span> :</span></span><br><span class="line"><span class="comment">* <span class="doctag">@School</span>:优极限学堂</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Official</span>-Website: http://www.yjxxt.com</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Teacher</span>:李毅大帝</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Mail</span>:863159469@qq.com</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Hello09EventTimeWindow</span> &#123;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"><span class="comment">//启动一个线程专门发送消息给Kafka，这样我们才有数据消费</span></span><br><span class="line"><span class="keyword">new</span> <span class="title class_">Thread</span>(() -&gt; &#123;</span><br><span class="line"><span class="type">String</span> <span class="variable">uname</span> <span class="operator">=</span> RandomStringUtils.randomAlphabetic(<span class="number">8</span>);</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">100</span>; i &lt; <span class="number">200</span>; i++) &#123;</span><br><span class="line"><span class="type">String</span> <span class="variable">date</span> <span class="operator">=</span></span><br><span class="line">LocalDateTime.now().format(DateTimeFormatter.ofPattern(<span class="string">&quot;yyyy年MM月dd日HH 时mm分ss秒SSS&quot;</span>));</span><br><span class="line">KafkaUtil.sendMsg(<span class="string">&quot;yjxxt&quot;</span>, uname + i % <span class="number">2</span> + <span class="string">&quot;:&quot;</span> + i + <span class="string">&quot;:&quot;</span> + date);</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">Thread.sleep(<span class="number">495</span>);</span><br><span class="line">&#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">e.printStackTrace();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;).start();</span><br><span class="line"><span class="comment">//获取环境</span></span><br><span class="line"><span class="type">StreamExecutionEnvironment</span> <span class="variable">environment</span> <span class="operator">=</span></span><br><span class="line">StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"><span class="comment">//设置Kafka连接</span></span><br><span class="line">KafkaSource&lt;String&gt; source = KafkaSource.&lt;String&gt;builder()</span><br><span class="line">.setBootstrapServers(<span class="string">&quot;node01:9092,node02:9092,node03:9092&quot;</span>)</span><br><span class="line">.setTopics(<span class="string">&quot;yjxxt&quot;</span>)</span><br><span class="line">.setGroupId(<span class="string">&quot;flink_KafkaConnector&quot;</span>)</span><br><span class="line">.setStartingOffsets(OffsetsInitializer.latest())</span><br><span class="line">.setValueOnlyDeserializer(<span class="keyword">new</span> <span class="title class_">SimpleStringSchema</span>())</span><br><span class="line">.build();</span><br><span class="line"><span class="comment">//读取数据源</span></span><br><span class="line">DataStreamSource&lt;String&gt; kafkaSource = environment.fromSource(source, WatermarkStrategy.noWatermarks(),</span><br><span class="line"><span class="string">&quot;KafkaSource&quot;</span>);</span><br><span class="line">KeyedStream&lt;Tuple3&lt;String, String, String&gt;, String&gt; keyedStream</span><br><span class="line">= kafkaSource.map(word -&gt; &#123;</span><br><span class="line">String[] split = word.split(<span class="string">&quot;:&quot;</span>);</span><br><span class="line"><span class="keyword">return</span> Tuple3.of(split[<span class="number">0</span>], split[<span class="number">1</span>], split[<span class="number">2</span>]);</span><br><span class="line">&#125;, Types.TUPLE(Types.STRING, Types.STRING,</span><br><span class="line">Types.STRING))</span><br><span class="line">.keyBy(t -&gt; t.f0);</span><br><span class="line"><span class="comment">//TimeWindow--Tumbling</span></span><br><span class="line">keyedStream.window(TumblingEventTimeWindows.of(Time.seconds(<span class="number">5</span>)))</span><br><span class="line">.reduce((t1, t2) -&gt; &#123;</span><br><span class="line">t1.f1 = t1.f1 + <span class="string">&quot;-&quot;</span> + t2.f1;</span><br><span class="line"><span class="keyword">return</span> t1;</span><br><span class="line">&#125;).map(t -&gt; &#123;</span><br><span class="line">t.f2 =</span><br><span class="line">LocalDateTime.now().format(DateTimeFormatter.ofPattern(<span class="string">&quot;yyyy年MM月dd日HH 时mm分ss秒SSS&quot;</span>));</span><br><span class="line"><span class="keyword">return</span> t;</span><br><span class="line">&#125;, Types.TUPLE(Types.STRING, Types.STRING,</span><br><span class="line">Types.STRING))</span><br><span class="line">.print(<span class="string">&quot;[TimeWindow--Tumbling-- EventTime]&quot;</span>).setParallelism(<span class="number">1</span>);</span><br><span class="line"><span class="comment">//执行环境</span></span><br><span class="line">environment.execute();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></div></div><h2 id="Global-Windows"><a href="#Global-Windows" class="headerlink" title="Global Windows"></a>Global Windows</h2><div class="note white no-icon flat"><ul><li>GlobalWindows作为一个全局的窗口分配器，它不像TimeWindow或CountWindow那样通过元素<br>个数来划分成一个个窗口，而是把分区内所有的元素分配到同一个窗口，所以说如果没有定义触发<br>器，那么整个subTask中就只有一个窗口，且一直存在，不会触发计算。</li><li>窗口模式仅在你指定了自定义的 【trigger】时有用。 否则，计算不会发生，因为全局窗口没有天<br>然的终点去触发其中积累的数据。</li><li>使用Global Windows需要非常慎重，用户需要非常明确自己在整个窗口中统计出的结果是什么，<br>并指定对应的触发器，同时还需要有指定相应的数据清理机制，否则数据将一直留在内存中。</li><li>window和windowAll都是对stream定义窗口的方法，都需要传入WindowAssigner（窗口分配<br>器）执行具体的开窗操作<ul><li>window只能在已经分区的 KeyedStream 上定义，通过KeyedStream转化为<br>WindowedStream执行具体的开窗操作。</li><li>windowAll只能在未分区的DataStream上定义，调用windowAll方法后，会把DataStream转<br>化为AllWindowedStream，并得到全局统计结果。也就是说WindowAll并行度只能1，且不<br>可设置并行度。</li></ul></li><li>图就不贴了，没什么意义</li><li>代码实现<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.typeinfo.Types;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span></span><br><span class="line">org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.assigners.GlobalWindows;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.triggers.CountTrigger;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.triggers.PurgingTrigger;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Description</span> :</span></span><br><span class="line"><span class="comment">* <span class="doctag">@School</span>:优极限学堂</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Official</span>-Website: http://www.yjxxt.com</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Teacher</span>:李毅大帝</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Mail</span>:863159469@qq.com</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Hello08GlobalWindow</span> &#123;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"><span class="comment">//运行环境</span></span><br><span class="line"><span class="type">StreamExecutionEnvironment</span> <span class="variable">environment</span> <span class="operator">=</span></span><br><span class="line">StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"><span class="comment">//获取数据源-admin:3</span></span><br><span class="line">DataStreamSource&lt;String&gt; source =</span><br><span class="line">environment.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">19523</span>);</span><br><span class="line"><span class="comment">//GlobalWindow</span></span><br><span class="line">source.map(word -&gt; Tuple2.of(word.split(<span class="string">&quot;:&quot;</span>)[<span class="number">0</span>],</span><br><span class="line">Integer.parseInt(word.split(<span class="string">&quot;:&quot;</span>)[<span class="number">1</span>])), Types.TUPLE(Types.STRING, Types.INT))</span><br><span class="line">.keyBy(tuple2 -&gt; tuple2.f0)</span><br><span class="line">.window(GlobalWindows.create())</span><br><span class="line">.trigger(PurgingTrigger.of(CountTrigger.of(<span class="number">5</span>)))</span><br><span class="line">.reduce((t1, t2) -&gt; &#123;</span><br><span class="line">t1.f1 = t1.f1 + t2.f1;</span><br><span class="line"><span class="keyword">return</span> t1;</span><br><span class="line">&#125;).print(<span class="string">&quot;GlobalWindow:&quot;</span>).setParallelism(<span class="number">1</span>);</span><br><span class="line">source.map(word -&gt; Tuple2.of(word.split(<span class="string">&quot;:&quot;</span>)[<span class="number">0</span>],</span><br><span class="line">Integer.parseInt(word.split(<span class="string">&quot;:&quot;</span>)[<span class="number">1</span>])), Types.TUPLE(Types.STRING, Types.INT))</span><br><span class="line">.windowAll(GlobalWindows.create())</span><br><span class="line">.trigger(PurgingTrigger.of(CountTrigger.of(<span class="number">5</span>)))</span><br><span class="line">.reduce((t1, t2) -&gt; &#123;</span><br><span class="line">t1.f1 = t1.f1 + t2.f1;</span><br><span class="line"><span class="keyword">return</span> t1;</span><br><span class="line">&#125;).print(<span class="string">&quot;GlobalWindow:&quot;</span>).setParallelism(<span class="number">1</span>);</span><br><span class="line"><span class="comment">//运行环境</span></span><br><span class="line">environment.execute();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></div>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>process function</title>
      <link href="/2023/07/28/flink/flink_process/"/>
      <url>/2023/07/28/flink/flink_process/</url>
      
        <content type="html"><![CDATA[<h1 id="flink学习"><a href="#flink学习" class="headerlink" title="flink学习"></a>flink学习</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><div class="note white no-icon flat"><p>md，不弄还是不行，尤其flink这么重要，还是像kafka那样学习吧，累就累点</p></div><h2 id="Flink-process-function"><a href="#Flink-process-function" class="headerlink" title="Flink process function"></a>Flink process function</h2><h3 id="函数介绍"><a href="#函数介绍" class="headerlink" title="函数介绍"></a>函数介绍</h3><div class="note white no-icon flat"><ol><li>转换算子是无法访问事件的时间戳信息和水位线信息的，而这在一些应用场景下，极为重要。</li><li>ProcessFunction 函数是低阶流处理算子，可以访问流应用程序所有（非循环）基本构建块：<ul><li>事件 (数据流元素)</li><li>状态 (容错和一致性</li><li>定时器 (事件时间和处理时间)</li></ul></li></ol></div><h3 id="函数分类"><a href="#函数分类" class="headerlink" title="函数分类"></a>函数分类</h3><div class="note white no-icon flat"><ol><li>Flink 提供了 8 个不同的处理函数：<ul><li><div class="img-wrap"><div class="img-bg"><img class="img" src="http://localhost:4000/img/img_md/函数分类.png"/></div></div></li></ul></li><li>ProcessFunction<ul><li>最基本的处理函数，基于 DataStream 直接调用.process()时作为参数传入。</li></ul></li><li>ProcessWindowFunction<ul><li>开窗之后的处理函数，也是全窗口函数的代表。基于 WindowedStream 调用.process()时作<br>为参数传入。</li></ul></li><li>ProcessAllWindowFunction<ul><li>同样是开窗之后的处理函数，基于 AllWindowedStream 调用.process()时作为参数传入。</li></ul></li><li>CoProcessFunction<ul><li>合并（connect）两条流之后的处理函数，基于 ConnectedStreams 调用.process()时作为参<br>数传入。关于流的连接合并操作</li></ul></li><li>ProcessJoinFunction<ul><li>间隔连接（interval join）两条流之后的处理函数，基于 IntervalJoined 调用.process()时作<br>为参数传入。</li></ul></li><li>BroadcastProcessFunction<ul><li>广播连接流处理函数，基于 BroadcastConnectedStream 调用.process()时作为参数传入。</li><li>这里的“广播连接流”BroadcastConnectedStream，是一个未 keyBy 的普通 DataStream 与<br>一个广播流（BroadcastStream）做连接（conncet）之后的产物。</li></ul></li><li>KeyedBroadcastProcessFunction<ul><li>按键分区的广播连接流处理函数，同样是基于 BroadcastConnectedStream 调用.process()<br>时作为参数传入。</li><li>与 BroadcastProcessFunction 不同的是，这时的广播连接流，是一个 KeyedStream与广播<br>流（BroadcastStream）做连接之后的产物。</li></ul></li><li>代码展示<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span></span><br><span class="line">org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.ProcessFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.Collector;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Description</span> :</span></span><br><span class="line"><span class="comment">* <span class="doctag">@School</span>:优极限学堂</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Official</span>-Website: http://www.yjxxt.com</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Teacher</span>:李毅大帝</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Mail</span>:863159469@qq.com</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Hello13ProcessFunction</span> &#123;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"><span class="comment">//获取程序运行的环境</span></span><br><span class="line"><span class="type">StreamExecutionEnvironment</span> <span class="variable">environment</span> <span class="operator">=</span></span><br><span class="line">StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"><span class="comment">//数据源</span></span><br><span class="line">DataStreamSource&lt;String&gt; source =</span><br><span class="line">environment.fromElements(<span class="string">&quot;aa&quot;</span>, <span class="string">&quot;bb&quot;</span>, <span class="string">&quot;cc&quot;</span>).setParallelism(<span class="number">1</span>);</span><br><span class="line"><span class="comment">//处理数据</span></span><br><span class="line">source.map(word -&gt; <span class="string">&quot;yjxxt_&quot;</span> + word).process(<span class="keyword">new</span></span><br><span class="line"><span class="title class_">ProcessFunction</span>&lt;String, String&gt;() &#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">processElement</span><span class="params">(String s,</span></span><br><span class="line"><span class="params">ProcessFunction&lt;String, String&gt;.Context context, Collector&lt;String&gt;</span></span><br><span class="line"><span class="params">collector)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"><span class="comment">//查看Context</span></span><br><span class="line">System.out.println(<span class="string">&quot;[处理时间]&quot;</span> +</span><br><span class="line">context.timerService().currentProcessingTime());</span><br><span class="line">System.out.println(<span class="string">&quot;[水位线/水印]&quot;</span> +</span><br><span class="line">context.timerService().currentWatermark());</span><br><span class="line">collector.collect(s + <span class="string">&quot;_&quot;</span> + s.hashCode());</span><br><span class="line">&#125;</span><br><span class="line">&#125;).print();</span><br><span class="line"><span class="comment">//执行代码</span></span><br><span class="line">environment.execute();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol></div><h3 id="侧输出"><a href="#侧输出" class="headerlink" title="侧输出"></a>侧输出</h3><div class="note white no-icon flat"><ol><li>process function的side outputs功能可以产生多条流，并且这些流的数据类型可以不一样。</li><li>一个side output可以定义为OutputTag[X]对象，X是输出流的数据类型。</li><li>process function可以通过Context对象发射一个事件到一个或者多个side outputs。</li><li>代码展示<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;</span><br><span class="line"><span class="keyword">import</span></span><br><span class="line">        org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.ProcessFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.Collector;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.OutputTag;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Description</span> :</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@School</span>:优极限学堂</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Official</span>-Website: http://www.yjxxt.com</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Teacher</span>:李毅大帝</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Mail</span>:863159469@qq.com</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Hello14ProcessFunctionSideOutputTag</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"><span class="comment">//获取程序运行的环境</span></span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">environment</span> <span class="operator">=</span></span><br><span class="line">                StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"><span class="comment">//数据源</span></span><br><span class="line">        DataStreamSource&lt;String&gt; source = environment.fromElements(<span class="string">&quot;a&quot;</span>, <span class="string">&quot;b&quot;</span>,</span><br><span class="line">                <span class="string">&quot;c&quot;</span>, <span class="string">&quot;aa&quot;</span>, <span class="string">&quot;bb&quot;</span>, <span class="string">&quot;cc&quot;</span>, <span class="string">&quot;aaa&quot;</span>, <span class="string">&quot;bbb&quot;</span>, <span class="string">&quot;ccc&quot;</span>);</span><br><span class="line"><span class="comment">//处理数据,分别获取长度为2和长度为3的</span></span><br><span class="line"><span class="comment">// DataStream&lt;String&gt; str2 = source.filter(w -&gt; w.length() == 2);</span></span><br><span class="line"><span class="comment">// DataStream&lt;String&gt; str3 = source.filter(w -&gt; w.length() == 3);</span></span><br><span class="line"><span class="comment">//侧输出获取数据</span></span><br><span class="line">        OutputTag&lt;String&gt; outputTag2 = <span class="keyword">new</span> <span class="title class_">OutputTag</span>&lt;String&gt;(<span class="string">&quot;sideOutput2&quot;</span>)</span><br><span class="line">        &#123;</span><br><span class="line">        &#125;;</span><br><span class="line">        OutputTag&lt;String&gt; outputTag3 = <span class="keyword">new</span> <span class="title class_">OutputTag</span>&lt;String&gt;(<span class="string">&quot;sideOutput3&quot;</span>)</span><br><span class="line">        &#123;</span><br><span class="line">        &#125;;</span><br><span class="line">        SingleOutputStreamOperator&lt;String&gt; processStream =</span><br><span class="line">                source.process(<span class="keyword">new</span> <span class="title class_">ProcessFunction</span>&lt;String, String&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">processElement</span><span class="params">(String s, ProcessFunction&lt;String,</span></span><br><span class="line"><span class="params">                            String&gt;.Context context, Collector&lt;String&gt; collector)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"><span class="comment">//侧输出收集</span></span><br><span class="line">                        <span class="keyword">if</span> (s != <span class="literal">null</span> &amp;&amp; s.length() == <span class="number">2</span>) &#123;</span><br><span class="line">                            context.output(outputTag2, s);</span><br><span class="line">                        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (s != <span class="literal">null</span> &amp;&amp; s.length() == <span class="number">3</span>) &#123;</span><br><span class="line">                            context.output(outputTag3, s);</span><br><span class="line">                        &#125;</span><br><span class="line"><span class="comment">//主线收集</span></span><br><span class="line">                        collector.collect(s.toUpperCase());</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line"><span class="comment">//主线剧情</span></span><br><span class="line"><span class="comment">// processStream.print().setParallelism(1);</span></span><br><span class="line"><span class="comment">//获取侧输出数据</span></span><br><span class="line">        processStream.getSideOutput(outputTag2).print(<span class="string">&quot;outputTag2&quot;</span>).setParallelism(</span><br><span class="line">                <span class="number">1</span>);</span><br><span class="line">        processStream.getSideOutput(outputTag3).print(<span class="string">&quot;outputTag3&quot;</span>).setParallelism(</span><br><span class="line">                <span class="number">1</span>);</span><br><span class="line"><span class="comment">//执行代码</span></span><br><span class="line">        environment.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol></div>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>时间语义</title>
      <link href="/2023/07/28/flink/%E6%97%B6%E9%97%B4%E8%AF%AD%E4%B9%89/"/>
      <url>/2023/07/28/flink/%E6%97%B6%E9%97%B4%E8%AF%AD%E4%B9%89/</url>
      
        <content type="html"><![CDATA[<h1 id="Flink时间语义"><a href="#Flink时间语义" class="headerlink" title="Flink时间语义"></a>Flink时间语义</h1><h2 id="宋朝大事记"><a href="#宋朝大事记" class="headerlink" title="宋朝大事记"></a>宋朝大事记</h2><div class="note white no-icon flat"><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">宋太祖赵匡胤：</span><br><span class="line">公元960年：庚申，建隆元年，辽穆宗耶律璟应历十年，后周恭帝柴宗训显德七年</span><br><span class="line">公元963年：癸亥，建隆四年，乾德元年，北宋灭荆南</span><br><span class="line">公元965年：乙丑，乾德三年，北宋灭后蜀</span><br><span class="line">公元968年：戊辰，乾德六年，开宝元年</span><br><span class="line">公元969年：己巳，开宝二年，辽穆宗耶律璟应历十九年，辽景宗耶律贤保宁元年</span><br><span class="line">公元971年：辛未，开宝四年，北宋灭南汉</span><br><span class="line">公元975年：乙亥，开宝八年，北宋灭南唐</span><br><span class="line">公元976年：丙子，开宝九年，宋太宗赵炅改为太平兴国元年</span><br><span class="line">宋太宗赵匡义：</span><br><span class="line">公元976年：丙子，太平兴国元年，开宝九年</span><br><span class="line">公元978年：戊寅，太平兴国三年，北宋灭吴越</span><br><span class="line">公元979年：己卯，太平兴国四年，北宋灭北汉，五代十国片面完毕。</span><br><span class="line">公元983年：癸未，太平兴国八年，辽景宗耶律贤乾亨五年，辽圣宗耶律隆绪统和元年</span><br><span class="line">公元984年：甲申，太平兴国九年，雍熙元年</span><br></pre></td></tr></table></figure><ul><li>时间点：<ul><li>历史事件发生的时间，永远不会改变</li><li>历史事件回顾的时间</li></ul></li></ul></div><h2 id="时间概念"><a href="#时间概念" class="headerlink" title="时间概念"></a>时间概念</h2><div class="note white no-icon flat"><div class="img-wrap"><div class="img-bg"><img class="img" src="http://localhost:4000/img/img_md/时间语义_时间概念.png"/></div></div><ol><li>Event Time(事件时间)<ul><li>事件时间是每个事件在其生产设备上发生的时间。这个时间通常在记录进入Flink之前嵌入在<br>记录中，并且可以从每个记录中提取事件时间戳。</li><li>在事件时间中，时间的进展取决于数据，而不是任何挂钟。事件时间程序必须指定如何生成<br>事件时间水印，这是表示事件时间进度的机制。</li><li>事件时间指的是数据本身携带的时间。这个时间是在事件产生时的时间。</li><li>事件时间对于乱序、延时、或者数据重放等情况，都能给出正确的结果。</li><li>例如：充值数据</li></ul></li><li>Ingestion time(摄入时间)<ul><li>摄入时间指的是数据进入 Flink 的时间；</li></ul></li><li>Processing time（处理时间）<ul><li>处理时间是指正在执行相应操作的机器的系统时间。</li><li>当流式程序在基于处理时间运行时，所有基于时间的操作（如时间窗口）都将使用运行相应<br>的机器的系统时钟。</li><li>每小时处理时间窗口将包括在系统时钟指示整小时的时间之间到达特定操作员的所有记录。<ul><li>如果应用程序在上午9:15开始运行，第一个每小时处理时间窗口将包括上午9:15和上午<br>10:00之间处理的事件</li><li>下一个窗口将包括在上午10:00和上午11:00之间处理的活动，以此类推。</li></ul></li><li>处理时间是最简单的时间概念，不需要流和机器之间的协调。它提供了最佳的性能和最低的<br>延迟。但是，在分布式和异步环境中，处理时间不提供确定性，因为它容易受到记录到达系<br>统的速度（例如，从消息队列）、记录在系统内的操作员之间流动的速度以及中断（计划的<br>或其他）的影响。</li><li>例如：秒杀数据</li></ul></li></ol></div><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><div class="note white no-icon flat"><ul><li>时间语义主要为窗口计算而服务</li><li>1.12以前，flink 默认以 processing time 作为默认的时间语义；可以在 env 上设置所想要的时间<br>语义；但是新版本已经deprecated 了上述设置 api；<div class="note green no-icon flat"><p>//设置 EventTime 作为时间标准<br>environment.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);<br>//设置 IngestionTime 作为时间标准<br>environment.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime);<br>//设置 ProcessingTime 作为时间标准<br>environment.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime); </p></div><ul><li>1.12及以后，flink 以 event time 作为默认的时间语义<div class="note green no-icon flat"><p>keyedStream.window(SlidingEventTimeWindows.of(Time.seconds(5), Time.seconds(1)));<br>keyedStream.window(SlidingProcessingTimeWindows.of(Time.seconds(5), Time.seconds(1)));<br>keyedStream.window(TumblingEventTimeWindows.of(Time.seconds(5)));<br>keyedStream.window(TumblingProcessingTimeWindows.of(Time.seconds(5)));<br>keyedStream.window(EventTimeSessionWindows.withGap(Time.minutes(10)));<br>keyedStream.window(ProcessingTimeSessionWindows.withGap(Time.minutes(10))); </p></div></li></ul></div></li></ul>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SQL练习</title>
      <link href="/2023/07/28/SQL/%E7%89%9B%E5%AE%A2SQL264/"/>
      <url>/2023/07/28/SQL/%E7%89%9B%E5%AE%A2SQL264/</url>
      
        <content type="html"><![CDATA[<h1 id="牛客sql264"><a href="#牛客sql264" class="headerlink" title="牛客sql264"></a>牛客sql264</h1><div class="note white no-icon flat"><p>这是一道老sql了，太古老了，上一次写这道题给我道心干碎了。<br>经过了一个月的训练，看待问题的角度又有了新的变化。<br>先上题目</p><div class="img-wrap"><div class="img-bg"><img class="img" src="http://localhost:4000/img/img_sql/牛客264.png"/></div></div><p><details class="folding-tag" green><summary> sql </summary>              <div class='content'>              <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> if <span class="keyword">exists</span> login;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> `login` (</span><br><span class="line">`id` <span class="type">int</span>(<span class="number">4</span>) <span class="keyword">NOT</span> <span class="keyword">NULL</span>,</span><br><span class="line">`user_id` <span class="type">int</span>(<span class="number">4</span>) <span class="keyword">NOT</span> <span class="keyword">NULL</span>,</span><br><span class="line">`client_id` <span class="type">int</span>(<span class="number">4</span>) <span class="keyword">NOT</span> <span class="keyword">NULL</span>,</span><br><span class="line">`<span class="type">date</span>` <span class="type">date</span> <span class="keyword">NOT</span> <span class="keyword">NULL</span>,</span><br><span class="line"><span class="keyword">PRIMARY</span> KEY (`id`));</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> login <span class="keyword">VALUES</span></span><br><span class="line">(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="string">&#x27;2020-10-12&#x27;</span>),</span><br><span class="line">(<span class="number">2</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="string">&#x27;2020-10-12&#x27;</span>),</span><br><span class="line">(<span class="number">3</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="string">&#x27;2020-10-12&#x27;</span>),</span><br><span class="line">(<span class="number">4</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="string">&#x27;2020-10-13&#x27;</span>),</span><br><span class="line">(<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="string">&#x27;2020-10-13&#x27;</span>),</span><br><span class="line">(<span class="number">6</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="string">&#x27;2020-10-14&#x27;</span>),</span><br><span class="line">(<span class="number">7</span>,<span class="number">4</span>,<span class="number">1</span>,<span class="string">&#x27;2020-10-14&#x27;</span>),</span><br><span class="line">(<span class="number">8</span>,<span class="number">4</span>,<span class="number">1</span>,<span class="string">&#x27;2020-10-15&#x27;</span>);</span><br></pre></td></tr></table></figure>              </div>            </details><br>这道题思路其实很简单，因为是计算新客户的第二天留存率。<br>所以只需要先查出所有所有有新客户登录的日期，记为表t1.<br>再将t1表和login表连接，算出有新客户登录那一天的留存率。<br>最后再将上述表和login连接，算出没有新客户登录日期的留存率即可(全部记为0)</p><p><details class="folding-tag" green><summary> 答案 </summary>              <div class='content'>              <p>with temp as(<br>select user_id,date from<br>(select *,FIRST_VALUE(date)over(partition by user_id order by date asc) first_date from login) login1<br>where date = first_date)</p><p>select lzy1.date,if(lzy2.P is null,0.000,lzy2.P) P from<br>(select date from login group by date) lzy1<br>left join<br>( select  t1.date,round(count(l.user_id)/count(t1.user_id),3) P from temp t1<br>left  join login l on<br>t1.user_id=l.user_id and datediff(t1.date,l.date)=-1<br>group by t1.date) lzy2<br>on lzy1.date = lzy2.date</p>              </div>            </details><br>ok，爷的道心又回来了，没想到吧，我又回来了!!!</p></div>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>昨日小记</title>
      <link href="/2023/07/26/kafka/20230726%E6%80%BB%E7%BB%93/"/>
      <url>/2023/07/26/kafka/20230726%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<div class="note white no-icon flat"><p>昨天基本上又将kafka看了一遍，还看了kafka的StickyAssignor。<br>虽然还是看不懂，但是我感觉我感觉我又变强了。就是这种感觉。<br>粘性分区说白了，就是目前我们所学的消费者消费主题分区策略中最好的一种没有之一。<br>昨天还将搜索的图标页面改了，花了很久。这也是让我最自豪的一点，猜码能力有了明显的进步。<br>下一次要将页脚搞完，页脚不好看。然后是图片，大佬图片太好看了。<br>其他标签研究研究怎么弄。就这么多，over。<br>哈哈哈哈哈，ok，就写这么多</p></div>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 零零散散 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink</title>
      <link href="/2023/07/25/flink/flink/"/>
      <url>/2023/07/25/flink/flink/</url>
      
        <content type="html"><![CDATA[<h1 id="flink学习"><a href="#flink学习" class="headerlink" title="flink学习"></a>flink学习</h1><div class="row">    <embed src="http://localhost:4000/pdf/01flink.pdf" width="100%" height="550" type="application/pdf"></div>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>新成语之众生平等</title>
      <link href="/2023/07/24/20230723%E9%9A%8F%E6%83%B3/"/>
      <url>/2023/07/24/20230723%E9%9A%8F%E6%83%B3/</url>
      
        <content type="html"><![CDATA[<h1 id="众生平等"><a href="#众生平等" class="headerlink" title="众生平等"></a>众生平等</h1><div class="note white no-icon flat"><p>陈:又玩亚索？？</p></div>]]></content>
      
      
      <categories>
          
          <category> 生活 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随想 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2023/07/22/kafka/kafka%E6%95%B0%E6%8D%AE%E5%8F%91%E9%80%81%E4%BF%9D%E8%AF%81/"/>
      <url>/2023/07/22/kafka/kafka%E6%95%B0%E6%8D%AE%E5%8F%91%E9%80%81%E4%BF%9D%E8%AF%81/</url>
      
        <content type="html"><![CDATA[<hr><p>title: kafka学习(三)<br>date: 2023-07-22 15:09:30<br>categories: 学习<br>cover: /img/fomal/default_cover_11.webp<br>swiper_index: 8<br>description: kafka数据发送保证</p><h2 id="tags-kafka"><a href="#tags-kafka" class="headerlink" title="tags: kafka"></a>tags: kafka</h2><h1 id="数据发送保证"><a href="#数据发送保证" class="headerlink" title="数据发送保证"></a>数据发送保证</h1><h2 id="消息语义"><a href="#消息语义" class="headerlink" title="消息语义"></a>消息语义</h2><div class="note white no-icon flat"><ol><li>At Most Once：最多一次消费，数据可能会丢失，但是不会产生数据被重复消费的情况，该层语义<br>的可靠性最低。ACK 级别设置为 0。</li><li>At Least Once：至少一次消费，数据肯定不会丢失，但是可能会导致数据被重复的处理。ACK 级别<br>设置为 all，分区副本数大于等于 2，ISR 里应答的最小副本数大于等于 2</li><li>Exactly Once：精确一次消费，对于一些非常重要的信息，数据不仅不会丢失，也不会被重复的处<br>理。该层语义的基础是 At Least Once。幂等性 + At Least Once。</li><li><details class="folding-tag" green><summary> 提示 </summary>           <div class='content'>           <p>Kafka 0.11 版本以后，引入了一项重大特性：幂等性和事务，用于实现消息的精确消费。幂<br>等性就是指 Producer 不论向 Broker 发送多少次重复数据，Broker 端都只会持久化一条，保证了数<br>据的不重复。</p>           </div>         </details></li></ol></div><h2 id="消息确认机制"><a href="#消息确认机制" class="headerlink" title="消息确认机制"></a>消息确认机制</h2><div class="note white no-icon flat"><ol><li>为保证 Producer 发送的数据，能可靠地发送到指定的 Topic，Topic 的每个 Partition 收到 Producer<br>发送的数据后，都需要向 Producer发送 ACKnowledge 确认收到，如果 Producer 收到 ACK，就会进<br>行下一轮的发送，否则重新发送数据。</li><li>Kafka 为用户提供了三种可靠性级别，用户根据可靠性和延迟的要求进行权衡</li><li>Producers可以选择是否为数据的写入接收ack，有以下几种ack的选项：request.required.acks<ul><li>acks=0：<ul><li>这意味着 Producer 无需等待来自 Leader的确认而继续发送下一批消息。</li><li>当 Broker 故障时有可能丢失数据。</li></ul></li><li>acks=1：<ul><li>Producer 在 ISR 中的 Leader 已成功收到的数据并得到确认后发送下一条 Message。</li><li>如果在 Follower 同步成功之前 Leader 故障，那么将会丢失数据。</li></ul></li><li>acks=-1：<ul><li>Producer 需要等待 ISR 中的所有 Follower 都确认接收到数据后才算一次发送完成，可靠<br>性最高。</li><li>在 Broker 发送 ACK 时，Leader 发生故障，则会造成数据重复。</li></ul></li></ul></li><li>核心代码如下<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// 数据保证 acks，默认为 all</span><br><span class="line">properties.put(ProducerConfig.ACKS_CONFIG, &quot;all&quot;);</span><br></pre></td></tr></table></figure></li><li>总结<ul><li>acks=0，生产者发送数据后就不管了，可靠性差，效率高；</li><li>acks=1，生产者发送数据后 Leader 应答，可靠性中等，效率中等；</li><li>acks=all，生产者发送数据后 Leader 和 ISR 队列里面所有 Follwer 应答，可靠性高，效率低；</li><li>在生产环境中，acks=0 很少使用；acks=1 一般用于传输普通日志，允许丢失个别数据；<br>acks=all 一般用于传输和金钱相关的数据，或对可靠性要求比较高的场景。</li></ul></li></ol></div><h2 id="消息发送丢失"><a href="#消息发送丢失" class="headerlink" title="消息发送丢失"></a>消息发送丢失</h2><div class="note white no-icon flat"><ul><li>消息不是一直都会发送成功的，也可能发送失败。发送失败分为可重试恢复错误和不可重试恢复<br>错误。当然不管是什么错误，只要发送失败了，客户端就会自动的进行失败重试。<ul><li>可重试恢复错误：找不到 Leader；找不到目标分区。这种情况往往重试一下就能发送成功。</li><li>不可重试恢复错误：消息体过大、缓冲区满了。这种情况即使你再重试也是会失败的，因为<br>消息体过大除非你减少消息量，或者采用压缩算法，重试是没用的。</li></ul></li><li>核心代码如下<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">//核心代码如下：</span><br><span class="line">// 消息重发的次数 retries，默认为 2147483647（int 最大值）</span><br><span class="line">properties.put(ProducerConfig.RETRIES_CONFIG, 3);</span><br><span class="line">// 消息重发的间隔时间 retry.backoff.ms，默认为 100ms</span><br><span class="line">properties.put(ProducerConfig.RETRY_BACKOFF_MS_CONFIG, 100);</span><br></pre></td></tr></table></figure></li></ul></div><h2 id="消息发送重复"><a href="#消息发送重复" class="headerlink" title="消息发送重复"></a>消息发送重复</h2><div class="note white no-icon flat"><div class="img-wrap"><div class="img-bg"><img class="img" src="http://localhost:4000/img/img_md/消息发送重复.png"/></div></div><ul><li>分布式系统中，不可控因素很多，比如网络、OOM、FullGC 等。在 Kafka Broker 确认 Ack 时，如果<br>出现网络异常、FullGC、OOM 等问题时会导致 Ack 超时，此时 Producer 会进行重复发送，可能会<br>导致消息被重复处理。以银行结算平台来说，业务方作为上游把数据上报到银行结算平台，如果一<br>份数据被计算、处理多次，那么产生的影响可想而知。</li></ul></div><h2 id="Kafka幂等性"><a href="#Kafka幂等性" class="headerlink" title="Kafka幂等性"></a>Kafka幂等性</h2><div class="note white no-icon flat"><ul><li>概念<ol><li>保证在消息重发的时候，消费者不会重复处理。即使在消费者收到重复消息的时候，重复处<br>理，也要保证最终结果的一致性。</li><li>所谓幂等性，数学概念就是： f(f(x)) = f(x) 。f函数表示对消息的处理。</li><li>比如，银行转账，如果失败，需要重试。不管重试多少次，都要保证最终结果一定是一致<br>的。</li></ol></li><li>实现<ol><li>Kafka 引入了 Producer ID(即 PID) 和 Sequence Number。<ul><li>ProducerID：在每个新的Producer初始化时，会被分配一个唯一的ProducerID，这个<br>ProducerID对客户端使用者是不可见的。</li><li>SequenceNumber：对于每个ProducerID，Producer发送数据的每个Topic和Partition都对<br>应一个从0开始单调递增的SequenceNumber值。</li></ul></li><li>Broker 端也会为每个 <PID, Topic, Partition> 维护一个序号，每次 Commit 一条消息时将其对<br>应序号递增。对于接收的每条消息：比较生产者端序号和Broker端序号<ul><li>等于1，则 Broker 会接受它，否则将其丢弃。</li><li>大于1，说明中间有数据尚未写入，也即乱序，此时 Broker 拒绝该消息，Producer 抛出<br>InvalidSequenceNumber 异常。</li><li>小于1，说明该消息已被保存，即为重复消息，Broker 直接丢弃该消息，Producer 抛出<br>DuplicateSequenceNumber 异常。</li><li>总结下来，重复数据的判断标准为：具有 <PID, Partition, SeqNumber> 相同主键的消息提交<br>时，Broker 只会持久化一条。所以幂等性只能保证在单分区单会话内不重复</li></ul></li><li>实现方式<ul><li>通过参数 enable.idempotence=true 开启幂等性，默认为 true 开启。</li></ul></li><li>幂等性引用之前<ul><li>Kafka 在引入幂等性之前，Producer 向 Broker 发送消息，Broker 将消息追加到消息流中后给<br>Producer 返回 Ack 信号值。实现流程如下：<div class="img-wrap"><div class="img-bg"><img class="img" src="http://localhost:4000/img/img_md/幂等性引用之前.png"/></div></div></li><li>上图的实现流程是一种理想状态下的消息发送情况，但是实际情况中，会出现各种不确定的<br>因素，比如 Producer 在发送给 Broker 的时候出现网络异常。具体异常情况如下：<div class="img-wrap"><div class="img-bg"><img class="img" src="http://localhost:4000/img/img_md/幂等性引用之前2.png"/></div></div></li><li>上图这种情况，当 Producer 第一次发送消息给 Broker 时，Broker 将消息(x2,y2)追加到了消息<br>流中，但是在返回 Ack 信号给 Producer 时失败了（比如网络异常） 。此时，Producer 端触发<br>重试机制，将消息(x2,y2)重新发送给 Broker，Broker 接收到消息后，再次将该消息追加到消<br>息流中，然后成功返回 Ack 信号给 Producer。这样下来，消息流中就被重复追加了两条相同<br>的(x2,y2)的消息。 </li></ul></li></ol></li></ul></div><h2 id="kafka事务机制"><a href="#kafka事务机制" class="headerlink" title="kafka事务机制"></a>kafka事务机制</h2><div class="note white no-icon flat"><ul><li>事务意义<ol><li>kafka的事务机制，是kafka实现端到端有且仅有一次语义（end-to-end EOS)的基础；事务涉及到<br>transactional producer 和transactional consumer, 两者配合使用，才能实现端到端有且仅有一次的语<br>义（end-to-end EOS)，producer和consumer是解耦的，也可以使用非transactional的consumer来消<br>费transactional producer生产的消息，但此时就丢失了事务ACID的支持；</li><li>通过事务机制，kafka可以实现对多个topic的多个partition的原子性的写入，即处于同一个事务内<br>的所有消息，不管最终需要落地到哪个topic的哪个partition, 最终结果都是要么全部写成功，要么<br>全部写失败（Atomic multi-partition writes）；kafka的事务机制，在底层依赖于幂等生产者，幂等<br>生产者是kafka事务的必要不充分条件；</li><li>事实上，开启kafka事务时，kafka会自动开启幂等生产者</li></ol></li><li>设计原理<ol><li>transaction coordinator是kafka broker内部的一个模块，transaction coordinator负责对分区写操作进<br>行控制，而transaction log是kakfa的一个内部topic, 所以kafka可以通过内部的复制协议和选举机制<br>（replication protocol and leader election processes)，来确保transaction coordinator的可用性和<br>transaction state的持久性；transaction log topic内部存储的只是事务的最新状态和其相关元数据信<br>息，kafka producer生产的原始消息，仍然是只存储在kafka producer指定的topic中；</li><li>Procedure就是和Transaction Coordinator交互获得TransactionID对应的任务状态。Transaction<br>Coordinator还负责将事务写入kafka内部的一个topic，这样即使整个服务重启，由于事务状态得到<br>保存，正在进行的事务状态可以得到恢复，从而继续进行；</li></ol></li><li>实现流程<ol><li>kafka生产者通过initTransactions API将 transactional.id注册到 transactional coordinator：此时，此<br>时 coordinator会关闭所有有相同transactional.id 且处于pending状态的事务，同时也会递增epoch<br>来屏蔽僵尸生产者 （zombie producers）. 该操作对每个 producer session只执行一次<br>(producer.initTransaction())；</li><li>kafka生产者通过beginTransaction API开启事务，并通过send API发送消息到目标topic：此时消息对<br>应的 partition会首先被注册到transactional coordinator，然后producer按照正常流程发送消息到目<br>标topic，且在发送消息时内部会通过校验屏蔽掉僵尸生产者（zombie producers are fenced out.<br>（producer.beginTransaction();producer.send()*N;）；</li><li>kafka生产者通过commitTransaction API提交事务或通过abortTransaction API回滚事务：此时会向<br>transactional coordinator提交请求，开始两阶段提交协议<br>(producer.commitTransaction();producer.abortTransaction(););<ul><li>在两阶段提交协议的第一阶段，transactional coordinator 更新内存中的事务状态为<br>“prepare_commit”，并将该状态持久化到transaction log中；</li><li>在两阶段提交协议的第二阶段， coordinator首先写transaction marker标记到目标topic的目标<br>partition，这里的transaction marker，就是我们上文说的控制消息，控制消息共有两种类型：<br>commit和abort，分别用来表征事务已经成功提交或已经被成功终止；</li><li>在两阶段提交协议的第二阶段，coordinator在向目标topic的目标partition写完控制消息后，会<br>更新事务状态为“commited” 或“abort”， 并将该状态持久化到transaction log中；</li></ul></li><li>kafka消费者消费消息时可以指定具体的读隔离级别，当指定使用read_committed隔离级别时，在<br>内部会使用存储在目标topic-partition中的事务控制消息，来过滤掉没有提交的消息，包括回滚的<br>消息和尚未提交的消息；kafka消费者消费消息时也可以指定使用read_uncommitted隔离级别，此<br>时目标topic-partition中的所有消息都会被返回，不会进行过滤；</li></ol></li><li>配置文件<ol><li>producer 配置项更改：<ul><li>enable.idempotence = true</li><li>acks = “all”</li><li>retries &gt; 1 (preferably MAX_INT)</li><li>transactional.id = ‘some unique id’</li></ul></li><li>consumer 配置项更改：<ul><li>根据需要配置 isolation.level 为 “read_committed”, 或 “read_uncommitted”；</li></ul></li><li>代码实现<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">This specifies that the KafkaConsumer should only read non-transactional</span></span><br><span class="line"><span class="comment">messages,</span></span><br><span class="line"><span class="comment">or committed transactional messages from its input topics.</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="type">KafkaConsumer</span> <span class="variable">consumer</span> <span class="operator">=</span> createKafkaConsumer(</span><br><span class="line">“bootstrap.servers”, “localhost:<span class="number">9092</span>”,</span><br><span class="line">“group.id”, “my-consumerGroup-id”,</span><br><span class="line"><span class="string">&quot;isolation.level&quot;</span>, <span class="string">&quot;read_committed&quot;</span>);</span><br><span class="line">consumer.subscribe(Collections.singleton(“inputTopic”));</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">Consume some records, start a transaction, process the consumed records,</span></span><br><span class="line"><span class="comment">write the processed records to the output topic, send the consumed offsets to</span></span><br><span class="line"><span class="comment">the offsets topic, and finally commit the transaction. With the guarantees</span></span><br><span class="line"><span class="comment">mentioned</span></span><br><span class="line"><span class="comment">above, we know that the offsets and the output records will be committed as</span></span><br><span class="line"><span class="comment">an atomic</span></span><br><span class="line"><span class="comment">unit.</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line"><span class="type">ConsumerRecords</span> <span class="variable">records</span> <span class="operator">=</span> consumer.poll(Long.MAX_VALUE);</span><br><span class="line">producer.beginTransaction();</span><br><span class="line"><span class="keyword">for</span> (ConsumerRecord record : records)</span><br><span class="line">producer.send(<span class="keyword">new</span> <span class="title class_">ProducerRecord</span>(“outputTopic”, record));</span><br><span class="line"><span class="comment">//This method should be used when you need to batch consumed and produced messages</span></span><br><span class="line"><span class="comment">//together, typically in a consume-transform-produce pattern.</span></span><br><span class="line">producer.sendOffsetsToTransaction(currentOffsets(consumer), my-consumerGroup-id);</span><br><span class="line">producer.commitTransaction();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li>事务主题</li><li>事务相关数据存储在 __transaction_state 主题中，默认有 50 个分区，每个分区负责一部分事<br>务。事务划分是根据 transactional.id 的 hashcode 值对 50 取模，计算出该事务属于哪个分区。该<br>分区 Leader 副本所在的 Broker 节点即为这个 transactional.id 对应的 Transaction Coordinator 节点<br>（事务协调器，用于生成 ProducerID，持久化事务信息等）。<ul><li><div class="img-wrap"><div class="img-bg"><img class="img" src="http://localhost:4000/img/img_md/事务主题.png"/></div></div></li></ul></li><li>数据顺序保证<ul><li>Kafka 可以保证同一个分区里的消息是有序的，按照生产者发送顺序进行写入分区。消费者也是按<br>照同样的顺序进行读取。某些情况下顺序很重要，比如先存钱再消费和先消费再存钱就是两种性<br>质。什么情况下会导致顺序不一致呢。如果把 retries 设置为比 0 大的整数，同时把<br>max.in.flight.requests.per.connection 设置为比 1 大的整数，那么，第一个批次消息写入失败，第二<br>个批次消息写入成功，然后第一个批次重试后成功，这时候两个批次的顺序就反过来了。<br><div class="note green no-icon flat"><p>max.in.flight.requests.per.connection：生产者在收到服务器响应之前可以发送多少个请求<br>（消息），默认为 5。它的值越高，占用的内存越多，不过会提升吞吐量，把它设为 1 可以<br>保证消息是按照发送的顺序写入服务器的（即使发生了重试）。如果设置为比 1 大的整数，<br>发生错误时，可能会造成数据的发送顺序改变。</p></div><br><ul><br><li>如果有些场景要求顺序是有序的，那么可以把 retries 设置为 0（失败不重试），但是往往消息是否<br>写入成功也是很关键的，因此可以把 max.in.flight.requests.per.connection 设置为 1。这样生产者在<br>发送第一个批次消息时，就不会有其他消息发送给 Broker。但是这样会严重影响生产者的吞吐<br>量，因此需要仔细评估是否对顺序有严格要求。<ol><li>Kafka 在 1.x 版本之前保证数据单分区有序的条件如下：<br>max.in.flight.requests.per.connection=1（不需要考虑是否开启幂等性）。</li><br><li>Kafka 在 1.x 版本以及以后版本中保证数据单分区有序的条件如下：<ul><li>未开启幂等性：max.in.flight.requests.per.connection=1；</li><br><li>开启幂等性：max.in.flight.requests.per.connection 需要设置为小于等于 5。因为 Kafka<br>1.x 以后，启用幂等后，Kafka 服务端会缓存 Producer 发来的最近 5 个 Request 的元数<br>据，所以无论如何，Kafka 都可以保证最近 5 个 Request 的数据都是有序的。</li></ul></li></ol></li></ul></div></li></ul></li></ol></li></ul><h2 id="数据序列化"><a href="#数据序列化" class="headerlink" title="数据序列化"></a>数据序列化</h2><div class="note white no-icon flat"><ul><li>Kafka 生产者将对象序列化成字节数组并发送到服务器，消费者需要将字节数组转换成对象（反序<br>列化）。序列化与反序列化需要匹配，推荐使用 Avro 序列化方式。</li><li>相关依赖<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;io.confluent&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;kafka-avro-serializer&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;5.3.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;io.confluent&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;kafka-schema-registry-client&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;5.3.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;io.confluent&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;common-utils&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;5.3.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;io.confluent&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;common-config&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;5.3.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure></li><li>代码实现<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 生产者</span></span><br><span class="line"><span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">properties.put(<span class="string">&quot;key.serializer&quot;</span>,</span><br><span class="line"><span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line">properties.put(<span class="string">&quot;value.serializer&quot;</span>,</span><br><span class="line"><span class="string">&quot;io.confluent.kafka.serializers.KafkaAvroSerializer&quot;</span>);</span><br><span class="line"><span class="comment">// 添加 schema 服务的地址，用于获取 schema</span></span><br><span class="line"><span class="comment">// AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG =&gt;</span></span><br><span class="line">schema.registry.url</span><br><span class="line">properties.put(<span class="string">&quot;schema.registry.url&quot;</span>, <span class="string">&quot;http://hadoop01:8081&quot;</span>);</span><br><span class="line">Producer&lt;String, Object&gt; producer = <span class="keyword">new</span> <span class="title class_">KafkaProducer</span>&lt;&gt;(properties);</span><br><span class="line"><span class="comment">// 消费者</span></span><br><span class="line"><span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">properties.put(<span class="string">&quot;key.deserializer&quot;</span>,</span><br><span class="line"><span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">properties.put(<span class="string">&quot;value.deserializer&quot;</span>,</span><br><span class="line"><span class="string">&quot;io.confluent.kafka.serializers.KafkaAvroDeserializer&quot;</span>);</span><br><span class="line"><span class="comment">// 添加 schema 服务的地址，用于获取 schema</span></span><br><span class="line">properties.put(<span class="string">&quot;schema.registry.url&quot;</span>, <span class="string">&quot;http://hadoop01:8081&quot;</span>);</span><br><span class="line">Consumer&lt;String, String&gt; consumer = <span class="keyword">new</span> <span class="title class_">KafkaConsumer</span>&lt;&gt;(properties);</span><br></pre></td></tr></table></figure></li></ul></div>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>kafka学习(二)</title>
      <link href="/2023/07/22/kafka/kafka%E5%89%AF%E6%9C%AC%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/"/>
      <url>/2023/07/22/kafka/kafka%E5%89%AF%E6%9C%AC%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/</url>
      
        <content type="html"><![CDATA[<h1 id="副本故障处理"><a href="#副本故障处理" class="headerlink" title="副本故障处理"></a>副本故障处理</h1><h2 id="Leader故障"><a href="#Leader故障" class="headerlink" title="Leader故障"></a>Leader故障</h2><div class="note white no-icon flat"><ol><li>Leader 发生故障后，优先从 ISR 中选出一个新的 Leader</li><li>为保证多个副本之间的数据一致性，竞选失败的 Follower 会先将各自的 log 文件高于 HW 的部分截<br>掉，然后从新的 Leader 同步数据。</li><li>这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复。</li><li>下面都是图，就不抠过来了，理解理解就行</li></ol></div>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>clickhouse学习之系统架构</title>
      <link href="/2023/07/22/clickhouse%E5%AD%A6%E4%B9%A0(1)/"/>
      <url>/2023/07/22/clickhouse%E5%AD%A6%E4%B9%A0(1)/</url>
      
        <content type="html"><![CDATA[<h1 id="Column与Field"><a href="#Column与Field" class="headerlink" title="Column与Field"></a>Column与Field</h1><div class="note white no-icon flat"><ol><li>Column和Field是ClickHouse数据最基础的映射单元。内存中的一列数据由一个Column对象表示。</li><li>Column对象分为接口和实现两个部分，在IColumn接口对象中，定义了对数据进行各种关系运算的<br>方法。几乎所有的操作都是不可变的：这些操作不会更改原始列，但是会创建一个新的修改后的<br>列。</li><li>在大多数场合，ClickHouse都会以整列的方式操作数据，但凡事也有例外。如果需要操作单个具体<br>的数值 ( 也就是单列中的一行数据 )，则需要使用Field对象，Field对象代表一个单值。</li><li>与Column对象的泛化设计思路不同，Field对象使用了聚合的设计模式。在Field对象内部聚合了<br>Null、UInt64、String和Array等13种数据类型及相应的处理逻辑。</li></ol></div><h1 id="数据类型DataType"><a href="#数据类型DataType" class="headerlink" title="数据类型DataType"></a>数据类型DataType</h1><div class="note white no-icon flat"><ol><li>IDataType 负责序列化和反序列化：读写二进制或文本形式的列或单个值构成的块。<br>IDataType 直接与表的数据类型相对应。</li><li>IDataType 与 IColumn 之间的关联并不大。不同的数据类型在内存中能够用相同的<br>IColumn 实现来表示。</li><li>IDataType 仅存储元数据。</li><li>数据的序列化和反序列化工作由DataType负责。IDataType接口定义了许多正反序列化的方法，它<br>们成对出现。IDataType也使用了泛化的设计模式，具体方法的实现逻辑由对应数据类型的实例承<br>载。</li><li>DataType虽然负责序列化相关工作，但它并不直接负责数据的读取，而是转由从Column或Field对<br>象获取。</li></ol></div>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> clickhouse </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kafka学习(一)</title>
      <link href="/2023/07/21/kafka/kafka%E9%9B%86%E7%BE%A4%E6%9E%B6%E6%9E%84%E5%AE%9A%E4%B9%89/"/>
      <url>/2023/07/21/kafka/kafka%E9%9B%86%E7%BE%A4%E6%9E%B6%E6%9E%84%E5%AE%9A%E4%B9%89/</url>
      
        <content type="html"><![CDATA[<h1 id="AR-ISR-OSR"><a href="#AR-ISR-OSR" class="headerlink" title="AR ISR OSR"></a>AR ISR OSR</h1><div class="note white no-icon flat"><ul><li>前提概念：<ul><li>kafka 的数据是多副本的，每个Partition都会有N个副本(replication)。每个topic 下的每个分区<br>下都有1个leader 和(N-1)个follower</li><li>每个follower 的数据都是同步leader的，follower 主动拉取leader 的数据</li><li>leader负责指定分区所有读写操作，follower复制指定分区日志备份和leader失效后的选举<br>Replication的个数应小于等于Broker的个数</li></ul></li><li>关键词<ul><li>AR : Assigned Replicas 用来标识副本的全集</li><li>OSR ：out -sync Replicas 离开同步队列的副本</li><li>ISR ：in -sync Replicas 加入同步队列的副本</li></ul></li><li>计算公式：<ul><li>ISR = Leader + 没有落后太多的副本;</li><li>AR = OSR+ ISR。</li></ul></li><li>ISR判定标准标准<ul><li>默认10s，isr中的follow没有向leader发送心跳包就会被移除</li><li>主副节点差4000条数据</li><li>kafka0.9版本之后移除了第二个判断条件，只保留了第一个，极端情况下，如果producor一次<br>性发来了10000条数据，默认条数差立马会大于4000</li><li>切记切记：即使在ISR中的Follow也是和Leader有数据差</li></ul></li></ul></div><h1 id="LEO-HW"><a href="#LEO-HW" class="headerlink" title="LEO HW"></a>LEO HW</h1><div class="note white no-icon flat"><ul><li>下图是LEO和HW的形象图，生动形象 <ul><li><div class="img-wrap"><div class="img-bg"><img class="img" src="http://localhost:4000/img/img_md/LEO_HW.png"/></div></div></li></ul></li><li>LEO：log end ow set<ul><li>日志末端偏移量，代表日志文件中下一条待写入消息的ow set，这个ow set上实际是没有消息<br>的。</li><li>不管是leader副本还是follower副本，都有这个值。当leader副本收到生产者的一条消息，LEO<br>通常会自增1，而follower副本需要从leader副本fetch到数据后，才会增加它的LEO，最后<br>leader副本会比较自己的LEO以及满足条件的follower副本上的LEO，选取两者中较小值作为新<br>的HW，来更新自己的HW值。</li></ul></li><li>HW：high watermark<ul><li>副本的高水印值，取值于所有副本中最小的LEO值。所以仅HW之前数据的对消费者可见</li><li>replica中leader副本和follower副本都会有这个值，通过它可以得知副本中已提交或已备份消<br>息的范围，HW决定了消费者能消费的最新消息能到哪个ow set。</li><li>剩下的没了，都是图，意思就是这么个意思，懂得都懂。</li><li>微观描述LEO与HW的变化关系<ul><li>生产者客户端发送消息至Leader副本中；</li><li>消息被追加到Leader副本的本地日志，并且会更新日志的偏移量。</li><li>当Follower副本向Leader副本拉取消息，在拉取的请求中会带有自身的LEO信息，这个LEO信<br>息对应的是FetchRequest请求中的fetch_ow set。</li><li>Leader副本根据Follower副本偏移量读取本地日志，并根据请求更新副本的信息<ul><li>例如重新计算HW值 HW = min（Leader_LEO,Follower1_LEO,Follower2_LEO,…）。</li></ul></li><li>Leader副本将新增日志和HW等信息返回给Follower副本。</li><li>Follower副本将收到Leader副本返回的日志追加到本地日志中，然后更新日志的偏移量信息、<br>LEO以及HW。</li><li>接下来 Follower 副本再次请求拉取 Leader 副本中的消息，在拉取的请求中依然会带有自身的<br>LEO信息</li><li>Leader副本根据Follower副本偏移量读取本地日志，并根据请求更新副本的信息，后续重复更<br>新流程</li></ul></li><li>了解即可<ul><li>LSO：是 LastStableOw set 的简称，对未完成的事务而言，LSO 的值等于事务中第一条消息的<br>位置(firstUnstableOw set)，对已完成的事务而言，它的值同 HW 相同</li><li>LW：Low Watermark 低水位, 代表 AR 集合中最小的 logStartOw set 值</li></ul></li></ul></li></ul></div><h1 id="CheckPoint"><a href="#CheckPoint" class="headerlink" title="CheckPoint"></a>CheckPoint</h1><div class="note white no-icon flat"><p>这老师的图，别问，我也不懂，看就完事了</p><p><div class="img-wrap"><div class="img-bg"><img class="img" src="http://localhost:4000/img/img_md/CheckPoint.png"/></div>&lt;/div&gt;</p><ol><li>在broker的log存储文件下，除了存储这各个topic的文件夹，还存在这几个checkpoint文件。<ol><li>recovery-point-ow set-checkpoint：<ul><li>表示已经刷写到磁盘的ow set信息，对应LEO信息</li><li>kafka中会有一个定时任务负责将所有分区的LEO刷写到恢复点文件recovery-point-ow setcheckpoint中，定时周期由broker端参数log.flush.ow set.checkpoint.interval.ms配置，默<br>认值60000，即60s。</li><li>Kafka在启动时会检查文件的完整性，如果没有.kafka_cleanshutdown这个文件，就会进<br>入一个recover逻辑，recover就是从此文件中的ow set开始。</li></ul></li></ol></li><li>replication-ow set-checkpoint：<ul><li>用来存储每个replica的HW，表示已经被commited的ow set信息。</li><li>失败的follower开始恢复时，会首先将自己的日志截断到上次的checkpointed时刻的<br>HW，然后向leader拉取消息。</li><li>kafka有一个定时任务负责将所有分区的HW刷写到复制点文件replication-ow setcheckpoint中，定时周期由broker端参数replica.high.watermark.checkpoint.interval.ms配<br>置，默认值5000，即5s。</li></ul></li><li>log-start-ow set-checkpoint：<ul><li>对应logStartOw set，用来标识日志的起始偏移量。</li><li>kafka中有一个定时任务负责将所有分区的logStartOw set刷写到起始点文件log-startow set-checkpoint中，定时周期有broker端参数log.flush.start.ow set.checkpoint.interval.ms<br>配置，默认值60000，即60s。</li></ul></li><li>cleaner-ow set-checkpoint：<ul><li>存了每个log的最后清理ow set。</li></ul></li><li>checkpointing大体的作用都是将Kafka Broker端重要的日志元数据保存下来，避免后面“书到用时方<br>恨少”的尴尬。</li></ol></div>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>页顶大波浪</title>
      <link href="/2023/07/21/%E4%B8%87%E4%B8%88%E6%B7%B1%E6%B8%8A/hexo%E9%AD%94%E6%94%B9%E4%B9%8B%E9%A1%B5%E9%A1%B6%E5%A4%A7%E6%B3%A2%E6%B5%AA/"/>
      <url>/2023/07/21/%E4%B8%87%E4%B8%88%E6%B7%B1%E6%B8%8A/hexo%E9%AD%94%E6%94%B9%E4%B9%8B%E9%A1%B5%E9%A1%B6%E5%A4%A7%E6%B3%A2%E6%B5%AA/</url>
      
        <content type="html"><![CDATA[<div class="note white no-icon flat"><p>太痛苦辣，太痛苦辣。我到了下午才知道pug格式也是和yml一样按照缩进嵌套的。<br>md，我弄了一下午。整体上还是比较好配置的。<br>一共分为两部，第一步是修改源码。套用安知鱼大佬的代码进行修改。<br>第二步就是加一点css样式。当然，我是不会写的</p></div>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kafka第二天</title>
      <link href="/2023/07/21/kafka/kafka_day02/"/>
      <url>/2023/07/21/kafka/kafka_day02/</url>
      
        <content type="html"><![CDATA[<h1 id="kafka学习第二天"><a href="#kafka学习第二天" class="headerlink" title="kafka学习第二天"></a>kafka学习第二天</h1><h2 id="kafka生产者"><a href="#kafka生产者" class="headerlink" title="kafka生产者"></a>kafka生产者</h2><div class="note white no-icon flat"><p>事实上，kafka的生产者远远比第一天所想的那样要复杂，果然。</p></div><h2 id="数据发送流程"><a href="#数据发送流程" class="headerlink" title="数据发送流程"></a>数据发送流程</h2><div class="note white no-icon flat"><div class="img-wrap"><div class="img-bg"><img class="img" src="http://localhost:4000/img/img_md/kafka数据发送流程.png"/></div></div></div><h3 id="生产者数据格式"><a href="#生产者数据格式" class="headerlink" title="生产者数据格式"></a>生产者数据格式</h3><div class="note white no-icon flat"><p>这个是比较好理解的，就相当于给数据穿衣服，更有利于kafka内部识别分析。</p><ul><li>topic：string 类型，NotNull。</li><li>partition：int 类型，可选。</li><li>timestamp：long 类型，可选。</li><li>key：string 类型，可选。</li><li>value：string 类型，可选。</li><li>headers：array 类型，Nullable。</li></ul></div><h3 id="ProducerInterceptor"><a href="#ProducerInterceptor" class="headerlink" title="ProducerInterceptor"></a>ProducerInterceptor</h3><div class="note white no-icon flat"><p>拦截器（Interceptor）功能最早在kafka0.10.0.0中引入，<br>kafka一共有两种拦截器：生产者拦截 器和消费者拦截器。生产者拦截器可以用来在消息发送前做一些处理</p></div><h3 id="Serializer"><a href="#Serializer" class="headerlink" title="Serializer"></a>Serializer</h3><div class="note white no-icon flat"><ul><li>生产者需要使用序列化（Serializer）将对象转换成字节数组才能够通过网络发送给kafka。</li><li>消费者端需要使用反序列化（Deserializer）把从kafka接收到的字节数组转换成相应的对象。<br>总结：IO必备的技术，因为传递的是对象流，所以基本都要序列化，以及反序列化</li></ul></div><h3 id="Partitoner"><a href="#Partitoner" class="headerlink" title="Partitoner"></a>Partitoner</h3><div class="note white no-icon flat"><ul><li>分区选择器，默认是对于key进行hash计算然后对于总分区数求模以此得到被发送的分区号，<br>当然我们实现producer时可以自定义partition，或者指定特定分区。</li><li>分区器的作用就是为消息分配消息。单个分区的消息是有序的。</li></ul></div><h3 id="RecordAccumulator"><a href="#RecordAccumulator" class="headerlink" title="RecordAccumulator"></a>RecordAccumulator</h3><div class="note white no-icon flat"><ul><li>主线程调用send()方法将已经和topic分区绑定好的消息，以分区为单位维护一个双端队<br>列，将消息缓存起来。</li><li>当达到一定的条件，会唤醒Sender线程发送RecordAccumulator里面的消息。<ol><li>buw er.memory：设置生产者内存缓冲区的大小，默认为 33554432Byte（32M）。如果<br>应用程序发送消息的速度大于缓冲区往服务器发送的速度，那么就会导致缓冲区空间不<br>足，这时候 send() 方法要么阻塞要么抛出异常，取决于 max.block.ms 参数，表示在抛<br>出异常之前可用阻塞一段时间。</li><li>max.block.ms：该参数指定了在调用 send() 方法或者使用 partitionsFor() 方法获取元数<br>据时生产者的阻塞时间，默认为 60000ms（1分钟）。当生产者的发送缓冲区已满或者<br>没有可用的元数据的时候，就会阻塞。如果阻塞到达设定的时间，生产者就会抛出超时<br>异常。</li><li>batch.size：批次大小，默认 16384Byte（16K）。当有多个消息需要发送到同一个分区<br>的时候，生产者会将他们放到同一个批次里，该参数指定了一个批次可以使用的内存大<br>小，按照字节计算，当批次被填满，批次里的消息会被发送出去，当然生产者不一定都<br>会等批次被填满才发送，半满的批次甚至只有一个消息的批次也有可能被发送。</li><li>linger.ms：等待时间，默认 0ms，即来一条发一条。该参数指定了生产者在发送批次之<br>前等待更多消息加入批次的时间，生产者会在批次填满或者达到这个时间时把批次发送<br>出去，默认情况下，只要有可用的线程，生产者就会把消息发送出去，就算批次里只有<br>一个消息。把 linger.ms 设置成比 0 大的数，让生产者在发送批次之前等待一会儿，使更<br>多消息加入到这个批次，这样虽然会增加延迟，但是也会提升吞吐量，因为一次性发送<br>更多消息，每个消息的开销就变小了。</li><li>compression.type：消息压缩算法，默认不启用压缩。可以指定为：gzip、snappy、lz4<br>或 zstd，使用压缩可以降低网络传输开销和存储开销，这往往是 Kafka 发送消息的瓶颈<br>所在。</li><li>retries：发生错误后，消息重发的次数，默认为 2147483647（int 最大值）。如果达到<br>设定值，生产者就会放弃重试并返回错误。默认情况下，每次重试之间会等待 100 ms，<br>不过可以通过 retry.backow .ms 参数来改变这个时间间隔。</li><li>retry.backow .ms：发生错误后，消息重发的间隔时间，默认为 100ms。</li><li>max.request.size：限制生产者在单次请求中发送的字节量，以避免发送庞大的请求。</li><li><details class="folding-tag" yellow><summary> 核心代码 </summary>         <div class='content'>         <figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">//核心代码如下</span><br><span class="line">//缓冲区大小buffer.memory，默认为32m</span><br><span class="line">properties.put(ProducerConfig.BUFFER_MEMORY_CONFIG,33554432);</span><br><span class="line">//批次大小batch.size，默认为16k</span><br><span class="line">properties.put(ProducerConfig.BATCH_SIZE_CONFIG,16384);</span><br><span class="line">//等待时间linger.ms，默认为0ms，即来一条发一条</span><br><span class="line">properties.put(ProducerConfig.LINGER_MS_CONFIG,1);</span><br><span class="line">//数据压缩compression.type，默认为不启用压缩，可配置值：gzip、snappy、1z4或zstd</span><br><span class="line">properties.put(ProducerConfig.COMPRESSION_TYPE_CONFIG,&quot;snappy&quot;);</span><br></pre></td></tr></table></figure>         </div>       </details></li><li><details class="folding-tag" yellow><summary> 总结 </summary>        <div class='content'>        <p>这个第九根本看不懂，其他都比较好懂，就主要讲述了数据发送流程的几个步骤，还是比较好懂的。<br>这个第9个，我浅浅的猜一下哈。第九个差不多就是说生产者是怎么发送数据到对应分区中的。<br>大致就是这么个意思，然后还有许多机制，差不多就是这样</p>        </div>      </details></li><li><details class="folding-tag" yellow><summary> kafka生产者架构执行步骤 </summary>        <div class='content'>        <ol><li>主线程KafkaProducer创建消息对象ProducerRecord，然后通过生产者拦截器。</li><li>生产者拦截器对消息的key ，value做一定的处理，交给序列化器。</li><li>序列化器对消息key和vlue做序列化处理，然后给分区器。</li><li>分区器给消息分配分区、并发送给消息收集器。</li><li>将一条ProducerRecord添加到RecordAccumulator，首先会根据分区确定对应分区所在的双端<br>队列，在双端队列获取尾部的一个ProducerBatch对象，查看该ProducerBatch是否可以写入该<br>ProducerRecord消息，如果可以则写入，不能的话，会在双端队列末尾在创建一个<br>ProducerBatch对象，创建时会评估这条消息是否是否超过batch.size参数的大小。如果不超<br>过，就以batch.size参数的大小来创建ProducerBatch对象。通知sender线程发送消息。</li><li>sender线程获取RecordAccumulator中的消息，需要将原本的&lt;分区，Deque&gt;形式再次封装成<br><Node,List的形式，其中Node节点表示Kafka集群中的broker节点。对于网络连接而言，生产者客户端是与具体broker节点建立连接，也就是向具体的broker节点发送消息。而并不关心消//核心代码如下// 缓冲区大小 buffer.memory，默认为 32mproperties.put(ProducerConfig.BUFFER_MEMORY_CONFIG, 33554432);// 批次大小 batch.size，默认为 16kproperties.put(ProducerConfig.BATCH_SIZE_CONFIG, 16384);// 等待时间 linger.ms，默认为 0ms，即来一条发一条properties.put(ProducerConfig.LINGER_MS_CONFIG, 1);// 数据压缩 compression.type，默认为不启用压缩，可配置值：gzip、snappy、1z4 或zstdproperties.put(ProducerConfig.COMPRESSION_TYPE_CONFIG, "snappy");息属于哪个分区；sender线程还会进一步封装成<Node,Request>的形式，这样就可以将请求<br>发往各个Node了。这里的Request是指kafka的各种请求协议。</li><li>sender线程发往Kafka之前还会保存到InFlightRequest中，InFlightRequest保存对象的具体形<br>式为Map<NodeId,Deque>,他的主要作用是缓存了已经发出去但是还没有收到相应的请求。</li><li>sender将Request交给Selector准备发送。</li><li>Selector将Request发送到对应的kafka节点（Broker）。</li><li>Selector相应结果反馈给InFlightRequest。</li><li>主线程清理RecordAccumulator已经发送完毕的消息。</li></ol>        </div>      </details> </li></ol></li></ul></div><h2 id="数据分区发送"><a href="#数据分区发送" class="headerlink" title="数据分区发送"></a>数据分区发送</h2><div class="note white no-icon flat"><p><div class="img-wrap"><div class="img-bg"><img class="img" src="http://localhost:4000/img/img_md/数据分区发送.png"/></div>&lt;/div&gt;</p><ul><li>在 Kafka 中，Topic 是一个逻辑上的概念，而 Partition 则是一个物理上的概念，消息最终都是存储<br>在Partition 中的，但主题中的每条消息只会保存在一个分区中，而不会在多个分区中被保存多份。</li><li>Kafka 支持使用分区来解决水平扩展的问题，同时还可以解决消息顺序读取的问题，其实还可以解<br>决负载均衡的问题。<ul><li>水平扩展：通过分区可以把海量数据按照分区切割成一块一块的数据存储在多台 Broker 上；</li><li>消息顺序读取：将需要顺序处理的消息，按照一定的顺序发送到同一个分区中，订阅了这个<br>分区的消费者就可以按照相同的顺序对消息进行处理；</li><li>提高并行度：生产者可以以分区为单位发送数据，消费者可以以分区为单位消费数据；</li><li>负载均衡：合理控制分区的任务，可以实现负载均衡的效果。</li></ul></li><li><details class="folding-tag" yellow><summary> 创建ProducerRecord的常见方式 </summary>            <div class='content'>            <div class="img-wrap"><div class="img-bg"><img class="img" src="http://localhost:4000/img/img_md/创建ProducerRecord的常见方式.png"/></div></div>            </div>          </details></li><li><details class="folding-tag" yellow><summary> 总结 </summary>            <div class='content'>            <p>这一块就明显不如上一块了，这一块就主要讲的是topic分区的好处了，好处就如上面所言，这纯纯八股文</p>            </div>          </details></li></ul></div><h2 id="发送分区策略"><a href="#发送分区策略" class="headerlink" title="发送分区策略"></a>发送分区策略</h2><div class="note white no-icon flat"><ul><li>有了分区之后，我们自然希望数据能均匀地分布到各个分区上，而不是某个分区数据满了，其他分<br>区却一条数据都没有，这样就造成了数据倾斜。那么，怎么才能将数据均匀地分布到各个分区上<br>呢？这就要依赖于分区策略了。在 Kafka 中，分区器的顶级接口是 Partitioner 接口。</li><li>KafkaProducer源码给出答案： 优先选择ProducerRecord自带partition，如果没有，则使用<br>Partitioner接口计算partition。</li><li>Partitioner接口实现类可以通过KafkaProducer配置项partitioner.class指定，默认值为<br>DefaultPartitioner。<ul><li><div class="img-wrap"><div class="img-bg"><img class="img" src="http://localhost:4000/img/img_md/partitionerClass.png"/></div></div></li></ul></li></ul></div>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2023/07/21/hello-world/"/>
      <url>/2023/07/21/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>看《封神》有感</title>
      <link href="/2023/07/21/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/%E7%9C%8B%E5%B0%81%E7%A5%9E%E6%9C%89%E6%84%9F/"/>
      <url>/2023/07/21/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/%E7%9C%8B%E5%B0%81%E7%A5%9E%E6%9C%89%E6%84%9F/</url>
      
        <content type="html"><![CDATA[<div class="note white no-icon flat"><p>其实也没什么感觉，就是和同学一起在10点一起看电影。<br>我觉得每一次电影都要好好记录下来，以后好好回忆。<br>对于这部电影呢，我的评价是还可以。<br>除了在大场面上，描写的不够细腻，结尾和我预想的不符合以外(我以为姬发要到第二部才杀殷寿，结果死这么快)<br>整体上还算可以，差强人意。</p></div>]]></content>
      
      
      <categories>
          
          <category> 生活 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 电影 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kafka第一天</title>
      <link href="/2023/07/20/kafka/kafka_day01/"/>
      <url>/2023/07/20/kafka/kafka_day01/</url>
      
        <content type="html"><![CDATA[<h1 id="kafka学习第一天"><a href="#kafka学习第一天" class="headerlink" title="kafka学习第一天"></a>kafka学习第一天</h1><h2 id="kafka架构"><a href="#kafka架构" class="headerlink" title="kafka架构"></a>kafka架构</h2><h3 id="生产者producer"><a href="#生产者producer" class="headerlink" title="生产者producer"></a>生产者producer</h3><div class="note white no-icon flat"><p>这感觉就好像什么也没学一样，我尝试说一说自己的理解。<br>在我看来，producer就是生产数据的部分，<br>他将数据推送到相应的主题topic，producer应该就只是一个程序，而不是一个文件，不是一份持久化的东西。<br>不确定，再看看。</p></div><h3 id="消费者"><a href="#消费者" class="headerlink" title="消费者"></a>消费者</h3><h4 id="消费者-1"><a href="#消费者-1" class="headerlink" title="消费者"></a>消费者</h4><div class="note white no-icon flat"><p>消费者顾名思义，就是消费的组件。消费啥呢？？<br>消费生产者给的数据，但是不是直接从生产者那来的，而是从主题那儿来，靠消费者自己去拉取。<br>消费者不仅仅是一段程序，他也需要一些文件来判断拉啥数据，比如offset</p></div><h4 id="消费组"><a href="#消费组" class="headerlink" title="消费组"></a>消费组</h4><div class="note white no-icon flat"><p>整体上就是一个消费者，但是呢，一个消费组里面的所有消费者共享offset文件。<br>也就是说你读取了我就不会再读取</p></div><h4 id="位移偏量offset"><a href="#位移偏量offset" class="headerlink" title="位移偏量offset"></a>位移偏量offset</h4><div class="note white no-icon flat"><p>说白了，就是判断消费者从数据哪个地方读取的东西，比如offset是10，就从第10个读起，这个应该是比较容易理解的</p><div class="img-wrap"><div class="img-bg"><img class="img" src="http://localhost:4000/img/img_md/img.png"/></div></div></div><h3 id="主题topic"><a href="#主题topic" class="headerlink" title="主题topic"></a>主题topic</h3><h4 id="主题topic-1"><a href="#主题topic-1" class="headerlink" title="主题topic"></a>主题topic</h4><div class="note white no-icon flat"><p>主题的意思可以理解为业务，比如订单业务的数据就放在这个topic里面，退货业务的数据就放在退货的topic里面，<br>这样子话，可以将数据按照业务分开，方便后续作业的进展</p></div><h4 id="分区partition"><a href="#分区partition" class="headerlink" title="分区partition"></a>分区partition</h4><div class="note white no-icon flat"><p>分区说的一个topic无法承受那么多数据或者说因为过大的数据而导致性能降低从而出现的东西—分区。<br>说白了，和消费组很像，整体就是一个topic，但是我们操作的是局部，也就是说他应该和消费者组一样。<br>a分区存储的数据，b分区不再存储，大致应该是这么个逻辑。不确定，再看看</p></div><h4 id="服务者broker"><a href="#服务者broker" class="headerlink" title="服务者broker"></a>服务者broker</h4><div class="note white no-icon flat"><p>topic是一个逻辑概念，也就是他根本就不存在。而对应的partition却是物理概念，<br>但是partition既然是一个物理概念，就一定要存在于一台机器上嘛，这台机器我们就称之为broker<br>一个broker可以有多个分区，很像hbase里的hrgionsever存放着多个hrgion。</p></div><h4 id="控制器controller"><a href="#控制器controller" class="headerlink" title="控制器controller"></a>控制器controller</h4><div class="note white no-icon flat"><p>controller是相对于broker而言的，因为在集群里面，总得有人出来管理大家，这份工作总得有人做嘛</p><ul><li>Controller 表示控制器，Kafka 节点中的主节点。集群中任意一台 Broker 都能充当控制器的角色，<br>但是，在运行过程中，只能有一个 Broker 成为控制器，行使其管理和协调的职责。</li><li>在分布式系统中，通常需要有一个协调者，该协调者会在分布式系统发生异常时发挥特殊的作用。<br>在 Kafka 中该协调者称之为控制器（Controller），其实该控制器并没有什么特殊之处，它本身也<br>是一个普通的 Broker，只不过需要负责一些额外的工作：<ul><li>Broker 管理（新增 Broker、Broker 主动关闭、Broker 故障）；</li><li>Topic 管理（创建主题、删除主题）；</li><li>Partition 管理（Leader 分区选举、增加分区、Rebalance 分区）。</li></ul></li><li>值得注意的是：Kafka 集群中始终只有一个 Controller Broker。</li></ul></div><h2 id="kafka数据存储"><a href="#kafka数据存储" class="headerlink" title="kafka数据存储"></a>kafka数据存储</h2><h3 id="文件存储结构"><a href="#文件存储结构" class="headerlink" title="文件存储结构"></a>文件存储结构</h3><div class="note white no-icon flat"><p>前面都是小菜，只是一个抽象的概念，虽然整体学习我感觉都比较抽象，但是这个还是不能这么抽象的。<br>这个就是数据到底是怎么存储在partition中的，大概就是这个意思。<br>ok，在partition中存储数据都是以segment的形式。<br>注意，这里，一个partition中有多个segment。<br>segment也是一个逻辑概念，物理上是由.log，.index，.timeindex三个文件组成的<br>并且这三个文件名字都是一样的，都是segment里面的第一个offset值。</p><div class="note red no-icon flat"><p>log文件存储的数据，可以理解成我们之前所说的业务数据。<br>然后呢，log文件大到1G或者工作了7天，就可以退休了，这个segment就不再生成数据了</p></div><div class="note red no-icon flat"><p>index存储的是索引数据，默认是log文件每存储4k，index就存储一条。<br>也就是说如果一条数据是0.5k的话，会有128M的索引数据，只需要找这128M的索引数据<br>然后再从4k中去找数据。也就是说我们要找一条数据，基本上就找这么多就行了</p></div><div class="note red no-icon flat"><p>timeindex和index基本上差不多，也是4k存储一条，只是timeindex存储的是时间偏量</p></div></div><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>md，真的累，以后不弄这玩意儿了，不能随心所欲，太麻烦了</p>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>20230720</title>
      <link href="/2023/07/20/20230720/"/>
      <url>/2023/07/20/20230720/</url>
      
        <content type="html"><![CDATA[<p>今天2023年7月20号，也是我的第一篇正式博客，看看能不能坚持下去。</p><p> 今天的任务</p><ul><li>[ ] kafka 16到43个，能看多少是多少，最好写下自己的笔记。</li><li>[ ] 博客建站，最好将第一天的写完。<div class="note no-icon flat"><p>你是刷 Visa 还是 UnionPay</p></div></li></ul>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
    </entry>
    
    
  
  
</search>
